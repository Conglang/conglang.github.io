<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="编辑中。">
<meta property="og:type" content="article">
<meta property="og:title" content="Natural Language Processing with Python 笔记">
<meta property="og:url" content="http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/index.html">
<meta property="og:site_name" content="A Stellar Hiker">
<meta property="og:description" content="编辑中。">
<meta property="og:image" content="http://www.nltk.org/images/authors.png">
<meta property="og:image" content="http://www.nltk.org/images/dialogue.png">
<meta property="og:image" content="http://www.nltk.org/images/inaugural2.png">
<meta property="og:image" content="http://www.nltk.org/images/word-len-dist.png">
<meta property="og:image" content="http://www.nltk.org/images/text-corpus-structure.png">
<meta property="og:image" content="http://www.nltk.org/images/lexicon.png">
<meta property="og:image" content="http://www.nltk.org/images/target.png">
<meta property="og:image" content="http://www.nltk.org/images/cfd-gender.png">
<meta property="og:image" content="http://www.nltk.org/images/wordnet-hierarchy.png">
<meta property="og:image" content="http://www.nltk.org/images/pipeline1.png">
<meta property="og:image" content="http://www.nltk.org/images/unicode.png">
<meta property="og:image" content="http://www.nltk.org/images/polish-utf8.png">
<meta property="og:image" content="http://www.nltk.org/images/brent.png">
<meta property="og:image" content="http://www.nltk.org/images/multi-module.png">
<meta property="og:image" content="http://www.nltk.org/images/modal_genre.png">
<meta property="og:image" content="http://www.nltk.org/images/dog-graph.png">
<meta property="og:image" content="http://www.nltk.org/images/tag-context.png">
<meta property="og:image" content="http://www.nltk.org/images/supervised-classification.png">
<meta property="og:image" content="http://www.nltk.org/images/decision-tree.png">
<meta property="og:image" content="http://www.nltk.org/images/naive-bayes-triangle.png">
<meta property="og:image" content="http://www.nltk.org/images/naive_bayes_bargraph.png">
<meta property="og:image" content="http://www.nltk.org/images/ie-architecture.png">
<meta property="og:image" content="http://www.nltk.org/images/chunk-segmentation.png">
<meta property="og:image" content="http://www.nltk.org/book/tree_images/ch07-tree-1.png">
<meta property="og:image" content="http://www.nltk.org/images/chunk-tagrep.png">
<meta property="og:image" content="http://www.nltk.org/images/chunk-treerep.png">
<meta property="og:image" content="http://www.nltk.org/book/tree_images/ch07-tree-2.png">
<meta property="og:image" content="http://www.nltk.org/book/tree_images/ch08-tree-1.png">
<meta property="og:image" content="http://www.nltk.org/book/tree_images/ch08-tree-2.png">
<meta property="og:updated_time" content="2018-07-31T15:04:59.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Natural Language Processing with Python 笔记">
<meta name="twitter:description" content="编辑中。">
<meta name="twitter:image" content="http://www.nltk.org/images/authors.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/astro.png">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/astro.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/astro.png">
          
        
    
    <!-- title -->
    <title>Natural Language Processing with Python 笔记</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
  	<link href="https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css" rel="stylesheet">
</head>

<body class="max-width mx-auto px3 ltr">    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/archives/">博文</a></li>
         
          <li><a href="/categories/">分类</a></li>
         
          <li><a href="/tags/">标签</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2016/04/03/note-the-discipline-of-machine-learning/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2016/02/01/debug-in-vmware/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&text=Natural Language Processing with Python 笔记"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&title=Natural Language Processing with Python 笔记"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&is_video=false&description=Natural Language Processing with Python 笔记"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Natural Language Processing with Python 笔记&body=Check out this article: http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&title=Natural Language Processing with Python 笔记"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&title=Natural Language Processing with Python 笔记"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&title=Natural Language Processing with Python 笔记"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&title=Natural Language Processing with Python 笔记"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&name=Natural Language Processing with Python 笔记&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#preface"><span class="toc-number">1.</span> <span class="toc-text"> Preface</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#language-processing-and-python"><span class="toc-number">2.</span> <span class="toc-text"> Language Processing and Python</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#在ubuntu里设置时可能的坑"><span class="toc-number">2.1.</span> <span class="toc-text"> 在Ubuntu里设置时可能的坑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#computing-with-language-texts-and-words"><span class="toc-number">2.2.</span> <span class="toc-text"> Computing with Language: Texts and Words</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-closer-look-at-python-texts-as-lists-of-words"><span class="toc-number">2.3.</span> <span class="toc-text"> A Closer Look at Python: Texts as Lists of Words</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#computing-with-language-simple-statistics"><span class="toc-number">2.4.</span> <span class="toc-text"> Computing with Language: Simple Statistics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#back-to-python-making-decisions-and-taking-control"><span class="toc-number">2.5.</span> <span class="toc-text"> Back to Python: Making Decisions and Taking Control</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#automatic-natural-language-understanding"><span class="toc-number">2.6.</span> <span class="toc-text"> Automatic Natural Language Understanding</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#accessing-text-corpora-and-lexical-resources"><span class="toc-number">3.</span> <span class="toc-text"> Accessing Text Corpora and Lexical Resources</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#accessing-text-corpora"><span class="toc-number">3.1.</span> <span class="toc-text"> Accessing Text Corpora</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#corporas"><span class="toc-number">3.1.1.</span> <span class="toc-text"> Corporas</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#corpus-functionality"><span class="toc-number">3.1.2.</span> <span class="toc-text"> Corpus Functionality</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#loading-your-own-corpus"><span class="toc-number">3.1.3.</span> <span class="toc-text"> Loading your own Corpus</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#conditional-frequency-distributions"><span class="toc-number">3.2.</span> <span class="toc-text"> Conditional Frequency Distributions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#more-python-reusing-code"><span class="toc-number">3.3.</span> <span class="toc-text"> More Python: Reusing Code</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lexical-resources"><span class="toc-number">3.4.</span> <span class="toc-text"> Lexical Resources</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#wordnet"><span class="toc-number">3.5.</span> <span class="toc-text"> WordNet</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#processing-raw-text"><span class="toc-number">4.</span> <span class="toc-text"> Processing Raw Text</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#accessing-text-from-the-web-and-from-disk"><span class="toc-number">4.1.</span> <span class="toc-text"> Accessing Text from the Web and from Disk</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#strings-text-processing-at-the-lowest-level"><span class="toc-number">4.2.</span> <span class="toc-text"> Strings: Text Processing at the Lowest Level</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#text-processing-with-unicode"><span class="toc-number">4.3.</span> <span class="toc-text"> Text Processing with Unicode</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#regular-expressions-for-detecting-word-patterns"><span class="toc-number">4.4.</span> <span class="toc-text"> Regular Expressions for Detecting Word Patterns</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#useful-applications-of-regular-expressions"><span class="toc-number">4.5.</span> <span class="toc-text"> Useful Applications of Regular Expressions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#normalizeing-text"><span class="toc-number">4.6.</span> <span class="toc-text"> Normalizeing Text</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#regular-expressions-for-tokenizing-text"><span class="toc-number">4.7.</span> <span class="toc-text"> Regular Expressions for Tokenizing Text</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#segmentation"><span class="toc-number">4.8.</span> <span class="toc-text"> Segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#formatting-from-lists-to-strings"><span class="toc-number">4.9.</span> <span class="toc-text"> Formatting: From Lists to Strings</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#writing-structured-programs"><span class="toc-number">5.</span> <span class="toc-text"> Writing Structured Programs</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#back-to-the-basics"><span class="toc-number">5.1.</span> <span class="toc-text"> Back to the Basics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sequences"><span class="toc-number">5.2.</span> <span class="toc-text"> Sequences</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#questions-of-style"><span class="toc-number">5.3.</span> <span class="toc-text"> Questions of Style</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#functions-the-foundation-of-structured-programming"><span class="toc-number">5.4.</span> <span class="toc-text"> Functions: The Foundation of Structured Programming</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#doing-more-with-functions"><span class="toc-number">5.5.</span> <span class="toc-text"> Doing More with Functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#program-development"><span class="toc-number">5.6.</span> <span class="toc-text"> Program Development</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#algorithm-design"><span class="toc-number">5.7.</span> <span class="toc-text"> Algorithm Design</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-sample-of-python-libraries"><span class="toc-number">5.8.</span> <span class="toc-text"> A Sample of Python Libraries</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#categorizing-and-tagging-words"><span class="toc-number">6.</span> <span class="toc-text"> Categorizing and Tagging Words</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#using-a-tagger"><span class="toc-number">6.1.</span> <span class="toc-text"> Using a Tagger</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tagged-corpora"><span class="toc-number">6.2.</span> <span class="toc-text"> Tagged Corpora</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapping-words-to-properties-using-python-dictionaries"><span class="toc-number">6.3.</span> <span class="toc-text"> Mapping Words to Properties Using Python Dictionaries</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#automatic-tagging"><span class="toc-number">6.4.</span> <span class="toc-text"> Automatic Tagging</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#n-gram-tagging"><span class="toc-number">6.5.</span> <span class="toc-text"> N-Gram Tagging</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transformation-based-tagging"><span class="toc-number">6.6.</span> <span class="toc-text"> Transformation-Based Tagging</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-to-determine-the-category-of-a-word"><span class="toc-number">6.7.</span> <span class="toc-text"> How to Determine the Category of a Word</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#learning-to-classify-text"><span class="toc-number">7.</span> <span class="toc-text"> Learning to Classify Text</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#supervised-classification"><span class="toc-number">7.1.</span> <span class="toc-text"> Supervised Classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#further-examples-of-supervised-classification"><span class="toc-number">7.2.</span> <span class="toc-text"> Further Examples of Supervised Classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#evaluation"><span class="toc-number">7.3.</span> <span class="toc-text"> Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#decision-trees"><span class="toc-number">7.4.</span> <span class="toc-text"> Decision Trees</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#naive-bayes-classifiers"><span class="toc-number">7.5.</span> <span class="toc-text"> Naive Bayes Classifiers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#maximum-entropy-classifiers"><span class="toc-number">7.6.</span> <span class="toc-text"> Maximum Entropy Classifiers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#generative-vs-conditional-classifiers"><span class="toc-number">7.7.</span> <span class="toc-text"> Generative vs Conditional Classifiers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#modeling-linguistic-patterns"><span class="toc-number">7.8.</span> <span class="toc-text"> Modeling Linguistic Patterns</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#extracting-information-from-text"><span class="toc-number">8.</span> <span class="toc-text"> Extracting Information from Text</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#information-extraction"><span class="toc-number">8.1.</span> <span class="toc-text"> Information Extraction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#chunking"><span class="toc-number">8.2.</span> <span class="toc-text"> Chunking</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#developing-and-evaluating-chunkers"><span class="toc-number">8.3.</span> <span class="toc-text"> Developing and Evaluating Chunkers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#recursion-in-linguistic-structure"><span class="toc-number">8.4.</span> <span class="toc-text"> Recursion in Linguistic Structure</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#named-entity-recognition"><span class="toc-number">8.5.</span> <span class="toc-text"> Named Entity Recognition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#relation-extraction"><span class="toc-number">8.6.</span> <span class="toc-text"> Relation Extraction</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#analyzing-sentence-structure"><span class="toc-number">9.</span> <span class="toc-text"> Analyzing Sentence Structure</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#some-grammatical-dilemmas"><span class="toc-number">9.1.</span> <span class="toc-text"> Some Grammatical Dilemmas</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#whats-the-use-of-syntax"><span class="toc-number">9.2.</span> <span class="toc-text"> What’s the Use of Syntax?</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index my4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Natural Language Processing with Python 笔记
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">A Stellar Hiker</span>
      </span>
      
    <div class="postdate">
        <time datetime="2016-02-05T07:35:49.000Z" itemprop="datePublished">2016-02-05</time>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/Book/">Book</a>, <a class="tag-link" href="/tags/Natural-Language-Processing/">Natural Language Processing</a>, <a class="tag-link" href="/tags/Python/">Python</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>网上能找到的pdf格式中英文版均基于Python2.5，NLTK现已更新支持Python3.0，最新书籍英文版参看<a href="http://www.nltk.org/book/" target="_blank" rel="external">这里</a>。还有<a href="http://www.oreilly.com/catalog/errata.csp?isbn=9780596516499" target="_blank" rel="external">Errata</a>。<br>
<img src="http://www.nltk.org/images/authors.png" alt="Edward Loper, Ewan Klein, and Steven Bird, Stanford, July 2007"></p>
<hr>
<h2 id="preface"><a class="markdownIt-Anchor" href="#preface"></a> Preface</h2>
<p><strong>Software Requirements</strong></p>
<ul>
<li><a href="http://docs.python.org/" target="_blank" rel="external">Python</a><br>
The material presented in this book assumes that you are using Python version 3.2 or later. (Note that NLTK 3.0 also works with Python 2.6 and 2.7.)</li>
<li><a href="http://www.nltk.org/" target="_blank" rel="external">NLTK</a><br>
The code examples in this book use NLTK version 3.0. Subsequent releases of NLTK will be backward-compatible with NLTK 3.0.</li>
<li>NLTK-Data<br>
This contains the linguistic corpora that are analyzed and processed in the book.</li>
<li>NumPy<br>
(recommended) This is a scientific computing library with support for multidimensional arrays and linear algebra, required for certain probability, tagging, clustering, and classification tasks.</li>
<li>Matplotlib<br>
(recommended) This is a 2D plotting library for data visualization, and is used in some of the book’s code samples that produce line graphs and bar charts.</li>
<li>Stanford NLP Tools:<br>
(recommended) NLTK includes interfaces to the Stanford NLP Tools which are useful for large scale language processing (see <a href="http://nlp.stanford.edu/software/" target="_blank" rel="external">http://nlp.stanford.edu/software/</a>).</li>
<li>NetworkX<br>
(optional) This is a library for storing and manipulating network structures consisting of nodes and edges. For visualizing semantic networks, also install the Graphviz library.</li>
<li>Prover9<br>
(optional) This is an automated theorem prover for first-order and equational logic, used to support inference in language processing.</li>
</ul>
<p><strong>Natural Language Toolkit (NLTK)</strong></p>
<table>
<thead>
<tr>
<th>Language processing task</th>
<th>NLTK modules</th>
<th>Functionality</th>
</tr>
</thead>
<tbody>
<tr>
<td>Accessing corpora</td>
<td>corpus</td>
<td>standardized interfaces to corpora and lexicons</td>
</tr>
<tr>
<td>String processing</td>
<td>tokenize, stem</td>
<td>tokenizers, sentence tokenizers, stemmers</td>
</tr>
<tr>
<td>Collocation discovery</td>
<td>collocations</td>
<td>t-test, chi-squared, point-wise mutual information</td>
</tr>
<tr>
<td>Part-of-speech tagging</td>
<td>tag</td>
<td>n-gram, backoff, Brill, HMM, TnT</td>
</tr>
<tr>
<td>Machine learning</td>
<td>classify, cluster, tbl</td>
<td>decision tree, maximum entropy, naive Bayes, EM, k-means</td>
</tr>
<tr>
<td>Chunking</td>
<td>chunk</td>
<td>regular expression, n-gram, named-entity</td>
</tr>
<tr>
<td>Parsing</td>
<td>parse, ccg</td>
<td>chart, feature-based, unification, probabilistic, dependency</td>
</tr>
<tr>
<td>Semantic interpretation</td>
<td>sem, inference</td>
<td>lambda calculus, first-order logic, model checking</td>
</tr>
<tr>
<td>Evaluation metrics</td>
<td>metrics</td>
<td>precision, recall, agreement coefficients</td>
</tr>
<tr>
<td>Probability and estimation</td>
<td>probability</td>
<td>frequency distributions, smoothed probability distributions</td>
</tr>
<tr>
<td>Applications</td>
<td>app, chat</td>
<td>graphical concordancer, parsers, WordNet browser, chatbots</td>
</tr>
<tr>
<td>Linguistic fieldwork</td>
<td>toolbox</td>
<td>manipulate data in SIL Toolbox format</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="language-processing-and-python"><a class="markdownIt-Anchor" href="#language-processing-and-python"></a> Language Processing and Python</h2>
<p>本章要解决的问题：</p>
<ol>
<li>What can we achieve by combining simple programming techniques with large quantities of text?</li>
<li>How can we automatically extract key words and phrases that sum up the style and content of a text?</li>
<li>What tools and techniques does the Python programming language provide for such work?</li>
<li>What are some of the interesting challenges of natural language processing?</li>
</ol>
<h3 id="在ubuntu里设置时可能的坑"><a class="markdownIt-Anchor" href="#在ubuntu里设置时可能的坑"></a> 在Ubuntu里设置时可能的坑</h3>
<p>为了用新例子，<a href="http://www.cnblogs.com/ningvsban/p/4384995.html" target="_blank" rel="external">更改Ubuntu的默认Python版本</a>为3.2以上的版本。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 安装pyenv</span><br><span class="line">git clone git://github.com/yyuu/pyenv.git ~/.pyenv</span><br><span class="line">echo &apos;export PYENV_ROOT=&quot;$HOME/.pyenv&quot;&apos; &gt;&gt; ~/.bashrc</span><br><span class="line">echo &apos;export PATH=&quot;$PYENV_ROOT/bin:$PATH&quot;&apos; &gt;&gt; ~/.bashrc</span><br><span class="line">echo &apos;eval &quot;$(pyenv init -)&quot;&apos; &gt;&gt; ~/.bashrc</span><br><span class="line">exec $SHELL -l</span><br><span class="line"># 查看可安装的python版本</span><br><span class="line">pyenv install --list</span><br><span class="line"># 安装python所需依赖包</span><br><span class="line">sudo apt-get install libc6-dev gcc</span><br><span class="line">sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm</span><br><span class="line"># 安装python</span><br><span class="line">pyenv install 3.5.1 -v</span><br><span class="line"># 更新数据库，用pip安装后也需更新</span><br><span class="line">pyenv rehash</span><br><span class="line"># 查看已安装的python版本</span><br><span class="line">pyenv versions</span><br><span class="line"># 设置全局的python版本</span><br><span class="line">pyenv global 3.5.1</span><br></pre></td></tr></table></figure>
<p>此时nltk等包需<a href="https://docs.python.org/3/installing/" target="_blank" rel="external">安装在Python指定版本下</a>。用<code>python -m pip install nltk</code>。<br>
如果不能正常使用IDLE，提示&quot;IDLE can’t import Tkinter. Your python may not be configured for Tk.&quot;。使用<a href="http://stackoverflow.com/questions/26357567/cannot-import-tkinter-after-installing-python-3-with-pyenv" target="_blank" rel="external">以下方法</a>：</p>
<ul>
<li>First uninstall Python 3.5.1 : <code>pyenv uninstall 3.5.1</code>, then</li>
<li>Run <code>sudo apt-get install tk-dev</code></li>
<li>And reinstall Python 3.5.1 : <code>pyenv install 3.5.1</code></li>
</ul>
<h3 id="computing-with-language-texts-and-words"><a class="markdownIt-Anchor" href="#computing-with-language-texts-and-words"></a> Computing with Language: Texts and Words</h3>
<p><strong>Token</strong>: technical name for a sequence of characters that we want to treat as a group.<br>
<strong>Word Type</strong>: the form or spelling of the word independently of its specific occurrences in a text. i.e. the word considered as a unique item of vocabulary.</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># import nltk module</span><br><span class="line">import nltk</span><br><span class="line"># import everything in book data</span><br><span class="line">from nltk.book import *</span><br><span class="line"># show name of the text source</span><br><span class="line">text1</span><br><span class="line"># searching text</span><br><span class="line">text1.concordance("monstrous")</span><br><span class="line"># what other words appear in a similar range of context</span><br><span class="line">text1.similar("monstrous")</span><br><span class="line"># the contexts shared by two or more words</span><br><span class="line">text2.common_contexts(["monstrous", "very"])</span><br><span class="line"># location of a word in the text: how many words from the beginning it appears</span><br><span class="line">text4.dispersion_plot(["citizens", "democracy", "freedom", "duties", "America"])</span><br><span class="line"># generate random text. 3.0版本没有。</span><br><span class="line">text3.generate()</span><br><span class="line"># finding out the length of a text from start to finish</span><br><span class="line">len(text3)</span><br><span class="line"># a sorted list of the set of unique tokens(word type) in text</span><br><span class="line">sorted(set(text3))</span><br><span class="line"># length of the unique tokens set</span><br><span class="line">len(set(text3))</span><br><span class="line"># calculate lexical richness of the text</span><br><span class="line">len(set(text3)) / len(text3)</span><br><span class="line"># count how often a word occures in a text</span><br><span class="line">text3.count("smote")</span><br><span class="line"># compute what percentage of the text is taken up by a specific word</span><br><span class="line">100 * text4.count('a') / len(text4)</span><br><span class="line"># define python functions</span><br><span class="line">def lexical_diversity(text)</span><br><span class="line">	return len(set(text)) / len(text)</span><br><span class="line">lexical_diversity(text3)</span><br></pre></td></tr></table></figure>
<h3 id="a-closer-look-at-python-texts-as-lists-of-words"><a class="markdownIt-Anchor" href="#a-closer-look-at-python-texts-as-lists-of-words"></a> A Closer Look at Python: Texts as Lists of Words</h3>
<p><a href="https://learnxinyminutes.com/docs/python3/" target="_blank" rel="external">基础Python3语法复习</a>。<br>
<a href="http://docs.python.org/" target="_blank" rel="external">官方Python文档</a>。<br>
注意这里index从0开始。</p>
<h3 id="computing-with-language-simple-statistics"><a class="markdownIt-Anchor" href="#computing-with-language-simple-statistics"></a> Computing with Language: Simple Statistics</h3>
<p><strong>Hapaxes</strong>: words that occur only once.<br>
<strong>Collocation</strong>: a sequence of words that occur together unusually often.<br>
<strong>Bigrams</strong>: a list of word pairs.</p>
<p>Frequency Distributions</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># find frequency distribution in text</span></span><br><span class="line">fdist1 = FreqDist(text1)</span><br><span class="line">print(fdist1)</span><br><span class="line">fdist1.most_common(<span class="number">50</span>)	<span class="comment"># most common 50 words</span></span><br><span class="line">fdist1[<span class="string">"whale"</span>]</span><br><span class="line">fdist1.plot(<span class="number">50</span>, cumulative=<span class="keyword">True</span>)</span><br><span class="line">fdist1.hapaxes()	<span class="comment"># words that occur only once</span></span><br></pre></td></tr></table></figure>
<p>Fine-grained Selection of Words</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># find the words that are longer than seven characters that occur more than 7 times</span></span><br><span class="line">fdist5 = FreqDist(text5)</span><br><span class="line">sorted(w <span class="keyword">for</span> w <span class="keyword">in</span> set(text5) <span class="keyword">if</span> len(w) &gt; <span class="number">7</span> <span class="keyword">and</span> fdist5[w] &gt; <span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<p>Collocations and Bigrams</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># find bigrams</span></span><br><span class="line">list(bigrams([<span class="string">'more'</span>, <span class="string">'is'</span>, <span class="string">'said'</span>, <span class="string">'than'</span>, <span class="string">'done'</span>]))</span><br><span class="line"><span class="comment"># find bigrams that occur more often than we would expect based on the frequency of the individual words</span></span><br><span class="line">text4.collocations()</span><br></pre></td></tr></table></figure>
<p>Counting Other Things</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the distribution of word lengths in a text</span></span><br><span class="line">fdist = FreqDist(len(w) <span class="keyword">for</span> w <span class="keyword">in</span> text1)</span><br><span class="line">fdist.most_common()</span><br><span class="line">fdist.max()</span><br><span class="line">fdist[<span class="number">3</span>]</span><br><span class="line">fdist.freq(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Functions Defined for NLTK’s Frequency Distributions</strong></p>
<table>
<thead>
<tr>
<th>Example</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>fdist = FreqDist(samples)</td>
<td>create a frequency distribution containing the given samples</td>
</tr>
<tr>
<td>fdist[sample] += 1</td>
<td>increment the count for this sample</td>
</tr>
<tr>
<td>fdist[‘monstrous’]</td>
<td>count of the number of times a given sample occurred</td>
</tr>
<tr>
<td>fdist.freq(‘monstrous’)</td>
<td>frequency of a given sample</td>
</tr>
<tr>
<td>fdist.N()</td>
<td>total number of samples</td>
</tr>
<tr>
<td>fdist.most_common(n)</td>
<td>the n most common samples and their frequencies</td>
</tr>
<tr>
<td>for sample in fdist:</td>
<td>iterate over the samples</td>
</tr>
<tr>
<td>fdist.max()</td>
<td>sample with the greatest count</td>
</tr>
<tr>
<td>fdist.tabulate()</td>
<td>tabulate the frequency distribution</td>
</tr>
<tr>
<td>fdist.plot()</td>
<td>graphical plot of the frequency distribution</td>
</tr>
<tr>
<td>fdist.plot(cumulative=True)</td>
<td>cumulative plot of the frequency distribution</td>
</tr>
<tr>
<td>fdist1 ¦= fdist2</td>
<td>update fdist1 with counts from fdist2</td>
</tr>
<tr>
<td>fdist1 &lt; fdist2</td>
<td>test if samples in fdist1 occur less frequently than in fdist2</td>
</tr>
</tbody>
</table>
<h3 id="back-to-python-making-decisions-and-taking-control"><a class="markdownIt-Anchor" href="#back-to-python-making-decisions-and-taking-control"></a> Back to Python: Making Decisions and Taking Control</h3>
<p><strong>Some Word Comparison Operators</strong></p>
<table>
<thead>
<tr>
<th>Function</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>s.startswith(t)</td>
<td>test if s starts with t</td>
</tr>
<tr>
<td>s.endswith(t)</td>
<td>test if s ends with t</td>
</tr>
<tr>
<td>t in s</td>
<td>test if t is a substring of s</td>
</tr>
<tr>
<td>s.islower()</td>
<td>test if s contains cased characters and all are lowercase</td>
</tr>
<tr>
<td>s.isupper()</td>
<td>test if s contains cased characters and all are uppercase</td>
</tr>
<tr>
<td>s.isalpha()</td>
<td>test if s is non-empty and all characters in s are alphabetic</td>
</tr>
<tr>
<td>s.isalnum()</td>
<td>test if s is non-empty and all characters in s are alphanumeric</td>
</tr>
<tr>
<td>s.isdigit()</td>
<td>test if s is non-empty and all characters in s are digits</td>
</tr>
<tr>
<td>s.istitle()</td>
<td>test if s contains cased characters and is titlecased (i.e. all words in s have initial capitals)</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sorted(wd <span class="keyword">for</span> wd <span class="keyword">in</span> set(text3) <span class="keyword">if</span> wd.istitle() <span class="keyword">and</span> len(wd) &gt; <span class="number">10</span>)</span><br><span class="line">sorted(t <span class="keyword">for</span> t <span class="keyword">in</span> set(text2) <span class="keyword">if</span> <span class="string">'cie'</span> <span class="keyword">in</span> t <span class="keyword">or</span> <span class="string">'cei'</span> <span class="keyword">in</span> t)</span><br><span class="line">len(set(word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> text1 <span class="keyword">if</span> word.isalpha()))</span><br></pre></td></tr></table></figure>
<h3 id="automatic-natural-language-understanding"><a class="markdownIt-Anchor" href="#automatic-natural-language-understanding"></a> Automatic Natural Language Understanding</h3>
<p><strong>Word Sense Disambiguation</strong></p>
<blockquote>
<p>Nearby words have closely related meanings.<br>
Example:<br>
a. The lost children were found by the searchers (agentive — Chesterton was the author of the book)<br>
b. The lost children were found by the mountain (locative — the stove is where the cup is)<br>
c. The lost children were found by the afternoon (temporal — Friday is the time of the submitting)</p>
</blockquote>
<p><strong>Pronoun Resolution</strong></p>
<blockquote>
<p>Who did what to whom.<br>
Example:<br>
a. The thieves stole the paintings. They were subsequently sold.<br>
b. The thieves stole the paintings. They were subsequently caught.<br>
c. The thieves stole the paintings. They were subsequently found.<br>
They到底指代谁。<br>
<strong>Anaphora Resolution</strong>: identifying what a pronoun or noun phrase refers to.<br>
<strong>Semantic Role Labeling</strong>: identifying how a noun phrase relates to the verb.</p>
</blockquote>
<p><strong>Generating Language Output</strong></p>
<blockquote>
<p><strong>Question Answering</strong> and <strong>Machine Translation</strong><br>
a. Text: … The thieves stole the paintings. They were subsequently sold. …<br>
b. Human: Who or what was sold?<br>
c. Machine: The paintings.<br>
翻译成法语，指thief时用男性，指painting时用女性。<br>
a. The thieves stole the paintings. They were subsequently found.<br>
b. Les voleurs ont volé les peintures. Ils ont été trouvés plus tard. (the thieves)<br>
c. Les voleurs ont volé les peintures. Elles ont été trouvées plus tard. (the paintings)</p>
</blockquote>
<p><strong>Machine Translation</strong></p>
<blockquote>
<p>把一个句子翻译过去又翻译回来就可以看到翻译系统的不足。<br>
0&gt; how long before the next flight to Alice Springs?<br>
1&gt; wie lang vor dem folgenden Flug zu Alice Springs?<br>
2&gt; how long before the following flight to Alice jump?<br>
3&gt; wie lang vor dem folgenden Flug zu Alice springen Sie?<br>
4&gt; how long before the following flight to Alice do you jump?<br>
5&gt; wie lang, bevor der folgende Flug zu Alice tun, Sie springen?<br>
6&gt; how long, before the following flight to Alice does, do you jump?<br>
7&gt; wie lang bevor der folgende Flug zu Alice tut, tun Sie springen?<br>
8&gt; how long before the following flight to Alice does, do you jump?<br>
9&gt; wie lang, bevor der folgende Flug zu Alice tut, tun Sie springen?<br>
10&gt; how long, before the following flight does to Alice, do do you jump?<br>
11&gt; wie lang bevor der folgende Flug zu Alice tut, Sie tun Sprung?<br>
12&gt; how long before the following flight does leap to Alice, does you?<br>
<strong>Text Alignment</strong>: pair up the sentences of different languages.</p>
</blockquote>
<p><strong>Spoken Dialog Systems</strong></p>
<blockquote>
<p><strong>Turing Test</strong>: can a dialogue system, responding to a user’s text input, perform so natually that we cannot distinguish it from a human-generated response?<br>
<img src="http://www.nltk.org/images/dialogue.png" alt="Simple Pipeline Architecture for a Spoken Dialogue System: Spoken input (top left) is analyzed, words are recognized, sentences are parsed and interpreted in context, application-specific actions take place (top right); a response is planned, realized as a syntactic structure, then to suitably inflected words, and finally to spoken output; different types of linguistic knowledge inform each stage of the process."><br>
<code>nltk.chat.chatbots()</code>来感受一下分分钟起无名火的聊天系统。</p>
</blockquote>
<p><strong>Textual Entailment</strong></p>
<blockquote>
<p>Recognizing Textual Entailment (RTE)分析某段文字能否为某论点提供论证。<br>
a. Text: David Golinkin is the editor or author of eighteen books, and over 150 responsa, articles, sermons and books<br>
b. Hypothesis: Golinkin has written eighteen books<br>
需要做如下论证：<br>
(i) if someone is an author of a book, then he/she has written that book; (ii) if someone is an editor of a book, then he/she has not written (all of) that book; (iii) if someone is editor or author of eighteen books, then one cannot conclude that he/she is author of eighteen books.</p>
</blockquote>
<p><strong>Limitations of NLP</strong></p>
<blockquote>
<p>An important goal of NLP research has been to make progress on the difficult task of building technologies that “understand language,” using superficial yet powerful techniques instead of unrestricted knowledge and reasoning capabilities.</p>
</blockquote>
<hr>
<h2 id="accessing-text-corpora-and-lexical-resources"><a class="markdownIt-Anchor" href="#accessing-text-corpora-and-lexical-resources"></a> Accessing Text Corpora and Lexical Resources</h2>
<p>本章要解决的问题：</p>
<ol>
<li>What are some useful text corpora and lexical resources, and how can we access them with Python?</li>
<li>Which Python constructs are most helpful for this work?</li>
<li>How do we avoid repeating ourselves when writing Python code?</li>
</ol>
<p><strong>Copora</strong>: large bodied of linguistic data.</p>
<h3 id="accessing-text-corpora"><a class="markdownIt-Anchor" href="#accessing-text-corpora"></a> Accessing Text Corpora</h3>
<h4 id="corporas"><a class="markdownIt-Anchor" href="#corporas"></a> Corporas</h4>
<p><strong>Gutenberg Corpus</strong><br>
contains some 25,000 free electronic books, hosted at <a href="http://www.gutenberg.org/" target="_blank" rel="external">http://www.gutenberg.org/</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> gutenberg</span><br><span class="line">gutenberg.fileids()</span><br><span class="line">emma = nltk.Text(gutenberg.words(<span class="string">'austen-emma.txt'</span>))</span><br><span class="line">emma.concordance(<span class="string">"surprize"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span><br><span class="line">average word length, average sentence length, the number of times each vocabulary item </span><br><span class="line">appears in the text on average(our lexical diversity score) for each text in gutenberg</span><br><span class="line">'''</span></span><br><span class="line"><span class="keyword">for</span> fileid <span class="keyword">in</span> gutenberg.fileids():</span><br><span class="line">	num_chars = len(gutenberg.raw(fileid)) [<span class="number">1</span>]</span><br><span class="line">	num_words = len(gutenberg.words(fileid))</span><br><span class="line">	num_sents = len(gutenberg.sents(fileid))</span><br><span class="line">	num_vocab = len(set(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> gutenberg.words(fileid)))</span><br><span class="line">	print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sents() divides the text up into its sentences</span></span><br><span class="line">macbeth_sentences = gutenberg.sents(<span class="string">"shakespeare-macbeth.txt"</span>)</span><br><span class="line">longest_len = max(len(s) <span class="keyword">for</span> s <span class="keyword">in</span> macbeth_sentences)</span><br><span class="line">[s <span class="keyword">for</span> s <span class="keyword">in</span> macbeth_sentences <span class="keyword">if</span> len(s) == longest_len]</span><br></pre></td></tr></table></figure>
<p>words(),raw(),sents()之外还有更多操作，以后再讲。</p>
<p><strong>Web and Chat Text</strong><br>
content from a Firefox discussion forum, conversations overheard in New York, the movie script of Pirates of the Carribean, personal advertisements, wine reviews.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> webtext</span><br><span class="line"><span class="keyword">for</span> fileid <span class="keyword">in</span> webtext.fileids():</span><br><span class="line">	print(fileid, webtext.raw(fileid)[:<span class="number">65</span>], <span class="string">'...'</span>)</span><br></pre></td></tr></table></figure>
<p>instant messaging chat sessions.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> nps_chat</span><br><span class="line">chatroom = nps_chat.posts(<span class="string">'10-19-20s_706posts.xml'</span>)</span><br><span class="line">chatroom[<span class="number">123</span>]</span><br></pre></td></tr></table></figure>
<p><strong>Brown Corpus</strong><br>
first million-word electronic corpus of English, sources categorized by genre.<br>
genre的完整列表见http://icame.uib.no/brown/bcm-los.html。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compare genres in their usage of modal verbs</span></span><br><span class="line"><span class="comment"># 1. produce the counts for a particular genre</span></span><br><span class="line">fro nltk.corpus <span class="keyword">import</span> brown</span><br><span class="line">brown.categories()</span><br><span class="line">news_text = brown.words(categories=<span class="string">'news'</span>)</span><br><span class="line">fdist = nltk.FreqDist(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> news_text)</span><br><span class="line">modals = [<span class="string">'can'</span>, <span class="string">'could'</span>, <span class="string">'may'</span>, <span class="string">'might'</span>, <span class="string">'must'</span>, <span class="string">'will'</span>]</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> modals:</span><br><span class="line">	print(m + <span class="string">':'</span>, fdist[m], end=<span class="string">' '</span>)</span><br><span class="line"><span class="comment"># 2. obtain counts for each genre of interest</span></span><br><span class="line">cfd = nltk.ConditionalFreqDist(</span><br><span class="line">	(genre, word)</span><br><span class="line">	<span class="keyword">for</span> genre <span class="keyword">in</span> brown.categories()</span><br><span class="line">	<span class="keyword">for</span> word <span class="keyword">in</span> brown.words(categories=genre))</span><br><span class="line">genres = [<span class="string">'news'</span>, <span class="string">'religion'</span>, <span class="string">'hobbies'</span>, <span class="string">'science_fiction'</span>, <span class="string">'romance'</span>, <span class="string">'humor'</span>]</span><br><span class="line">modals = [<span class="string">'can'</span>, <span class="string">'could'</span>, <span class="string">'may'</span>, <span class="string">'might'</span>, <span class="string">'must'</span>, <span class="string">'will'</span>]</span><br><span class="line">cfd.tabulate(conditions=genres, samples=modals)</span><br></pre></td></tr></table></figure>
<p><strong>Reuters Corpus</strong><br>
classified into 90 topics, grouped into “training” and “test”.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># categories overlap with each other</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> reuters</span><br><span class="line">reuters.fileids()</span><br><span class="line">reuters.categories()</span><br><span class="line">reuters.categories([<span class="string">'training/9865'</span>, <span class="string">'training/9880'</span>])</span><br><span class="line">reuters.fileids([<span class="string">'barley'</span>, <span class="string">'corn'</span>])</span><br><span class="line"><span class="comment"># words or sentences in terms of files or categories</span></span><br><span class="line">reuters.words(<span class="string">'training/9865'</span>)[:<span class="number">14</span>]</span><br><span class="line">reuters.words(categories=[<span class="string">'barley'</span>, <span class="string">'corn'</span>])</span><br></pre></td></tr></table></figure>
<p><strong>Inaugural Address Corpus</strong><br>
a collection of 55 texts.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get the year of each text</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> inaugural</span><br><span class="line">inaugural.fileids()</span><br><span class="line">[fileid[:<span class="number">4</span>] <span class="keyword">for</span> fileid <span class="keyword">in</span> inaugural.fileids()]</span><br><span class="line"><span class="comment"># 'America' and 'citizen' are used over time</span></span><br><span class="line">cfd = nltk.ConditionalFreqDist(</span><br><span class="line">	(target, fileid[:<span class="number">4</span>])</span><br><span class="line">	<span class="keyword">for</span> fileid <span class="keyword">in</span> inaugural.fileids()</span><br><span class="line">	<span class="keyword">for</span> w <span class="keyword">in</span> inaugural.words(fileid)</span><br><span class="line">	<span class="keyword">for</span> target <span class="keyword">in</span> [<span class="string">'america'</span>, <span class="string">'citizen'</span>]</span><br><span class="line">	<span class="keyword">if</span> w.lower().startswith(target))</span><br><span class="line">cfd.plot()</span><br></pre></td></tr></table></figure>
<p><img src="http://www.nltk.org/images/inaugural2.png" alt="Plot of a Conditional Frequency Distribution: all words in the Inaugural Address Corpus that begin with america or citizen are counted; separate counts are kept for each address; these are plotted so that trends in usage over time can be observed; counts are not normalized for document length."></p>
<p><strong>Annotated Text Corpora</strong><br>
contain linguistic annotations, representing POS tags, named entities, syntactic structures, semantic roles, and so forth.<br>
For information about downloading them, see <a href="http://nltk.org/data" target="_blank" rel="external">http://nltk.org/data</a>. For more examples of how to access NLTK corpora, please consult the Corpus HOWTO at <a href="http://nltk.org/howto" target="_blank" rel="external">http://nltk.org/howto</a>.</p>
<p><strong>Some of the Corpora and Corpus Samples Distributed with NLTK</strong></p>
<table>
<thead>
<tr>
<th>Corpus</th>
<th>Compiler</th>
<th>Contents</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brown Corpus</td>
<td>Francis, Kucera</td>
<td>15 genres, 1.15M words, tagged, categorized</td>
</tr>
<tr>
<td>CESS Treebanks</td>
<td>CLiC-UB</td>
<td>1M words, tagged and parsed (Catalan, Spanish)</td>
</tr>
<tr>
<td>Chat-80 Data Files</td>
<td>Pereira &amp; Warren</td>
<td>World Geographic Database</td>
</tr>
<tr>
<td>CMU Pronouncing Dictionary</td>
<td>CMU</td>
<td>127k entries</td>
</tr>
<tr>
<td>CoNLL 2000 Chunking Data</td>
<td>CoNLL</td>
<td>270k words, tagged and chunked</td>
</tr>
<tr>
<td>CoNLL 2002 Named Entity</td>
<td>CoNLL</td>
<td>700k words, pos- and named-entity-tagged (Dutch, Spanish)</td>
</tr>
<tr>
<td>CoNLL 2007 Dependency Treebanks (sel)</td>
<td>CoNLL</td>
<td>150k words, dependency parsed (Basque, Catalan)</td>
</tr>
<tr>
<td>Dependency Treebank</td>
<td>Narad</td>
<td>Dependency parsed version of Penn Treebank sample</td>
</tr>
<tr>
<td>FrameNet</td>
<td>Fillmore, Baker et al</td>
<td>10k word senses, 170k manually annotated sentences</td>
</tr>
<tr>
<td>Floresta Treebank</td>
<td>Diana Santos et al</td>
<td>9k sentences, tagged and parsed (Portuguese)</td>
</tr>
<tr>
<td>Gazetteer Lists</td>
<td>Various</td>
<td>Lists of cities and countries</td>
</tr>
<tr>
<td>Genesis Corpus</td>
<td>Misc web sources</td>
<td>6 texts, 200k words, 6 languages</td>
</tr>
<tr>
<td>Gutenberg (selections)</td>
<td>Hart, Newby, et al</td>
<td>18 texts, 2M words</td>
</tr>
<tr>
<td>Inaugural Address Corpus</td>
<td>CSpan</td>
<td>US Presidential Inaugural Addresses (1789-present)</td>
</tr>
<tr>
<td>Indian POS-Tagged Corpus</td>
<td>Kumaran et al</td>
<td>60k words, tagged (Bangla, Hindi, Marathi, Telugu)</td>
</tr>
<tr>
<td>MacMorpho Corpus</td>
<td>NILC, USP, Brazil</td>
<td>1M words, tagged (Brazilian Portuguese)</td>
</tr>
<tr>
<td>Movie Reviews</td>
<td>Pang, Lee</td>
<td>2k movie reviews with sentiment polarity classification</td>
</tr>
<tr>
<td>Names Corpus</td>
<td>Kantrowitz, Ross</td>
<td>8k male and female names</td>
</tr>
<tr>
<td>NIST 1999 Info Extr (selections)</td>
<td>Garofolo</td>
<td>63k words, newswire and named-entity SGML markup</td>
</tr>
<tr>
<td>Nombank</td>
<td>Meyers</td>
<td>115k propositions, 1400 noun frames</td>
</tr>
<tr>
<td>NPS Chat Corpus</td>
<td>Forsyth, Martell</td>
<td>10k IM chat posts, POS-tagged and dialogue-act tagged</td>
</tr>
<tr>
<td>Open Multilingual WordNet</td>
<td>Bond et al</td>
<td>15 languages, aligned to English WordNet</td>
</tr>
<tr>
<td>PP Attachment Corpus</td>
<td>Ratnaparkhi</td>
<td>28k prepositional phrases, tagged as noun or verb modifiers</td>
</tr>
<tr>
<td>Proposition Bank</td>
<td>Palmer</td>
<td>113k propositions, 3300 verb frames</td>
</tr>
<tr>
<td>Question Classification</td>
<td>Li, Roth</td>
<td>6k questions, categorized</td>
</tr>
<tr>
<td>Reuters Corpus</td>
<td>Reuters</td>
<td>1.3M words, 10k news documents, categorized</td>
</tr>
<tr>
<td>Roget’s Thesaurus</td>
<td>Project Gutenberg</td>
<td>200k words, formatted text</td>
</tr>
<tr>
<td>RTE Textual Entailment</td>
<td>Dagan et al</td>
<td>8k sentence pairs, categorized</td>
</tr>
<tr>
<td>SEMCOR</td>
<td>Rus, Mihalcea</td>
<td>880k words, part-of-speech and sense tagged</td>
</tr>
<tr>
<td>Senseval 2 Corpus</td>
<td>Pedersen</td>
<td>600k words, part-of-speech and sense tagged</td>
</tr>
<tr>
<td>SentiWordNet</td>
<td>Esuli, Sebastiani</td>
<td>sentiment scores for 145k WordNet synonym sets</td>
</tr>
<tr>
<td>Shakespeare texts (selections)</td>
<td>Bosak</td>
<td>8 books in XML format</td>
</tr>
<tr>
<td>State of the Union Corpus</td>
<td>CSPAN</td>
<td>485k words, formatted text</td>
</tr>
<tr>
<td>Stopwords Corpus</td>
<td>Porter et al</td>
<td>2,400 stopwords for 11 languages</td>
</tr>
<tr>
<td>Swadesh Corpus</td>
<td>Wiktionary</td>
<td>comparative wordlists in 24 languages</td>
</tr>
<tr>
<td>Switchboard Corpus (selections)</td>
<td>LDC</td>
<td>36 phonecalls, transcribed, parsed</td>
</tr>
<tr>
<td>Univ Decl of Human Rights</td>
<td>United Nations</td>
<td>480k words, 300+ languages</td>
</tr>
<tr>
<td>Penn Treebank (selections)</td>
<td>LDC</td>
<td>40k words, tagged and parsed</td>
</tr>
<tr>
<td>TIMIT Corpus (selections)</td>
<td>NIST/LDC</td>
<td>audio files and transcripts for 16 speakers</td>
</tr>
<tr>
<td>VerbNet 2.1</td>
<td>Palmer et al</td>
<td>5k verbs, hierarchically organized, linked to WordNet</td>
</tr>
<tr>
<td>Wordlist Corpus</td>
<td><a href="http://OpenOffice.org" target="_blank" rel="external">OpenOffice.org</a> et al</td>
<td>960k words and 20k affixes for 8 languages</td>
</tr>
<tr>
<td>WordNet 3.0 (English)</td>
<td>Miller, Fellbaum</td>
<td>145k synonym sets</td>
</tr>
</tbody>
</table>
<p><strong>Corpora in Other Languages</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">nltk.corpus.cess_esp.words()</span><br><span class="line">nltk.corpus.floresta.words()</span><br><span class="line">nltk.corpus.indian.words(<span class="string">'hindi.pos'</span>)</span><br><span class="line"><span class="comment"># udhr, contains the Universal Declaration of Human Rights in over 300 languages</span></span><br><span class="line">nltk.corpus.udhr.fileids()</span><br><span class="line">nltk.corpus.udhr.words(<span class="string">'Javanese-Latin1'</span>)[<span class="number">11</span>:]</span><br><span class="line"><span class="comment"># use a conditional frequency distribution to examine the differences </span></span><br><span class="line"><span class="comment"># in word lengths for a selection of languages included in the udhr corpus</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> udhr</span><br><span class="line">languages = [<span class="string">'Chickasaw'</span>, <span class="string">'English'</span>, <span class="string">'German_Deutsch'</span>,</span><br><span class="line">	<span class="string">'Greenlandic_Inuktikut'</span>, <span class="string">'Hungarian_Magyar'</span>, <span class="string">'Ibibio_Efik'</span>]</span><br><span class="line">cfd = nltk.ConditionalFreqDist(</span><br><span class="line">	(lang, len(word))</span><br><span class="line">	<span class="keyword">for</span> lang <span class="keyword">in</span> languages</span><br><span class="line">	<span class="keyword">for</span> word <span class="keyword">in</span> udhr.words(lang + <span class="string">'-Latin1'</span>))</span><br><span class="line">cfd.plot(cumulative=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p><img src="http://www.nltk.org/images/word-len-dist.png" alt="Cumulative Word Length Distributions: Six translations of the Universal Declaration of Human Rights are processed; this graph shows that words having 5 or fewer letters account for about 80% of Ibibio text, 60% of German text, and 25% of Inuktitut text."></p>
<p><strong>Text Corpus Structure</strong><br>
<img src="http://www.nltk.org/images/text-corpus-structure.png" alt="Common Structures for Text Corpora: The simplest kind of corpus is a collection of isolated texts with no particular organization; some corpora are structured into categories like genre (Brown Corpus); some categorizations overlap, such as topic categories (Reuters Corpus); other corpora represent language use over time (Inaugural Address Corpus)."></p>
<h4 id="corpus-functionality"><a class="markdownIt-Anchor" href="#corpus-functionality"></a> Corpus Functionality</h4>
<p><strong>Basic Corpus Functionality defined in NLTK</strong> [<a href="http://nltk.org/howto" target="_blank" rel="external">More</a>]</p>
<table>
<thead>
<tr>
<th>Example</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>fileids()</td>
<td>the files of the corpus</td>
</tr>
<tr>
<td>fileids([categories])</td>
<td>the files of the corpus corresponding to these categories</td>
</tr>
<tr>
<td>categories()</td>
<td>the categories of the corpus</td>
</tr>
<tr>
<td>categories([fileids])</td>
<td>the categories of the corpus corresponding to these files</td>
</tr>
<tr>
<td>raw()</td>
<td>the raw content of the corpus</td>
</tr>
<tr>
<td>raw(fileids=[f1,f2,f3])</td>
<td>the raw content of the specified files</td>
</tr>
<tr>
<td>raw(categories=[c1,c2])</td>
<td>the raw content of the specified categories</td>
</tr>
<tr>
<td>words()</td>
<td>the words of the whole corpus</td>
</tr>
<tr>
<td>words(fileids=[f1,f2,f3])</td>
<td>the words of the specified fileids</td>
</tr>
<tr>
<td>words(categories=[c1,c2])</td>
<td>the words of the specified categories</td>
</tr>
<tr>
<td>sents()</td>
<td>the sentences of the whole corpus</td>
</tr>
<tr>
<td>sents(fileids=[f1,f2,f3])</td>
<td>the sentences of the specified fileids</td>
</tr>
<tr>
<td>sents(categories=[c1,c2])</td>
<td>the sentences of the specified categories</td>
</tr>
<tr>
<td>abspath(fileid)</td>
<td>the location of the given file on disk</td>
</tr>
<tr>
<td>encoding(fileid)</td>
<td>the encoding of the file (if known)</td>
</tr>
<tr>
<td>open(fileid)</td>
<td>open a stream for reading the given corpus file</td>
</tr>
<tr>
<td>root</td>
<td>if the path to the root of locally installed corpus</td>
</tr>
<tr>
<td>readme()</td>
<td>the contents of the README file of the corpus</td>
</tr>
</tbody>
</table>
<p>corpus readers can be used to work with new corpora.</p>
<h4 id="loading-your-own-corpus"><a class="markdownIt-Anchor" href="#loading-your-own-corpus"></a> Loading your own Corpus</h4>
<p>Use NLTK’s PlaintextCorpusReader.<br>
Set file location as the value of corpus_root.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> PlaintextCorpusReader</span><br><span class="line">corpus_root = <span class="string">'/usr/share/dict'</span></span><br><span class="line">wordlists = PlaintextCorpusReader(corpus_root, <span class="string">'[abc]/.*\.txt'</span>)</span><br><span class="line">wordlists.fileids()</span><br><span class="line">wordlists.words(<span class="string">'sample1'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> BracketParseCorpusReader</span><br><span class="line">corpus_root = <span class="string">r"C:\corpora\penntreebank\parsed\mrg\wsj"</span></span><br><span class="line">file_pattern = <span class="string">r".*/wsj_.*\.mrg"</span></span><br><span class="line">ptb = BracketParseCorpusReader(corpus_root, file_pattern)</span><br><span class="line">ptb.fileids()</span><br><span class="line">ptb.sents(fileids=<span class="string">'20/wsj_2013.mrg'</span>)[<span class="number">19</span>]</span><br></pre></td></tr></table></figure>
<h3 id="conditional-frequency-distributions"><a class="markdownIt-Anchor" href="#conditional-frequency-distributions"></a> Conditional Frequency Distributions</h3>
<p><strong>Conditions and Events</strong><br>
A conditional frequency distribution needs to pair each event with a condition, so we will process a sequence of pairs with the form (condition, event).</p>
<p><strong>Counting Words by Genre</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> brown</span><br><span class="line">cfd = nltk.ConditionalFreqDist(</span><br><span class="line">	(genre, word)</span><br><span class="line">	<span class="keyword">for</span> genre <span class="keyword">in</span> brown.categories()</span><br><span class="line">	<span class="keyword">for</span> word <span class="keyword">in</span> brown.words(categories=genre))</span><br><span class="line">cfd.conditions()</span><br><span class="line">cfd[<span class="string">'romance'</span>].most_common(<span class="number">20</span>)</span><br><span class="line">cfd[<span class="string">'romance'</span>][<span class="string">'could'</span>]</span><br></pre></td></tr></table></figure>
<p><strong>Plotting and Tabulating Distributions</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前文america和citizen在不同年份词频图</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> inaugural</span><br><span class="line">cfd = nltk.ConditionalFreqDist(</span><br><span class="line">	(target, fileid[:<span class="number">4</span>])</span><br><span class="line">	<span class="keyword">for</span> fileid <span class="keyword">in</span> inaugural.fileids()</span><br><span class="line">	<span class="keyword">for</span> w <span class="keyword">in</span> inaugural.words(fileid)</span><br><span class="line">	<span class="keyword">for</span> target <span class="keyword">in</span> [<span class="string">'america'</span>, <span class="string">'citizen'</span>]</span><br><span class="line">	<span class="keyword">if</span> w.lower().startswith(target))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前文不同语言词汇数图</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> udhr</span><br><span class="line">languages = [<span class="string">'Chickasaw'</span>, <span class="string">'English'</span>, <span class="string">'German_Deutsch'</span>, <span class="string">'Greenlandic_Inuktikut'</span>, <span class="string">'Hungarian_Magyar'</span>, <span class="string">'Ibibio_Efik'</span>]</span><br><span class="line">cfd = nltk.ConditionalFreqDist(</span><br><span class="line">	(lang, len(word))</span><br><span class="line">	<span class="keyword">for</span> lang <span class="keyword">in</span> languages</span><br><span class="line">	<span class="keyword">for</span> word <span class="keyword">in</span> udhr.words(lang + <span class="string">'-Latin1'</span>))</span><br><span class="line">cfd.tabulate(conditions=[<span class="string">'English'</span>, <span class="string">'German_Deutsch'</span>], samples=range(<span class="number">10</span>), cumulative=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>还可以看前文Brown Corpus那里的例子。</p>
<p><strong>Generating Random Text with Bigrams</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_model</span><span class="params">(cfdist, word, num=<span class="number">15</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">        print(word, end=<span class="string">' '</span>)</span><br><span class="line">        word = cfdist[word].max()</span><br><span class="line">text = nltk.corpus.genesis.words(<span class="string">'english-kjv.txt'</span>)</span><br><span class="line">bigrams = nltk.bigrams(text)</span><br><span class="line">cfd = nltk.ConditionalFreqDist(bigrams)</span><br></pre></td></tr></table></figure>
<p><strong>NLTK’s Conditional Frequency Distributions</strong><br>
commonly-used methods and idioms for defining, accessing, and visualizing a conditional frequency distribution of counters.</p>
<table>
<thead>
<tr>
<th>Example</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>cfdist = ConditionalFreqDist(pairs)</td>
<td>create a conditional frequency distribution from a list of pairs</td>
</tr>
<tr>
<td>cfdist.conditions()</td>
<td>the conditions</td>
</tr>
<tr>
<td>cfdist[condition]</td>
<td>the frequency distribution for this condition</td>
</tr>
<tr>
<td>cfdist[condition][sample]</td>
<td>frequency for the given sample for this condition</td>
</tr>
<tr>
<td>cfdist.tabulate()</td>
<td>tabulate the conditional frequency distribution</td>
</tr>
<tr>
<td>cfdist.tabulate(samples, conditions)</td>
<td>tabulation limited to the specified samples and conditions</td>
</tr>
<tr>
<td>cfdist.plot()</td>
<td>graphical plot of the conditional frequency distribution</td>
</tr>
<tr>
<td>cfdist.plot(samples, conditions)</td>
<td>graphical plot limited to the specified samples and conditions</td>
</tr>
<tr>
<td>cfdist1 &lt; cfdist2</td>
<td>test if samples in cfdist1 occur less frequently than in cfdist2</td>
</tr>
</tbody>
</table>
<h3 id="more-python-reusing-code"><a class="markdownIt-Anchor" href="#more-python-reusing-code"></a> More Python: Reusing Code</h3>
<p>text editors and Python functions.</p>
<p><strong>Creating Programs with a Text Editor</strong><br>
IDLE -&gt; File Menu -&gt; New File<br>
Run Menu -&gt; Run Module or <code>from test import *</code></p>
<p><strong>Functions</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plural</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> word.endswith(<span class="string">'y'</span>):</span><br><span class="line">        <span class="keyword">return</span> word[:<span class="number">-1</span>] + <span class="string">'ies'</span></span><br><span class="line">    <span class="keyword">elif</span> word[<span class="number">-1</span>] <span class="keyword">in</span> <span class="string">'sx'</span> <span class="keyword">or</span> word[<span class="number">-2</span>:] <span class="keyword">in</span> [<span class="string">'sh'</span>, <span class="string">'ch'</span>]:</span><br><span class="line">        <span class="keyword">return</span> word + <span class="string">'es'</span></span><br><span class="line">    <span class="keyword">elif</span> word.endswith(<span class="string">'an'</span>):</span><br><span class="line">        <span class="keyword">return</span> word[:<span class="number">-2</span>] + <span class="string">'en'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> word + <span class="string">'s'</span></span><br><span class="line">plural(<span class="string">'fairy'</span>)</span><br><span class="line">plural(<span class="string">'woman'</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Modules</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> text_proc <span class="keyword">import</span> plural</span><br><span class="line">plural(<span class="string">'wish'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="lexical-resources"><a class="markdownIt-Anchor" href="#lexical-resources"></a> Lexical Resources</h3>
<p><strong>Lexicon(or Lexical Resource)</strong>:  a collection of words and/or phrases along with associated information such as part of speech and sense definitions.<br>
A <strong>lexical entry</strong> consists of a <strong>headword</strong> (also known as a <strong>lemma</strong>) along with additional information such as the part of speech and the sense definition. Two distinct words having the same spelling are called <strong>homonyms</strong>.<br>
<img src="http://www.nltk.org/images/lexicon.png" alt="Lexicon Terminology: lexical entries for two lemmas having the same spelling (homonyms), providing part of speech and gloss information."></p>
<p><strong>Wordlist Corpora</strong><br>
nothing more than wordlist. The Words Corpus is the /usr/share/dict/words file from Unix, used by some spell checkers.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Filtering a Text: </span></span><br><span class="line"><span class="comment"># computes the vocabulary of a text, then removes all items that occur in an existing wordlist, </span></span><br><span class="line"><span class="comment"># leaving just the uncommon or mis-spelt words.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unusual_words</span><span class="params">(text)</span>:</span></span><br><span class="line">    text_vocab = set(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> text <span class="keyword">if</span> w.isalpha())</span><br><span class="line">    english_vocab = set(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> nltk.corpus.words.words())</span><br><span class="line">    unusual = text_vocab - english_vocab</span><br><span class="line">    <span class="keyword">return</span> sorted(unusual)</span><br><span class="line">unusual_words(nltk.corpus.gutenberg.words(<span class="string">'austen-sense.txt'</span>))</span><br><span class="line">unusual_words(nltk.corpus.nps_chat.words())</span><br></pre></td></tr></table></figure>
<p>stopwords: high-frequency words like the, to and also that we sometimes want to filter out of a document before further processing.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">content_fraction</span><span class="params">(text)</span>:</span></span><br><span class="line">	stopwords = nltk.corpus.stopwords.words(<span class="string">'english'</span>)</span><br><span class="line">	content = [w <span class="keyword">for</span> w <span class="keyword">in</span> text <span class="keyword">if</span> w.lower() <span class="keyword">not</span> <span class="keyword">in</span> stopwords]</span><br><span class="line">	<span class="keyword">return</span> len(content) / len(text)</span><br><span class="line">content_fraction(nltk.corpus.reuters.words())</span><br></pre></td></tr></table></figure>
<p>Example:<br>
<img src="http://www.nltk.org/images/target.png" alt="A Word Puzzle: a grid of randomly chosen letters with rules for creating words out of the letters; this puzzle is known as &quot;Target.&quot;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The FreqDist comparison method permits us to check that the frequency of each letter in the candidate word </span></span><br><span class="line"><span class="comment"># is less than or equal to the frequency of the corresponding letter in the puzzle.</span></span><br><span class="line">puzzle_letters = nltk.FreqDist(<span class="string">'egivrvonl'</span>)</span><br><span class="line">obligatory = <span class="string">'r'</span></span><br><span class="line">wordlist = nltk.corpus.words.words()</span><br><span class="line">[w <span class="keyword">for</span> w <span class="keyword">in</span> wordlist <span class="keyword">if</span> len(w) &gt;= <span class="number">6</span> [<span class="number">1</span>]</span><br><span class="line">		<span class="keyword">and</span> obligatory <span class="keyword">in</span> w [<span class="number">2</span>]</span><br><span class="line">		<span class="keyword">and</span> nltk.FreqDist(w) &lt;= puzzle_letters]</span><br></pre></td></tr></table></figure>
<p>Names ending in the letter a are almost always female.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> names</span><br><span class="line">cfd = nltk.ConditionalFreqDist(</span><br><span class="line">	(fileid, name[<span class="number">-1</span>])</span><br><span class="line">	<span class="keyword">for</span> fileid <span class="keyword">in</span> names.fileids()</span><br><span class="line">	<span class="keyword">for</span> name <span class="keyword">in</span> names.words(fileid))</span><br><span class="line">cfd.plot()</span><br></pre></td></tr></table></figure>
<p><img src="http://www.nltk.org/images/cfd-gender.png" alt="Conditional Frequency Distribution: this plot shows the number of female and male names ending with each letter of the alphabet; most names ending with a, e or i are female; names ending in h and l are equally likely to be male or female; names ending in k, o, r, s, and t are likely to be male."></p>
<p><strong>A Pronouncing Dictionary</strong><br>
spreadsheet containing a word plus some properties in each row.<br>
CMU Pronouncing Dictionary for US English, which was designed for use by speech synthesizers. 单词和其发音，如<code>('fireball', ['F', 'AY1', 'ER0', 'B', 'AO2', 'L'])</code>，数字表示重音。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">entries = nltk.corpus.cmudict.entries()</span><br><span class="line"><span class="keyword">for</span> word, pron <span class="keyword">in</span> entries:</span><br><span class="line">	<span class="keyword">if</span> len(pron) == <span class="number">3</span>:</span><br><span class="line">		ph1, ph2, ph3 = pron</span><br><span class="line">		<span class="keyword">if</span> ph1 == <span class="string">'P'</span> <span class="keyword">and</span> ph3 == <span class="string">'T'</span>:</span><br><span class="line">			print(word, ph2, end=<span class="string">' '</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># finds all words whose pronunciation ends with a syllable sounding like nicks.</span></span><br><span class="line">syllable = [<span class="string">'N'</span>, <span class="string">'IH0'</span>, <span class="string">'K'</span>, <span class="string">'S'</span>]</span><br><span class="line">[word <span class="keyword">for</span> word, pron <span class="keyword">in</span> entries <span class="keyword">if</span> pron[<span class="number">-4</span>:] == syllable]</span><br><span class="line">[w <span class="keyword">for</span> w, pron <span class="keyword">in</span> entries <span class="keyword">if</span> pron[<span class="number">-1</span>] == <span class="string">'M'</span> <span class="keyword">and</span> w[<span class="number">-1</span>] == <span class="string">'n'</span>]</span><br><span class="line">sorted(set(w[:<span class="number">2</span>] <span class="keyword">for</span> w, pron <span class="keyword">in</span> entries <span class="keyword">if</span> pron[<span class="number">0</span>] == <span class="string">'N'</span> <span class="keyword">and</span> w[<span class="number">0</span>] != <span class="string">'n'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># find words having a particular stress pattern.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stress</span><span class="params">(pron)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> [char <span class="keyword">for</span> phone <span class="keyword">in</span> pron <span class="keyword">for</span> char <span class="keyword">in</span> phone <span class="keyword">if</span> char.isdigit()]</span><br><span class="line">[w <span class="keyword">for</span> w, pron <span class="keyword">in</span> entries <span class="keyword">if</span> stress(pron) == [<span class="string">'0'</span>, <span class="string">'1'</span>, <span class="string">'0'</span>, <span class="string">'2'</span>, <span class="string">'0'</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># find all the p-words consisting of three sounds, and group them according to their first and last sounds.</span></span><br><span class="line">p3 = [(pron[<span class="number">0</span>]+<span class="string">'-'</span>+pron[<span class="number">2</span>], word)</span><br><span class="line">	<span class="keyword">for</span> (word, pron) <span class="keyword">in</span> entries</span><br><span class="line">	<span class="keyword">if</span> pron[<span class="number">0</span>] == <span class="string">'P'</span> <span class="keyword">and</span> len(pron) == <span class="number">3</span>]</span><br><span class="line">cfd = nltk.ConditionalFreqDist(p3)</span><br><span class="line"><span class="keyword">for</span> template <span class="keyword">in</span> sorted(cfd.conditions()):</span><br><span class="line">	<span class="keyword">if</span> len(cfd[template]) &gt; <span class="number">10</span>:</span><br><span class="line">		words = sorted(cfd[template])</span><br><span class="line">		wordstring = <span class="string">' '</span>.join(words)</span><br><span class="line">		print(template, wordstring[:<span class="number">70</span>] + <span class="string">"..."</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">prondict = nltk.corpus.cmudict.dict()</span><br><span class="line">prondict[<span class="string">'fire'</span>]</span><br><span class="line"><span class="comment"># [['F', 'AY1', 'ER0'], ['F', 'AY1', 'R']]</span></span><br></pre></td></tr></table></figure>
<p><strong>Comparative Wordlists</strong><br>
Swadesh wordlists consist of of about 200 common words in several languages. The languages are identified using an ISO 639 two-letter code.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> swadesh</span><br><span class="line">swadesh.fileids()</span><br><span class="line">swadesh.words(<span class="string">'en'</span>)</span><br><span class="line">fr2en = swadesh.entries([<span class="string">'fr'</span>, <span class="string">'en'</span>])	<span class="comment"># 法英对照list</span></span><br><span class="line">translate = dict(fr2en)</span><br><span class="line">translate[<span class="string">'chien'</span>]</span><br><span class="line">de2en = swadesh.entries([<span class="string">'de'</span>, <span class="string">'en'</span>])</span><br><span class="line">translate.update(dict(de2en))</span><br><span class="line">translate[<span class="string">'Hund'</span>]</span><br></pre></td></tr></table></figure>
<p><strong>Shoebox and Toolbox Lexicons</strong><br>
A Toolbox file consists of a collection of entries, where each entry is made up of one or more fields. loose structured.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> toolbox</span><br><span class="line">toolbox.entries(<span class="string">'rotokas.dic'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="wordnet"><a class="markdownIt-Anchor" href="#wordnet"></a> WordNet</h3>
<p>WordNet is a semantically-oriented dictionary of English, similar to a traditional thesaurus but with a richer structure.</p>
<p><strong>Senses and Synonyms</strong><br>
The entity car.n.01 is called a <strong>synset</strong>, or “synonym set”, a collection of synonymous words (or “<strong>lemmas</strong>”):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> wordnet <span class="keyword">as</span> wn</span><br><span class="line">wn.synsets(<span class="string">'motorcar'</span>)	<span class="comment"># 有几种释义</span></span><br><span class="line">wn.synset(<span class="string">'car.n.01'</span>).lemma_names()	<span class="comment"># 此释义下的所有同义词</span></span><br><span class="line">wn.synset(<span class="string">'car.n.01'</span>).definition()	<span class="comment"># 释义</span></span><br><span class="line">wn.synset(<span class="string">'car.n.01'</span>).examples()	<span class="comment"># 例句</span></span><br><span class="line">wn.synset(<span class="string">'car.n.01'</span>).lemmas()</span><br><span class="line">wn.lemma(<span class="string">'car.n.01.automobile'</span>).synset()</span><br><span class="line">wn.lemma(<span class="string">'car.n.01.automobile'</span>).name()</span><br></pre></td></tr></table></figure>
<p><strong>The WordNet Hierarchy</strong><br>
<img src="http://www.nltk.org/images/wordnet-hierarchy.png" alt="Fragment of WordNet Concept Hierarchy: nodes correspond to synsets; edges indicate the hypernym/hyponym relation, i.e. the relation between superordinate and subordinate concepts."></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">motorcar = wn.synset(<span class="string">'car.n.01'</span>)</span><br><span class="line">types_of_motorcar = motorcar.hyponyms()</span><br><span class="line">sorted(lemma.name() <span class="keyword">for</span> synset <span class="keyword">in</span> types_of_motorcar <span class="keyword">for</span> lemma <span class="keyword">in</span> synset.lemmas())</span><br><span class="line">motorcar.hypernyms()</span><br><span class="line">paths = motorcar.hypernym_paths()</span><br><span class="line">len(paths)</span><br><span class="line">[synset.name() <span class="keyword">for</span> synset <span class="keyword">in</span> paths[<span class="number">0</span>]]</span><br><span class="line">motorcar.root_hypernyms()</span><br></pre></td></tr></table></figure>
<p><strong>More Lexical Relations</strong><br>
from items to their components (<strong>meronyms</strong>) or to the things they are contained in (<strong>holonyms</strong>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wn.synset(<span class="string">'tree.n.01'</span>).part_meronyms()</span><br><span class="line">wn.synset(<span class="string">'tree.n.01'</span>).substance_meronyms()</span><br><span class="line">wn.synset(<span class="string">'tree.n.01'</span>).member_holonyms()</span><br></pre></td></tr></table></figure>
<p>the act of walking involves the act of stepping, so walking <strong>entails</strong> stepping.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wn.synset(<span class="string">'eat.v.01'</span>).entailments()</span><br></pre></td></tr></table></figure>
<p>Some lexical relationships hold between lemmas, e.g., <strong>antonymy</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wn.lemma(<span class="string">'supply.n.02.supply'</span>).antonyms()</span><br></pre></td></tr></table></figure>
<p>查看synset的lexical relation可以用<code>dir(wn.synset('harmony.n.02'))</code>。</p>
<p><strong>Semantic Similarity</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">right = wn.synset(<span class="string">'right_whale.n.01'</span>)</span><br><span class="line">minke = wn.synset(<span class="string">'minke_whale.n.01'</span>)</span><br><span class="line">right.lowest_common_hypernyms(minke)</span><br><span class="line"> wn.synset(<span class="string">'baleen_whale.n.01'</span>).min_depth()</span><br><span class="line">right.path_similarity(minke)</span><br></pre></td></tr></table></figure>
<p>查看帮助<code>help(wn)</code>。另一个相似的verb lexicon是<code>nltk.corpus.verbnet</code>。</p>
<hr>
<h2 id="processing-raw-text"><a class="markdownIt-Anchor" href="#processing-raw-text"></a> Processing Raw Text</h2>
<p>本章要解决的问题：</p>
<ol>
<li>How can we write programs to access text from local files and from the web, in order to get hold of an unlimited range of language material?</li>
<li>How can we split documents up into individual words and punctuation symbols, so we can carry out the same kinds of analysis we did with text corpora in earlier chapters?</li>
<li>How can we write programs to produce formatted output and save it in a file?</li>
</ol>
<p>从本章起，代码执行前先：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk, re, pprint</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> word_tokenize</span><br></pre></td></tr></table></figure>
<h3 id="accessing-text-from-the-web-and-from-disk"><a class="markdownIt-Anchor" href="#accessing-text-from-the-web-and-from-disk"></a> Accessing Text from the Web and from Disk</h3>
<p><strong>Electronic Books</strong><br>
<a href="http://www.gutenberg.org/catalog/%E4%B8%8A%E6%9C%8925000%E6%9C%AC%E4%B9%A6%EF%BC%8C%E5%A4%9A%E8%AF%AD%E7%A7%8D%E7%9A%84%E3%80%82" target="_blank" rel="external">http://www.gutenberg.org/catalog/上有25000本书，多语种的。</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Access number 2554 from gutenberg</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line">url = <span class="string">"http://www.gutenberg.org/files/2554/2554.txt"</span></span><br><span class="line">response = request.urlopen(url)</span><br><span class="line">raw = response.read().decode(<span class="string">'utf8'</span>)	<span class="comment"># a string</span></span><br><span class="line">raw[:<span class="number">75</span>]</span><br></pre></td></tr></table></figure>
<p>如果开了代理，要用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">proxies = &#123;<span class="string">'http'</span>: <span class="string">'http://www.someproxy.com:3128'</span>&#125;</span><br><span class="line">request.ProxyHandler(proxies)</span><br></pre></td></tr></table></figure>
<p>Tokenization: break up the string into words and punctuation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tokenization</span></span><br><span class="line">tokens = word_tokenize(raw)</span><br><span class="line">tokens[:<span class="number">10</span>]</span><br><span class="line"><span class="comment"># other nltk operations</span></span><br><span class="line">text = nltk.Text(tokens)</span><br><span class="line">text[<span class="number">1024</span>:<span class="number">1062</span>]</span><br><span class="line">text.collocations()</span><br></pre></td></tr></table></figure>
<p><strong>Deal with HTML</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># original web page</span></span><br><span class="line">url = <span class="string">"http://news.bbc.co.uk/2/hi/health/2284783.stm"</span></span><br><span class="line">html = request.urlopen(url).read().decode(<span class="string">'utf8'</span>)</span><br><span class="line">html[:<span class="number">60</span>]</span><br><span class="line"><span class="comment"># extract text out of html</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">raw = BeautifulSoup(html).get_text()</span><br><span class="line">tokens = word_tokenize(raw)</span><br><span class="line">tokens</span><br><span class="line"><span class="comment"># manually find start and end of the text</span></span><br><span class="line">tokens = tokens[<span class="number">110</span>:<span class="number">390</span>]</span><br><span class="line">text = nltk.Text(tokens)</span><br><span class="line">text.concordance(<span class="string">'gene'</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Processing Search Engine Results</strong><br>
Massive yet unstable.</p>
<p><strong>Processing RSS Feeds</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> feedparser</span><br><span class="line">llog = feedparser.parse(<span class="string">"http://languagelog.ldc.upenn.edu/nll/?feed=atom"</span>)</span><br><span class="line">llog[<span class="string">'feed'</span>][<span class="string">'title'</span>]</span><br><span class="line">len(llog.entries)</span><br><span class="line">post = llog.entries[<span class="number">2</span>]</span><br><span class="line">post.title</span><br><span class="line">content = post.content[<span class="number">0</span>].value</span><br><span class="line">content[:<span class="number">70</span>]</span><br><span class="line">raw = BeautifulSoup(content).get_text()</span><br><span class="line">word_tokenize(raw)</span><br></pre></td></tr></table></figure>
<p><strong>Reading Local Files</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># read whole text</span></span><br><span class="line">f = open(<span class="string">'document.txt'</span>)</span><br><span class="line">raw = f.read()</span><br><span class="line"><span class="comment"># read line by line</span></span><br><span class="line">f = open(<span class="string">'document.txt'</span>, <span class="string">'rU'</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">	print(line.strip())</span><br><span class="line"><span class="comment"># read file of nltk</span></span><br><span class="line">path = nltk.data.find(<span class="string">'corpora/gutenberg/melville-moby_dick.txt'</span>)</span><br><span class="line">raw = open(path, <span class="string">'rU'</span>).read()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.listdir(<span class="string">'.'</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Extracting Text from PDF, MSWord and Other Binary Formats</strong><br>
Use thirdparty libraries like pypdf and pywin32.<br>
Better save is as a txt file manually. Or if the file is on the web, use search engine’s HTML version of the document.</p>
<p><strong>Capturing User Input</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = input(<span class="string">"Enter some text: "</span>)</span><br><span class="line">print(<span class="string">"You typed"</span>, len(word_tokenize(s)), <span class="string">"words."</span>)</span><br></pre></td></tr></table></figure>
<p><strong>The NLP Pipiline</strong><br>
<img src="http://www.nltk.org/images/pipeline1.png" alt="The Processing Pipeline: We open a URL and read its HTML content, remove the markup and select a slice of characters; this is then tokenized and optionally converted into an nltk.Text object; we can also lowercase all the words and extract the vocabulary."></p>
<h3 id="strings-text-processing-at-the-lowest-level"><a class="markdownIt-Anchor" href="#strings-text-processing-at-the-lowest-level"></a> Strings: Text Processing at the Lowest Level</h3>
<p>Strings are immutable, Lists are mutable.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">couplet = <span class="string">"Shall I compare thee to a Summer's day?"</span>\</span><br><span class="line">	<span class="string">"Thou are more lovely and more temperate:"</span>	<span class="comment"># 一个string</span></span><br><span class="line">couplet = <span class="string">"""Shall I compare thee to a Summer's day?</span><br><span class="line">	Thou are more lovely and more temperate:"""</span>	<span class="comment"># 一个string并保留换行符</span></span><br><span class="line">print(monty, <span class="string">"and the"</span>, grail)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> gutenberg</span><br><span class="line">raw = gutenberg.raw(<span class="string">'melville-moby_dick.txt'</span>)</span><br><span class="line">fdist = nltk.FreqDist(ch.lower() <span class="keyword">for</span> ch <span class="keyword">in</span> raw <span class="keyword">if</span> ch.isalpha())</span><br><span class="line">fdist.most_common(<span class="number">5</span>)</span><br><span class="line">[char <span class="keyword">for</span> (char, count) <span class="keyword">in</span> fdist.most_common()]</span><br></pre></td></tr></table></figure>
<p><strong>Useful String Methods:</strong></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Functionality</th>
</tr>
</thead>
<tbody>
<tr>
<td>s.find(t)</td>
<td>index of first instance of string t inside s (-1 if not found)</td>
</tr>
<tr>
<td>s.rfind(t)</td>
<td>index of last instance of string t inside s (-1 if not found)</td>
</tr>
<tr>
<td>s.index(t)</td>
<td>like s.find(t) except it raises ValueError if not found</td>
</tr>
<tr>
<td>s.rindex(t)</td>
<td>like s.rfind(t) except it raises ValueError if not found</td>
</tr>
<tr>
<td>s.join(text)</td>
<td>combine the words of the text into a string using s as the glue</td>
</tr>
<tr>
<td>s.split(t)</td>
<td>split s into a list wherever a t is found (whitespace by default)</td>
</tr>
<tr>
<td>s.splitlines()</td>
<td>split s into a list of strings, one per line</td>
</tr>
<tr>
<td>s.lower()</td>
<td>a lowercased version of the string s</td>
</tr>
<tr>
<td>s.upper()</td>
<td>an uppercased version of the string s</td>
</tr>
<tr>
<td>s.title()</td>
<td>a titlecased version of the string s</td>
</tr>
<tr>
<td>s.strip()</td>
<td>a copy of s without leading or trailing whitespace</td>
</tr>
<tr>
<td>s.replace(t, u)</td>
<td>replace instances of t with u inside s</td>
</tr>
</tbody>
</table>
<h3 id="text-processing-with-unicode"><a class="markdownIt-Anchor" href="#text-processing-with-unicode"></a> Text Processing with Unicode</h3>
<p><strong>What is Unicode?</strong><br>
In Python, <strong>code points</strong> are written in the form \uXXXX, where XXXX is the number in 4-digit hexadecimal form.<br>
<img src="http://www.nltk.org/images/unicode.png" alt="Unicode Decoding and Encoding"><br>
From a Unicode perspective, characters are abstract entities which can be realized as one or more glyphs. Only glyphs can appear on a screen or be printed on paper. A font is a mapping from characters to glyphs.</p>
<p><strong>Extracting encoded text from files</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">path = nltk.data.find(<span class="string">'corpora/unicode_samples/polish-lat2.txt'</span>)</span><br><span class="line">f = open(path, encoding=<span class="string">'latin2'</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">	line = line.strip()</span><br><span class="line">	print(line)</span><br><span class="line">	print(line.encode(<span class="string">'unicode_escape'</span>))	<span class="comment"># 显示codepoint</span></span><br></pre></td></tr></table></figure>
<p>In Python 3, source code is encoded using UTF-8 by default.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ord(<span class="string">'ń'</span>)	<span class="comment"># 324</span></span><br><span class="line">hex(<span class="number">324</span>)	<span class="comment"># 0144</span></span><br><span class="line">nacute = <span class="string">'\u0144'</span>	<span class="comment"># ń</span></span><br><span class="line">print(nacute.encode(<span class="string">'utf8'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line">lines = open(path, encoding=<span class="string">'latin2'</span>).readlines()</span><br><span class="line">line = lines[<span class="number">2</span>]</span><br><span class="line">print(line.encode(<span class="string">'unicode_escape'</span>))</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> line: [<span class="number">1</span>]</span><br><span class="line">	<span class="keyword">if</span> ord(c) &gt; <span class="number">127</span>:</span><br><span class="line">	print(<span class="string">'&#123;&#125; U+&#123;:04x&#125; &#123;&#125;'</span>.format(c.encode(<span class="string">'utf8'</span>), ord(c), unicodedata.name(c)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># re dealing with unicode</span></span><br><span class="line">line.find(<span class="string">'zosta\u0142y'</span>)</span><br><span class="line">line = line.lower()</span><br><span class="line">line.encode(<span class="string">'unicode_escape'</span>)</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">m = re.search(<span class="string">'\u015b\w*'</span>, line)</span><br><span class="line">m.group()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NLTK tokenizers allow Unicode</span></span><br><span class="line">word_tokenize(line)</span><br></pre></td></tr></table></figure>
<p><strong>Using your local encoding in Python</strong><br>
include the string <code># -*- coding: &lt;coding&gt; -*-</code> as the first or second line of your file. Note that <coding> has to be a string like ‘latin-1’, ‘big5’ or ‘utf-8’.<br>
<img src="http://www.nltk.org/images/polish-utf8.png" alt="Unicode and IDLE: UTF-8 encoded string literals in the IDLE editor; this requires that an appropriate font is set in IDLE's preferences; here we have chosen Courier CE."></coding></p>
<h3 id="regular-expressions-for-detecting-word-patterns"><a class="markdownIt-Anchor" href="#regular-expressions-for-detecting-word-patterns"></a> Regular Expressions for Detecting Word Patterns</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">wordlist = [w <span class="keyword">for</span> w <span class="keyword">in</span> nltk.corpus.words.words(<span class="string">'en'</span>) <span class="keyword">if</span> w.islower()]</span><br><span class="line"><span class="comment"># find words ending with ed</span></span><br><span class="line">[w <span class="keyword">for</span> w <span class="keyword">in</span> wordlist <span class="keyword">if</span> re.search(<span class="string">'ed$'</span>, w)]</span><br><span class="line"><span class="comment"># find an 8-letter word with j as its third letter and t as its sixth letter</span></span><br><span class="line">[w <span class="keyword">for</span> w <span class="keyword">in</span> wordlist <span class="keyword">if</span> re.search(<span class="string">'^..j..t..$'</span>, w)]</span><br><span class="line"><span class="comment"># what words can be produced using 4 words of each bucket</span></span><br><span class="line">[w <span class="keyword">for</span> w <span class="keyword">in</span> wordlist <span class="keyword">if</span> re.search(<span class="string">'^[ghi][mno][jlk][def]$'</span>, w)]</span><br><span class="line"><span class="comment"># + means one or more instances of the preceding item</span></span><br><span class="line">chat_words = sorted(set(w <span class="keyword">for</span> w <span class="keyword">in</span> nltk.corpus.nps_chat.words()))</span><br><span class="line">[w <span class="keyword">for</span> w <span class="keyword">in</span> chat_words <span class="keyword">if</span> re.search(<span class="string">'^m+i+n+e+$'</span>, w)]</span><br><span class="line">[w <span class="keyword">for</span> w <span class="keyword">in</span> chat_words <span class="keyword">if</span> re.search(<span class="string">'^[ha]+$'</span>, w)]</span><br><span class="line"><span class="comment"># use of \ &#123;&#125; () |</span></span><br><span class="line">wsj = sorted(set(nltk.corpus.treebank.words()))</span><br><span class="line">[w <span class="keyword">for</span> w <span class="keyword">in</span> wsj <span class="keyword">if</span> re.search(<span class="string">'^[a-z]&#123;5,&#125;-[a-z]&#123;2,3&#125;-[a-z]&#123;,6&#125;$'</span>, w)]</span><br><span class="line">[w <span class="keyword">for</span> w <span class="keyword">in</span> wsj <span class="keyword">if</span> re.search(<span class="string">'(ed|ing)$'</span>, w)]</span><br></pre></td></tr></table></figure>
<p><strong>Basic Regular Expression Meta-Characters</strong>, Including Wildcards, Ranges and Closures</p>
<table>
<thead>
<tr>
<th>Operator</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr>
<td>.</td>
<td>Wildcard, matches any character</td>
</tr>
<tr>
<td>^abc</td>
<td>Matches some pattern abc at the start of a string</td>
</tr>
<tr>
<td>abc$</td>
<td>Matches some pattern abc at the end of a string</td>
</tr>
<tr>
<td>[abc]</td>
<td>Matches one of a set of characters</td>
</tr>
<tr>
<td>[A-Z0-9]</td>
<td>Matches one of a range of characters</td>
</tr>
<tr>
<td>ed¦ing¦s</td>
<td>Matches one of the specified strings (disjunction)</td>
</tr>
<tr>
<td>*</td>
<td>Zero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure)</td>
</tr>
<tr>
<td>+</td>
<td>One or more of previous item, e.g. a+, [a-z]+</td>
</tr>
<tr>
<td>?</td>
<td>Zero or one of the previous item (i.e. optional), e.g. a?, [a-z]?</td>
</tr>
<tr>
<td>{n}</td>
<td>Exactly n repeats where n is a non-negative integer</td>
</tr>
<tr>
<td>{n,}</td>
<td>At least n repeats</td>
</tr>
<tr>
<td>{,n}</td>
<td>No more than n repeats</td>
</tr>
<tr>
<td>{m,n}</td>
<td>At least m and no more than n repeats</td>
</tr>
<tr>
<td>a(b¦c)+</td>
<td>Parentheses that indicate the scope of the operators</td>
</tr>
</tbody>
</table>
<h3 id="useful-applications-of-regular-expressions"><a class="markdownIt-Anchor" href="#useful-applications-of-regular-expressions"></a> Useful Applications of Regular Expressions</h3>
<p><strong>Extract Word Pieces</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># find all the vowels in a word, then count them</span></span><br><span class="line">word = <span class="string">'supercalifragilisticexpialidocious'</span></span><br><span class="line">len(re.findall(<span class="string">r'[aeiou]'</span>, word))</span><br><span class="line"><span class="comment"># look for all sequences of two or more vowels in some text, and determine their relative frequency</span></span><br><span class="line">wsj = sorted(set(nltk.corpus.treebank.words()))</span><br><span class="line">fd = nltk.FreqDist(vs <span class="keyword">for</span> word <span class="keyword">in</span> wsj</span><br><span class="line">	<span class="keyword">for</span> vs <span class="keyword">in</span> re.findall(<span class="string">r'[aeiou]&#123;2,&#125;'</span>, word))</span><br><span class="line">fd.most_common(<span class="number">12</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Doing More with Word Pieces</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compress word like 'declaration' -&gt; 'dclrtn'</span></span><br><span class="line">regexp = <span class="string">r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compress</span><span class="params">(word)</span>:</span></span><br><span class="line">	pieces = re.findall(regexp, word)</span><br><span class="line">	<span class="keyword">return</span> <span class="string">''</span>.join(pieces)</span><br><span class="line">english_udhr = nltk.corpus.udhr.words(<span class="string">'English-Latin1'</span>)</span><br><span class="line">print(nltk.tokenwrap(compress(w) <span class="keyword">for</span> w <span class="keyword">in</span> english_udhr[:<span class="number">75</span>]))	<span class="comment"># Unvrsl Dclrtn of Hmn Rghts Prmble...</span></span><br><span class="line"><span class="comment"># extract all consonant-vowel sequences from the words of Rotokas</span></span><br><span class="line">rotokas_words = nltk.corpus.toolbox.words(<span class="string">'rotokas.dic'</span>)</span><br><span class="line">cvs = [cv <span class="keyword">for</span> w <span class="keyword">in</span> rotokas_words <span class="keyword">for</span> cv <span class="keyword">in</span> re.findall(<span class="string">r'[ptksvr][aeiou]'</span>, w)]</span><br><span class="line">cfd = nltk.ConditionalFreqDist(cvs)</span><br><span class="line">cfd.tabulate()</span><br><span class="line"><span class="comment"># add index</span></span><br><span class="line">cv_word_pairs = [(cv, w) <span class="keyword">for</span> w <span class="keyword">in</span> rotokas_words</span><br><span class="line">	<span class="keyword">for</span> cv <span class="keyword">in</span> re.findall(<span class="string">r'[ptksvr][aeiou]'</span>, w)]</span><br><span class="line">cv_index = nltk.Index(cv_word_pairs)</span><br><span class="line">cv_index[<span class="string">'su'</span>]</span><br><span class="line">cv_index[<span class="string">'po'</span>]</span><br></pre></td></tr></table></figure>
<p><strong>Finding Word Stems</strong><br>
比如搜索的时候，laptops和laptop的效果是一样的。<br>
我们将来用NLTK的内置stemmer，不过可以先看看用正则表达式可以怎么做。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stem</span><span class="params">(word)</span>:</span></span><br><span class="line">	regexp = <span class="string">r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'</span></span><br><span class="line">	stem, suffix = re.findall(regexp, word)[<span class="number">0</span>]</span><br><span class="line">	<span class="keyword">return</span> stem</span><br><span class="line">raw = <span class="string">"""DENNIS: Listen, strange women lying in ponds distributing swords</span><br><span class="line">	is no basis for a system of government.  Supreme executive power derives from</span><br><span class="line">	a mandate from the masses, not from some farcical aquatic ceremony."""</span></span><br><span class="line">tokens = word_tokenize(raw)</span><br><span class="line">[stem(t) <span class="keyword">for</span> t <span class="keyword">in</span> tokens]</span><br></pre></td></tr></table></figure>
<p><strong>Searching Tokenized Text</strong><br>
a special kind of regular expression for searching across multiple words in a text.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> gutenberg, nps_chat</span><br><span class="line">moby = nltk.Text(gutenberg.words(<span class="string">'melville-moby_dick.txt'</span>))</span><br><span class="line">moby.findall(<span class="string">r"&lt;a&gt; (&lt;.*&gt;) &lt;man&gt;"</span>)	<span class="comment"># monied; nervous; dangerous; white; white; white; pious;...</span></span><br><span class="line">chat = nltk.Text(nps_chat.words())</span><br><span class="line">chat.findall(<span class="string">r"&lt;.*&gt; &lt;.*&gt; &lt;bro&gt;"</span>)	<span class="comment"># you rule bro; telling you bro; u twizted bro</span></span><br><span class="line">chat.findall(<span class="string">r"&lt;l.*&gt;&#123;3,&#125;"</span>)	<span class="comment"># lol lol lol; lmao lol lol; lol lol lol; la la la la la;...</span></span><br></pre></td></tr></table></figure>
<p><code>nltk.re_show(p, s)</code>annotates the string s to show every place where pattern p was matched. <code>nltk.app.nemo()</code>provides a graphical interface for exploring regular expressions.</p>
<h3 id="normalizeing-text"><a class="markdownIt-Anchor" href="#normalizeing-text"></a> Normalizeing Text</h3>
<p><strong>Stemmers</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">raw = <span class="string">"""DENNIS: Listen, strange women lying in ponds distributing swords</span><br><span class="line">	is no basis for a system of government.  Supreme executive power derives from</span><br><span class="line">	a mandate from the masses, not from some farcical aquatic ceremony."""</span></span><br><span class="line">tokens = word_tokenize(raw)</span><br><span class="line"><span class="comment"># 3 different stemmers</span></span><br><span class="line">porter = nltk.PorterStemmer()</span><br><span class="line">lancaster = nltk.LancasterStemmer()</span><br><span class="line">[porter.stem(t) <span class="keyword">for</span> t <span class="keyword">in</span> tokens]</span><br><span class="line">[lancaster.stem(t) <span class="keyword">for</span> t <span class="keyword">in</span> tokens]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># indexing some texts and want to support search using alternative forms of words</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IndexedText</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, stemmer, text)</span>:</span></span><br><span class="line">        self._text = text</span><br><span class="line">        self._stemmer = stemmer</span><br><span class="line">        self._index = nltk.Index((self._stem(word), i)</span><br><span class="line">                                 <span class="keyword">for</span> (i, word) <span class="keyword">in</span> enumerate(text))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">concordance</span><span class="params">(self, word, width=<span class="number">40</span>)</span>:</span></span><br><span class="line">        key = self._stem(word)</span><br><span class="line">        wc = int(width/<span class="number">4</span>)                <span class="comment"># words of context</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> self._index[key]:</span><br><span class="line">            lcontext = <span class="string">' '</span>.join(self._text[i-wc:i])</span><br><span class="line">            rcontext = <span class="string">' '</span>.join(self._text[i:i+wc])</span><br><span class="line">            ldisplay = <span class="string">'&#123;:&gt;&#123;width&#125;&#125;'</span>.format(lcontext[-width:], width=width)</span><br><span class="line">            rdisplay = <span class="string">'&#123;:&#123;width&#125;&#125;'</span>.format(rcontext[:width], width=width)</span><br><span class="line">            print(ldisplay, rdisplay)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_stem</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._stemmer.stem(word).lower()</span><br><span class="line">porter = nltk.PorterStemmer()</span><br><span class="line">grail = nltk.corpus.webtext.words(<span class="string">'grail.txt'</span>)</span><br><span class="line">text = IndexedText(porter, grail)</span><br><span class="line">text.concordance(<span class="string">'lie'</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Lemmatization</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compile the vocabulary of some texts and want a list of valid lemmas</span></span><br><span class="line">wnl = nltk.WordNetLemmatizer()</span><br><span class="line">[wnl.lemmatize(t) <span class="keyword">for</span> t <span class="keyword">in</span> tokens]</span><br></pre></td></tr></table></figure>
<h3 id="regular-expressions-for-tokenizing-text"><a class="markdownIt-Anchor" href="#regular-expressions-for-tokenizing-text"></a> Regular Expressions for Tokenizing Text</h3>
<p><strong>Simple Approaches to Tokenization</strong><br>
最简单的就是从空格劈开。<code>re.split(r'[ \t\n]+', raw)</code>。<br>
所有字母组劈开。<code>re.split(r'\W+', raw)</code>。<br>
考虑连字符等。<code>re.findall(r&quot;\w+(?:[-']\w+)*|'|[-.(]+|\S\w*&quot;, raw)</code>。</p>
<p><strong>Regular Expression Symbols</strong></p>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Function</th>
</tr>
</thead>
<tbody>
<tr>
<td>\b</td>
<td>Word boundary (zero width)</td>
</tr>
<tr>
<td>\d</td>
<td>Any decimal digit (equivalent to [0-9])</td>
</tr>
<tr>
<td>\D</td>
<td>Any non-digit character (equivalent to [^0-9])</td>
</tr>
<tr>
<td>\s</td>
<td>Any whitespace character (equivalent to [ \t\n\r\f\v])</td>
</tr>
<tr>
<td>\S</td>
<td>Any non-whitespace character (equivalent to [^ \t\n\r\f\v])</td>
</tr>
<tr>
<td>\w</td>
<td>Any alphanumeric character (equivalent to [a-zA-Z0-9_])</td>
</tr>
<tr>
<td>\W</td>
<td>Any non-alphanumeric character (equivalent to [^a-zA-Z0-9_])</td>
</tr>
<tr>
<td>\t</td>
<td>The tab character</td>
</tr>
<tr>
<td>\n</td>
<td>The newline character</td>
</tr>
</tbody>
</table>
<p><strong>NLTK’s Regular Expression Tokenizer</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'That U.S.A. poster-print costs $12.40...'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pattern = <span class="string">r'''(?x)    # set flag to allow verbose regexps</span><br><span class="line">	([A-Z]\.)+        # abbreviations, e.g. U.S.A.</span><br><span class="line">	| \w+(-\w+)*        # words with optional internal hyphens</span><br><span class="line">	| \$?\d+(\.\d+)?%?  # currency and percentages, e.g. $12.40, 82%</span><br><span class="line">	| \.\.\.            # ellipsis</span><br><span class="line">	| [][.,;"'?():-_`]  # these are separate tokens; includes ], [</span><br><span class="line">	'''</span></span><br><span class="line">nltk.regexp_tokenize(text, pattern)	<span class="comment"># ['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']</span></span><br></pre></td></tr></table></figure>
<p>可以先列好一组词，然后和tokenlizer的结果比较判断好坏。<code>set(tokens).difference(wordlist)</code>。</p>
<h3 id="segmentation"><a class="markdownIt-Anchor" href="#segmentation"></a> Segmentation</h3>
<p><strong>Sentence Segmentation</strong><br>
Punkt sentence segmenter</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">text = nltk.corpus.gutenberg.raw(<span class="string">'chesterton-thursday.txt'</span>)</span><br><span class="line">sents = nltk.sent_tokenize(text)</span><br><span class="line">pprint.pprint(sents[<span class="number">79</span>:<span class="number">89</span>])</span><br></pre></td></tr></table></figure>
<p><strong>Word Segmentataion</strong><br>
没有明显词语边界的语言，比如我们中文。或者一些实时语言流，没有标注，也无法预知后一词。<br>
Object Function，该字后要分词就标1，用下图方式评估好坏，然后随机出一大堆，选里面坠吼得。<br>
<img src="http://www.nltk.org/images/brent.png" alt="Calculation of Objective Function: Given a hypothetical segmentation of the source text (on the left), derive a lexicon and a derivation table that permit the source text to be reconstructed, then total up the number of characters used by each lexical item (including a boundary marker) and the number of lexical items used by each derivation, to serve as a score of the quality of the segmentation; smaller values of the score indicate a better segmentation."></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reconstruct Segmented Text from String Representation</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">segment</span><span class="params">(text, segs)</span>:</span></span><br><span class="line">    words = []</span><br><span class="line">    last = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(segs)):</span><br><span class="line">        <span class="keyword">if</span> segs[i] == <span class="string">'1'</span>:</span><br><span class="line">            words.append(text[last:i+<span class="number">1</span>])</span><br><span class="line">            last = i+<span class="number">1</span></span><br><span class="line">    words.append(text[last:])</span><br><span class="line">    <span class="keyword">return</span> words</span><br><span class="line"><span class="comment"># ------------------------------------------</span></span><br><span class="line"><span class="comment"># Computing the Cost of Storing the Lexicon and Reconstructing the Source Text</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(text, segs)</span>:</span></span><br><span class="line">    words = segment(text, segs)</span><br><span class="line">    text_size = len(words)</span><br><span class="line">    lexicon_size = sum(len(word) + <span class="number">1</span> <span class="keyword">for</span> word <span class="keyword">in</span> set(words))</span><br><span class="line">    <span class="keyword">return</span> text_size + lexicon_size</span><br><span class="line"><span class="comment"># ------------------------------------------</span></span><br><span class="line"><span class="comment"># Non-Deterministic Search Using Simulated Annealing</span></span><br><span class="line"><span class="comment"># begin searching with phrase segmentations only;</span></span><br><span class="line"><span class="comment"># randomly perturb the zeros and ones proportional to the "temperature"; </span></span><br><span class="line"><span class="comment"># with each iteration the temperature is lowered and the perturbation of boundaries is reduced.</span></span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flip</span><span class="params">(segs, pos)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> segs[:pos] + str(<span class="number">1</span>-int(segs[pos])) + segs[pos+<span class="number">1</span>:]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flip_n</span><span class="params">(segs, n)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        segs = flip(segs, randint(<span class="number">0</span>, len(segs)<span class="number">-1</span>))</span><br><span class="line">    <span class="keyword">return</span> segs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">anneal</span><span class="params">(text, segs, iterations, cooling_rate)</span>:</span></span><br><span class="line">    temperature = float(len(segs))</span><br><span class="line">    <span class="keyword">while</span> temperature &gt; <span class="number">0.5</span>:</span><br><span class="line">        best_segs, best = segs, evaluate(text, segs)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(iterations):</span><br><span class="line">            guess = flip_n(segs, round(temperature))</span><br><span class="line">            score = evaluate(text, guess)</span><br><span class="line">            <span class="keyword">if</span> score &lt; best:</span><br><span class="line">                best, best_segs = score, guess</span><br><span class="line">        score, segs = best, best_segs</span><br><span class="line">        temperature = temperature / cooling_rate</span><br><span class="line">        print(evaluate(text, segs), segment(text, segs))</span><br><span class="line">    print()</span><br><span class="line">    <span class="keyword">return</span> segs</span><br><span class="line">text = <span class="string">"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy"</span></span><br><span class="line">seg1 = <span class="string">"0000000000000001000000000010000000000000000100000000000"</span></span><br><span class="line">segment(text, seg1)</span><br><span class="line">evaluate(text, seg1)</span><br><span class="line">anneal(text, seg1, <span class="number">5000</span>, <span class="number">1.2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="formatting-from-lists-to-strings"><a class="markdownIt-Anchor" href="#formatting-from-lists-to-strings"></a> Formatting: From Lists to Strings</h3>
<p><strong>From Lists to Strings</strong><br>
<code>join()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">silly = [<span class="string">'We'</span>, <span class="string">'called'</span>, <span class="string">'him'</span>, <span class="string">'Tortoise'</span>, <span class="string">'because'</span>, <span class="string">'he'</span>, <span class="string">'taught'</span>, <span class="string">'us'</span>, <span class="string">'.'</span>]</span><br><span class="line"><span class="string">';'</span>.join(silly)	<span class="comment"># We;called;him;Tortoise;because;he;taught;us;.</span></span><br></pre></td></tr></table></figure>
<p><strong>Strings and Formats</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fdist = nltk.FreqDist([<span class="string">'dog'</span>, <span class="string">'cat'</span>, <span class="string">'dog'</span>, <span class="string">'cat'</span>, <span class="string">'dog'</span>, <span class="string">'snake'</span>, <span class="string">'dog'</span>, <span class="string">'cat'</span>])</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> sorted(fdist):</span><br><span class="line">	print(<span class="string">'&#123;0&#125;-&gt;&#123;1&#125;;'</span>.format(word, fdist[word]), end=<span class="string">' '</span>)	<span class="comment"># cat-&gt;3; dog-&gt;4; snake-&gt;1;</span></span><br></pre></td></tr></table></figure>
<p><strong>Lining Things Up</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'&#123;:&lt;6&#125;'</span> .format(<span class="number">41</span>)	<span class="comment"># '41    '</span></span><br><span class="line"><span class="string">'&#123;:&gt;6&#125;'</span>.format(<span class="string">'dog'</span>)	<span class="comment"># '   dog'</span></span><br><span class="line"><span class="string">'&#123;:.4f&#125;'</span>.format(math.pi)	<span class="comment"># '3.1416'</span></span><br><span class="line">count, total = <span class="number">3205</span>, <span class="number">9375</span></span><br><span class="line"><span class="string">"accuracy for &#123;&#125; words: &#123;:.4%&#125;"</span>.format(total, count / total)	<span class="comment"># 'accuracy for 9375 words: 34.1867%'</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tabulating data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tabulate</span><span class="params">(cfdist, words, categories)</span>:</span></span><br><span class="line">    print(<span class="string">'&#123;:16&#125;'</span>.format(<span class="string">'Category'</span>), end=<span class="string">' '</span>)                    <span class="comment"># column headings</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        print(<span class="string">'&#123;:&gt;6&#125;'</span>.format(word), end=<span class="string">' '</span>)</span><br><span class="line">    print()</span><br><span class="line">    <span class="keyword">for</span> category <span class="keyword">in</span> categories:</span><br><span class="line">        print(<span class="string">'&#123;:16&#125;'</span>.format(category), end=<span class="string">' '</span>)                  <span class="comment"># row heading</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:                                        <span class="comment"># for each word</span></span><br><span class="line">            print(<span class="string">'&#123;:6&#125;'</span>.format(cfdist[category][word]), end=<span class="string">' '</span>) <span class="comment"># print table cell</span></span><br><span class="line">        print()                                                   <span class="comment"># end the row</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> brown</span><br><span class="line">cfd = nltk.ConditionalFreqDist(</span><br><span class="line">	(genre, word)</span><br><span class="line">	<span class="keyword">for</span> genre <span class="keyword">in</span> brown.categories()</span><br><span class="line">	<span class="keyword">for</span> word <span class="keyword">in</span> brown.words(categories=genre))</span><br><span class="line">genres = [<span class="string">'news'</span>, <span class="string">'religion'</span>, <span class="string">'hobbies'</span>, <span class="string">'science_fiction'</span>, <span class="string">'romance'</span>, <span class="string">'humor'</span>]</span><br><span class="line">modals = [<span class="string">'can'</span>, <span class="string">'could'</span>, <span class="string">'may'</span>, <span class="string">'might'</span>, <span class="string">'must'</span>, <span class="string">'will'</span>]</span><br><span class="line">tabulate(cfd, modals, genres)</span><br></pre></td></tr></table></figure>
<p><strong>Writing Results to a File</strong><br>
文件名中不要有空格，或是只有大小写不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">output_file = open(<span class="string">'output.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line">words = set(nltk.corpus.genesis.words(<span class="string">'english-kjv.txt'</span>))</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> sorted(words):</span><br><span class="line">	print(word, file=output_file)</span><br></pre></td></tr></table></figure>
<p><strong>Text Wrapping</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> textwrap <span class="keyword">import</span> fill</span><br><span class="line">format = <span class="string">'%s (%d),'</span></span><br><span class="line">pieces = [format % (word, len(word)) <span class="keyword">for</span> word <span class="keyword">in</span> saying]</span><br><span class="line">output = <span class="string">' '</span>.join(pieces)</span><br><span class="line">wrapped = fill(output)</span><br><span class="line">print(wrapped)</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="writing-structured-programs"><a class="markdownIt-Anchor" href="#writing-structured-programs"></a> Writing Structured Programs</h2>
<p>本章要解决的问题：</p>
<ol>
<li>How can you write well-structured, readable programs that you and others will be able to re-use easily?</li>
<li>How do the fundamental building blocks work, such as loops, functions and assignment?</li>
<li>What are some of the pitfalls with Python programming and how can you avoid them?</li>
</ol>
<h3 id="back-to-the-basics"><a class="markdownIt-Anchor" href="#back-to-the-basics"></a> Back to the Basics</h3>
<p><strong>Assignment</strong><br>
List的Assignment实际上是引用。用<code>id()</code>可以看到id是否相同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nested = [[]] * <span class="number">3</span></span><br><span class="line">nested[<span class="number">1</span>].append(<span class="string">'Python'</span>)	<span class="comment"># 三个引用都变成Python</span></span><br><span class="line">nested[<span class="number">1</span>] = [<span class="string">'Monty'</span>]	<span class="comment"># 其中一个指向另一个list，剩下两个不受影响</span></span><br><span class="line">nested	<span class="comment"># [['Python'], ['Monty'], ['Python']]</span></span><br></pre></td></tr></table></figure>
<p>想要拷贝list，使用<code>bar = foo[:]</code>，不过list里面的引用还是老样子。如果完全不想拷贝任何引用，使用<code>copy.deepcopy()</code>。</p>
<p><strong>Equality</strong><br>
<code>==</code>表示值相等，<code>is</code>表示是同一物体。</p>
<p><strong>Conditionals</strong><br>
在<code>if</code>条件中，非空string或list被视为True，空的string或list视为False。<br>
<code>all()</code>和<code>any()</code>可以被用在list上，检查all或any项满足某条件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">all(len(w) &gt; <span class="number">4</span> <span class="keyword">for</span> w <span class="keyword">in</span> sent)	<span class="comment"># False</span></span><br><span class="line">any(len(w) &gt; <span class="number">4</span> <span class="keyword">for</span> w <span class="keyword">in</span> sent)	<span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<h3 id="sequences"><a class="markdownIt-Anchor" href="#sequences"></a> Sequences</h3>
<p>tuple初始化时用<code>,</code>，可以省略<code>()</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">typleT = <span class="string">'walk'</span>, <span class="string">'fem'</span>, <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p><strong>Operating on Sequence Types</strong></p>
<table>
<thead>
<tr>
<th>Python Expression</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td>for item in s</td>
<td>iterate over the items of s</td>
</tr>
<tr>
<td>for item in sorted(s)</td>
<td>iterate over the items of s in order</td>
</tr>
<tr>
<td>for item in set(s)</td>
<td>iterate over unique elements of s</td>
</tr>
<tr>
<td>for item in reversed(s)</td>
<td>iterate over elements of s in reverse</td>
</tr>
<tr>
<td>for item in set(s).difference(t)</td>
<td>iterate over elements of s not in t</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">words = [<span class="string">'I'</span>, <span class="string">'turned'</span>, <span class="string">'off'</span>, <span class="string">'the'</span>, <span class="string">'spectroroute'</span>]</span><br><span class="line">tags = [<span class="string">'noun'</span>, <span class="string">'verb'</span>, <span class="string">'prep'</span>, <span class="string">'det'</span>, <span class="string">'noun'</span>]</span><br><span class="line">list(zip(words, tags))	<span class="comment"># [('I', 'noun'), ('turned', 'verb'), ('off', 'prep'), ('the', 'det'), ('spectroroute', 'noun')]</span></span><br><span class="line">list(enumerate(words))	<span class="comment"># [(0, 'I'), (1, 'turned'), (2, 'off'), (3, 'the'), (4, 'spectroroute')]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">text = nltk.corpus.nps_chat.words()</span><br><span class="line">cut = int(<span class="number">0.9</span> * len(text))</span><br><span class="line">training_data, test_data = text[:cut], text[cut:]</span><br></pre></td></tr></table></figure>
<p><strong>Combining Different Sequence Types</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sorting the words in a string by their length</span></span><br><span class="line">words = <span class="string">'I turned off the spectroroute'</span>.split() [<span class="number">1</span>]</span><br><span class="line">wordlens = [(len(word), word) <span class="keyword">for</span> word <span class="keyword">in</span> words]</span><br><span class="line">wordlens.sort()</span><br><span class="line"><span class="string">' '</span>.join(w <span class="keyword">for</span> (_, w) <span class="keyword">in</span> wordlens)</span><br></pre></td></tr></table></figure>
<p>We often use lists to hold sequences of words. In contrast, a tuple is typically a collection of objects of <strong>different types</strong>, of <strong>fixed length</strong>. We often use a tuple to hold a <strong>record</strong>, a collection of different <strong>fields</strong> relating to some entity.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lexicon = [</span><br><span class="line">		(<span class="string">'the'</span>, <span class="string">'det'</span>, [<span class="string">'Di:'</span>, <span class="string">'D@'</span>]),</span><br><span class="line">		(<span class="string">'off'</span>, <span class="string">'prep'</span>, [<span class="string">'Qf'</span>, <span class="string">'O:f'</span>])</span><br><span class="line">	]</span><br></pre></td></tr></table></figure>
<p><strong>Generator Expressions</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">max([w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> word_tokenize(text)])</span><br><span class="line">max(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> word_tokenize(text))	<span class="comment"># generator，优化内存使用</span></span><br></pre></td></tr></table></figure>
<h3 id="questions-of-style"><a class="markdownIt-Anchor" href="#questions-of-style"></a> Questions of Style</h3>
<p>“The Art of Computer Programming”.</p>
<p><strong>Python Coding Style</strong><br>
<a href="http://www.python.org/dev/peps/pep-0008/" target="_blank" rel="external">A style guide for Python code</a> published by the designers of the Python language.<br>
consistency.</p>
<p><strong>Procedural vs Declarative Style</strong><br>
Procedural更像机器操作步骤，应该用Declarative风格。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total = sum(len(t) <span class="keyword">for</span> t <span class="keyword">in</span> tokens)</span><br><span class="line">print(total / len(tokens))</span><br><span class="line">maxlen = max(len(word) <span class="keyword">for</span> word <span class="keyword">in</span> text)</span><br><span class="line">[word <span class="keyword">for</span> word <span class="keyword">in</span> text <span class="keyword">if</span> len(word) == maxlen]</span><br></pre></td></tr></table></figure>
<p><strong>Some Legitimate Uses for Counters</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m, n = <span class="number">3</span>, <span class="number">7</span></span><br><span class="line">array = [[set() <span class="keyword">for</span> i <span class="keyword">in</span> range(n)] <span class="keyword">for</span> j <span class="keyword">in</span> range(m)]</span><br><span class="line">array[<span class="number">2</span>][<span class="number">5</span>].add(<span class="string">'Alice'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="functions-the-foundation-of-structured-programming"><a class="markdownIt-Anchor" href="#functions-the-foundation-of-structured-programming"></a> Functions: The Foundation of Structured Programming</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_text</span><span class="params">(file)</span>:</span></span><br><span class="line">    <span class="string">"""Read text from a file, normalizing whitespace and stripping HTML markup."""</span></span><br><span class="line">    text = open(file).read()</span><br><span class="line">    text = re.sub(<span class="string">r'&lt;.*?&gt;'</span>, <span class="string">' '</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">'\s+'</span>, <span class="string">' '</span>, text)</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>help(get_text)</span><br><span class="line">|   Help on function get_text <span class="keyword">in</span> module __main__:</span><br><span class="line">|</span><br><span class="line">|   get(text)</span><br><span class="line">|       Read text <span class="keyword">from</span> a file, normalizing whitespace <span class="keyword">and</span> stripping HTML markup.</span><br><span class="line"><span class="comment"># 所以要写说明啊</span></span><br></pre></td></tr></table></figure>
<p><strong>Function Inputs and Outputs</strong><br>
Functions should modify the contents of a parameter, or return a value, not both.</p>
<p><strong>Parameter Passing</strong><br>
value只传值，有结构的东西传的是引用。</p>
<p><strong>Variable Scopr</strong><br>
Function可以通过<code>global</code>改变global变量的值，不过应该尽量避免使用。应该用参数和返回值。</p>
<p><strong>Checking Parameter Types</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tag</span><span class="params">(word)</span>:</span></span><br><span class="line">	<span class="keyword">assert</span> isinstance(word, basestring), <span class="string">"argument to tag() must be a string"</span></span><br><span class="line">	<span class="keyword">if</span> word <span class="keyword">in</span> [<span class="string">'a'</span>, <span class="string">'the'</span>, <span class="string">'all'</span>]:</span><br><span class="line">		<span class="keyword">return</span> <span class="string">'det'</span></span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="string">'noun'</span></span><br></pre></td></tr></table></figure>
<p><strong>Functional Decomposition</strong><br>
大函数分解成若干小函数。不要有副作用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">freq_words</span><span class="params">(url, n)</span>:</span></span><br><span class="line">    html = request.urlopen(url).read().decode(<span class="string">'utf8'</span>)</span><br><span class="line">    text = BeautifulSoup(html).get_text()</span><br><span class="line">    freqdist = nltk.FreqDist(word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> word_tokenize(text))</span><br><span class="line">    <span class="keyword">return</span> [word <span class="keyword">for</span> (word, _) <span class="keyword">in</span> fd.most_common(n)]</span><br></pre></td></tr></table></figure>
<p><strong>Documenting Functions</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(reference, test)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Calculate the fraction of test items that equal the corresponding reference items.</span><br><span class="line">    Given a list of reference values and a corresponding list of test values,</span><br><span class="line">    return the fraction of corresponding values that are equal.</span><br><span class="line">    In particular, return the fraction of indexes</span><br><span class="line">    &#123;0&lt;i&lt;=len(test)&#125; such that C&#123;test[i] == reference[i]&#125;.</span><br><span class="line">        &gt;&gt;&gt; accuracy(['ADJ', 'N', 'V', 'N'], ['N', 'N', 'V', 'ADJ'])</span><br><span class="line">        0.5</span><br><span class="line">    :param reference: An ordered list of reference values</span><br><span class="line">    :type reference: list</span><br><span class="line">    :param test: A list of values to compare against the corresponding</span><br><span class="line">        reference values</span><br><span class="line">    :type test: list</span><br><span class="line">    :return: the accuracy score</span><br><span class="line">    :rtype: float</span><br><span class="line">    :raises ValueError: If reference and length do not have the same length</span><br><span class="line">    """</span></span><br><span class="line">    <span class="keyword">if</span> len(reference) != len(test):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"Lists must have the same length."</span>)</span><br><span class="line">    num_correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(reference, test):</span><br><span class="line">        <span class="keyword">if</span> x == y:</span><br><span class="line">            num_correct += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> float(num_correct) / len(reference)</span><br></pre></td></tr></table></figure>
<h3 id="doing-more-with-functions"><a class="markdownIt-Anchor" href="#doing-more-with-functions"></a> Doing More with Functions</h3>
<p><strong>Functions as Arguments</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sent = [<span class="string">'Take'</span>, <span class="string">'care'</span>, <span class="string">'of'</span>, <span class="string">'the'</span>, <span class="string">'sense'</span>, <span class="string">','</span>, <span class="string">'and'</span>, <span class="string">'the'</span>,</span><br><span class="line">	<span class="string">'sounds'</span>, <span class="string">'will'</span>, <span class="string">'take'</span>, <span class="string">'care'</span>, <span class="string">'of'</span>, <span class="string">'themselves'</span>, <span class="string">'.'</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_property</span><span class="params">(prop)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> [prop(word) <span class="keyword">for</span> word <span class="keyword">in</span> sent]</span><br><span class="line">extract_property(len)	<span class="comment"># [4, 4, 2, 3, 5, 1, 3, 3, 6, 4, 4, 4, 2, 10, 1]</span></span><br><span class="line">extract_property(<span class="keyword">lambda</span> w: w[<span class="number">-1</span>])	<span class="comment"># ['e', 'e', 'f', 'e', 'e', ',', 'd', 'e', 's', 'l', 'e', 'e', 'f', 's', '.']</span></span><br><span class="line">sorted(sent, <span class="keyword">lambda</span> x, y: cmp(len(y), len(x)))</span><br></pre></td></tr></table></figure>
<p><strong>Accumulative Functions</strong><br>
These functions start by initializing some storage, and iterate over input to build it up, before returning some final object.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search2</span><span class="params">(substring, words)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> substring <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">yield</span> word</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> search2(<span class="string">'zz'</span>, nltk.corpus.brown.words()):</span><br><span class="line">	print(item, end=<span class="string">" "</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">permutations</span><span class="params">(seq)</span>:</span></span><br><span class="line">	<span class="keyword">if</span> len(seq) &lt;= <span class="number">1</span>:</span><br><span class="line">		<span class="keyword">yield</span> seq</span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="keyword">for</span> perm <span class="keyword">in</span> permutations(seq[<span class="number">1</span>:]):</span><br><span class="line">			<span class="keyword">for</span> i <span class="keyword">in</span> range(len(perm)+<span class="number">1</span>):</span><br><span class="line">				<span class="keyword">yield</span> perm[:i] + seq[<span class="number">0</span>:<span class="number">1</span>] + perm[i:]</span><br><span class="line">list(permutations([<span class="string">'police'</span>, <span class="string">'fish'</span>, <span class="string">'buffalo'</span>]))</span><br></pre></td></tr></table></figure>
<p><strong>Higher-Order Functions</strong><br>
<code>filter()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_content_word</span><span class="params">(word)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> word.lower() <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'a'</span>, <span class="string">'of'</span>, <span class="string">'the'</span>, <span class="string">'and'</span>, <span class="string">'will'</span>, <span class="string">','</span>, <span class="string">'.'</span>]</span><br><span class="line">sent = [<span class="string">'Take'</span>, <span class="string">'care'</span>, <span class="string">'of'</span>, <span class="string">'the'</span>, <span class="string">'sense'</span>, <span class="string">','</span>, <span class="string">'and'</span>, <span class="string">'the'</span>,</span><br><span class="line">	<span class="string">'sounds'</span>, <span class="string">'will'</span>, <span class="string">'take'</span>, <span class="string">'care'</span>, <span class="string">'of'</span>, <span class="string">'themselves'</span>, <span class="string">'.'</span>]</span><br><span class="line">list(filter(is_content_word, sent))	<span class="comment"># ['Take', 'care', 'sense', 'sounds', 'take', 'care', 'themselves']</span></span><br></pre></td></tr></table></figure>
<p><code>map()</code>applies a function to every item in a sequence.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># equivalent</span></span><br><span class="line">lengths = list(map(len, nltk.corpus.brown.sents(categories=<span class="string">'news'</span>)))</span><br><span class="line">lengths = [len(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> nltk.corpus.brown.sents(categories=<span class="string">'news'</span>)]</span><br><span class="line">sum(lengths) / len(lengths)	<span class="comment"># 21.75081116158339</span></span><br><span class="line"><span class="comment"># equivalent</span></span><br><span class="line">list(map(<span class="keyword">lambda</span> w: len(filter(<span class="keyword">lambda</span> c: c.lower() <span class="keyword">in</span> <span class="string">"aeiou"</span>, w)), sent))	<span class="comment"># [2, 2, 1, 1, 2, 0, 1, 1, 2, 1, 2, 2, 1, 3, 0]</span></span><br><span class="line">[len(c <span class="keyword">for</span> c <span class="keyword">in</span> w <span class="keyword">if</span> c.lower() <span class="keyword">in</span> <span class="string">"aeiou"</span>) <span class="keyword">for</span> w <span class="keyword">in</span> sent]</span><br></pre></td></tr></table></figure>
<p><strong>Named Arguments</strong><br>
Take care not to use a mutable object as the default value of a parameter.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repeat</span><span class="params">(msg=<span class="string">'&lt;empty&gt;'</span>, num=<span class="number">1</span>)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> msg * num</span><br><span class="line">repeat(num=<span class="number">3</span>)</span><br><span class="line">repeat(num=<span class="number">5</span>, msg=<span class="string">'Alice'</span>)</span><br></pre></td></tr></table></figure>
<p><code>*args</code>= all the unnamed parameters of the function. <code>**kwargs</code>=named parameters.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generic</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">	print(args)</span><br><span class="line">	print(kwargs)</span><br><span class="line">generic(<span class="number">1</span>, <span class="string">"African swallow"</span>, monty=<span class="string">"python"</span>)</span><br><span class="line"><span class="comment"># (1, 'African swallow')</span></span><br><span class="line"><span class="comment"># &#123;'monty': 'python'&#125;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用with会自动关闭打开的文档</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"lexicon.txt"</span>) <span class="keyword">as</span> f:</span><br><span class="line">	data = f.read()</span><br></pre></td></tr></table></figure>
<h3 id="program-development"><a class="markdownIt-Anchor" href="#program-development"></a> Program Development</h3>
<p>Key high-level abilities are algorithm design and its manifestation in structured programming. Key low-level abilities include familiarity with the syntactic constructs of the language, and knowledge of a variety of diagnostic methods for trouble-shooting a program which does not exhibit the expected behavior.</p>
<p><strong>Structure of a Python Module</strong><br>
The purpose of a program module is to bring logically-related definitions and functions together in order to facilitate re-use and abstraction.<br>
individual .py files.<br>
可以用<code>__file__</code>查看NLTK module的位置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nltk.metrics.distance.__file__	<span class="comment"># '/usr/lib/python2.5/site-packages/nltk/metrics/distance.pyc'</span></span><br></pre></td></tr></table></figure>
<p>Some module variables and functions are only used within the module. These should have names beginning with an underscore, e.g. <code>_helper()</code>, since this will hide the name. If another module imports this one, using the idiom: <code>from module import *</code>, these names will not be imported. You can optionally list the externally accessible names of a module using a special built-in variable like this: <code>__all__ = ['edit_distance', 'jaccard_distance']</code>.</p>
<p><strong>Multi-Module Programs</strong><br>
<img src="http://www.nltk.org/images/multi-module.png" alt="Structure of a Multi-Module Program: The main program my_program.py imports functions from two other modules; unique analysis tasks are localized to the main program, while common loading and visualization tasks are kept apart to facilitate re-use and abstraction."></p>
<p><strong>Sources of Error</strong><br>
First, the input data may contain some unexpected characters.<br>
Second, a supplied function might not behave as expected.<br>
Third, our understanding of Python’s semantics may be at fault.</p>
<p><strong>Debugging Techniques</strong><br>
<code>print</code>, stack trace<br>
Python’s debugger:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pdb</span><br><span class="line"><span class="keyword">import</span> mymodule</span><br><span class="line">pdb.run(<span class="string">'mymodule.myfunction()'</span>)</span><br></pre></td></tr></table></figure>
<p>help/ step/ next/ break/ continue</p>
<p><strong>Defensive Programming</strong><br>
<code>assert</code>如<code>assert(isinstance(text, list))</code>。<br>
maintain a suite of test cases.<a href="http://docs.python.org/library/doctest.html" target="_blank" rel="external">regression testing</a></p>
<h3 id="algorithm-design"><a class="markdownIt-Anchor" href="#algorithm-design"></a> Algorithm Design</h3>
<p>看我自己的算法总结系列。<br>
Examples:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Four Ways to Compute Sanskrit Meter: </span></span><br><span class="line"><span class="comment"># (i) recursive; (ii) bottom-up dynamic programming; (iii) top-down dynamic programming; and (iv) built-in memoization.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">virahanka1</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">""</span>]</span><br><span class="line">    <span class="keyword">elif</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">"S"</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        s = [<span class="string">"S"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka1(n<span class="number">-1</span>)]</span><br><span class="line">        l = [<span class="string">"L"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka1(n<span class="number">-2</span>)]</span><br><span class="line">        <span class="keyword">return</span> s + l</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">virahanka2</span><span class="params">(n)</span>:</span></span><br><span class="line">    lookup = [[<span class="string">""</span>], [<span class="string">"S"</span>]]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>):</span><br><span class="line">        s = [<span class="string">"S"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> lookup[i+<span class="number">1</span>]]</span><br><span class="line">        l = [<span class="string">"L"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> lookup[i]]</span><br><span class="line">        lookup.append(s + l)</span><br><span class="line">    <span class="keyword">return</span> lookup[n]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">virahanka3</span><span class="params">(n, lookup=&#123;<span class="number">0</span>:[<span class="string">""</span>], <span class="number">1</span>:[<span class="string">"S"</span>]&#125;)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n <span class="keyword">not</span> <span class="keyword">in</span> lookup:</span><br><span class="line">        s = [<span class="string">"S"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka3(n<span class="number">-1</span>)]</span><br><span class="line">        l = [<span class="string">"L"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka3(n<span class="number">-2</span>)]</span><br><span class="line">        lookup[n] = s + l</span><br><span class="line">    <span class="keyword">return</span> lookup[n]</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> memoize</span><br><span class="line"><span class="meta">@memoize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">virahanka4</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">""</span>]</span><br><span class="line">    <span class="keyword">elif</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">"S"</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        s = [<span class="string">"S"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka4(n<span class="number">-1</span>)]</span><br><span class="line">        l = [<span class="string">"L"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka4(n<span class="number">-2</span>)]</span><br><span class="line">        <span class="keyword">return</span> s + l</span><br><span class="line"><span class="comment"># test</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>virahanka1(<span class="number">4</span>)</span><br><span class="line">[<span class="string">'SSSS'</span>, <span class="string">'SSL'</span>, <span class="string">'SLS'</span>, <span class="string">'LSS'</span>, <span class="string">'LL'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>virahanka2(<span class="number">4</span>)</span><br><span class="line">[<span class="string">'SSSS'</span>, <span class="string">'SSL'</span>, <span class="string">'SLS'</span>, <span class="string">'LSS'</span>, <span class="string">'LL'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>virahanka3(<span class="number">4</span>)</span><br><span class="line">[<span class="string">'SSSS'</span>, <span class="string">'SSL'</span>, <span class="string">'SLS'</span>, <span class="string">'LSS'</span>, <span class="string">'LL'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>virahanka4(<span class="number">4</span>)</span><br><span class="line">[<span class="string">'SSSS'</span>, <span class="string">'SSL'</span>, <span class="string">'SLS'</span>, <span class="string">'LSS'</span>, <span class="string">'LL'</span>]</span><br></pre></td></tr></table></figure>
<h3 id="a-sample-of-python-libraries"><a class="markdownIt-Anchor" href="#a-sample-of-python-libraries"></a> A Sample of Python Libraries</h3>
<p><strong>Matplotlib</strong>: 二维图形。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> arange</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line">colors = <span class="string">'rgbcmyk'</span> <span class="comment"># red, green, blue, cyan, magenta, yellow, black</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bar_chart</span><span class="params">(categories, words, counts)</span>:</span></span><br><span class="line">    <span class="string">"Plot a bar chart showing counts for each word by category"</span></span><br><span class="line">    ind = arange(len(words))</span><br><span class="line">    width = <span class="number">1</span> / (len(categories) + <span class="number">1</span>)</span><br><span class="line">    bar_groups = []</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(len(categories)):</span><br><span class="line">        bars = pyplot.bar(ind+c*width, counts[categories[c]], width,</span><br><span class="line">                         color=colors[c % len(colors)])</span><br><span class="line">        bar_groups.append(bars)</span><br><span class="line">    pyplot.xticks(ind+width, words)</span><br><span class="line">    pyplot.legend([b[<span class="number">0</span>] <span class="keyword">for</span> b <span class="keyword">in</span> bar_groups], categories, loc=<span class="string">'upper left'</span>)</span><br><span class="line">    pyplot.ylabel(<span class="string">'Frequency'</span>)</span><br><span class="line">    pyplot.title(<span class="string">'Frequency of Six Modal Verbs by Genre'</span>)</span><br><span class="line">    pyplot.show()</span><br><span class="line"><span class="comment"># test</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>genres = [<span class="string">'news'</span>, <span class="string">'religion'</span>, <span class="string">'hobbies'</span>, <span class="string">'government'</span>, <span class="string">'adventure'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>modals = [<span class="string">'can'</span>, <span class="string">'could'</span>, <span class="string">'may'</span>, <span class="string">'might'</span>, <span class="string">'must'</span>, <span class="string">'will'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cfdist = nltk.ConditionalFreqDist(</span><br><span class="line"><span class="meta">... </span>             (genre, word)</span><br><span class="line"><span class="meta">... </span>             <span class="keyword">for</span> genre <span class="keyword">in</span> genres</span><br><span class="line"><span class="meta">... </span>             <span class="keyword">for</span> word <span class="keyword">in</span> nltk.corpus.brown.words(categories=genre)</span><br><span class="line"><span class="meta">... </span>             <span class="keyword">if</span> word <span class="keyword">in</span> modals)</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>counts = &#123;&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> genre <span class="keyword">in</span> genres:</span><br><span class="line"><span class="meta">... </span>    counts[genre] = [cfdist[genre][word] <span class="keyword">for</span> word <span class="keyword">in</span> modals]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bar_chart(genres, modals, counts)</span><br></pre></td></tr></table></figure>
<p><img src="http://www.nltk.org/images/modal_genre.png" alt="Bar Chart Showing Frequency of Modals in Different Sections of Brown Corpus: this visualization was produced by the program"></p>
<p><strong>NetworkX</strong>: for defining and manipulating structures consisting of nodes and edges, known as graphs.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> wordnet <span class="keyword">as</span> wn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">traverse</span><span class="params">(graph, start, node)</span>:</span></span><br><span class="line">    graph.depth[node.name] = node.shortest_path_distance(start)</span><br><span class="line">    <span class="keyword">for</span> child <span class="keyword">in</span> node.hyponyms():</span><br><span class="line">        graph.add_edge(node.name, child.name) [<span class="number">1</span>]</span><br><span class="line">        traverse(graph, start, child) [<span class="number">2</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hyponym_graph</span><span class="params">(start)</span>:</span></span><br><span class="line">    G = nx.Graph() [<span class="number">3</span>]</span><br><span class="line">    G.depth = &#123;&#125;</span><br><span class="line">    traverse(G, start, start)</span><br><span class="line">    <span class="keyword">return</span> G</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">graph_draw</span><span class="params">(graph)</span>:</span></span><br><span class="line">    nx.draw_graphviz(graph,</span><br><span class="line">         node_size = [<span class="number">16</span> * graph.degree(n) <span class="keyword">for</span> n <span class="keyword">in</span> graph],</span><br><span class="line">         node_color = [graph.depth[n] <span class="keyword">for</span> n <span class="keyword">in</span> graph],</span><br><span class="line">         with_labels = <span class="keyword">False</span>)</span><br><span class="line">    matplotlib.pyplot.show()</span><br><span class="line"><span class="comment"># test</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dog = wn.synset(<span class="string">'dog.n.01'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>graph = hyponym_graph(dog)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>graph_draw(graph)</span><br></pre></td></tr></table></figure>
<p><img src="http://www.nltk.org/images/dog-graph.png" alt="Visualization with NetworkX and Matplotlib: Part of the WordNet hypernym hierarchy is displayed, starting with dog.n.01 (the darkest node in the middle); node size is based on the number of children of the node, and color is based on the distance of the node from dog.n.01; this visualization was produced by the program"></p>
<p><strong>csv</strong>: read and write files stored in this format.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line">input_file = open(<span class="string">"lexicon.csv"</span>, <span class="string">"rb"</span>) [<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> csv.reader(input_file): [<span class="number">2</span>]</span><br><span class="line">	print(row)</span><br></pre></td></tr></table></figure>
<p><strong>NumPy</strong>: provides substantial support for numerical processing in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># array</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> numpy <span class="keyword">import</span> array</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cube = array([ [[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]],</span><br><span class="line"><span class="meta">... </span>               [[<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>]],</span><br><span class="line"><span class="meta">... </span>               [[<span class="number">6</span>,<span class="number">6</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">7</span>,<span class="number">7</span>], [<span class="number">8</span>,<span class="number">8</span>,<span class="number">8</span>]] ])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cube[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cube[<span class="number">2</span>].transpose()</span><br><span class="line">array([[<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">       [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">       [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cube[<span class="number">2</span>,<span class="number">1</span>:]</span><br><span class="line">array([[<span class="number">7</span>, <span class="number">7</span>, <span class="number">7</span>],</span><br><span class="line">       [<span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>]])</span><br><span class="line"><span class="comment"># algebra functions</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> numpy <span class="keyword">import</span> linalg</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=array([[<span class="number">4</span>,<span class="number">0</span>], [<span class="number">3</span>,<span class="number">-5</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>u,s,vt = linalg.svd(a)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>u</span><br><span class="line">array([[<span class="number">-0.4472136</span> , <span class="number">-0.89442719</span>],</span><br><span class="line">       [<span class="number">-0.89442719</span>,  <span class="number">0.4472136</span> ]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line">array([ <span class="number">6.32455532</span>,  <span class="number">3.16227766</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vt</span><br><span class="line">array([[<span class="number">-0.70710678</span>,  <span class="number">0.70710678</span>],</span><br><span class="line">       [<span class="number">-0.70710678</span>, <span class="number">-0.70710678</span>]])</span><br></pre></td></tr></table></figure>
<p>NLTK’s clustering package nltk.cluster makes extensive use of NumPy arrays, and includes support for k-means clustering, Gaussian EM clustering, group average agglomerative clustering, and dendrogram plots. For details, type <code>help(nltk.cluster)</code>.</p>
<p><strong>Other Python Libraries</strong><br>
<a href="http://pypi.python.org/" target="_blank" rel="external">help of the Python Package Index</a>.</p>
<hr>
<h2 id="categorizing-and-tagging-words"><a class="markdownIt-Anchor" href="#categorizing-and-tagging-words"></a> Categorizing and Tagging Words</h2>
<p>本章要解决的问题：</p>
<ol>
<li>What are lexical categories and how are they used in natural language processing?</li>
<li>What is a good Python data structure for storing words and their categories?</li>
<li>How can we automatically tag each word of a text with its word class?</li>
</ol>
<p>The process of classifying words into their parts of speech and labeling them accordingly is known as <strong>part-of-speech tagging</strong>, <strong>POS-tagging</strong>, or simply <strong>tagging</strong>. Parts of speech are also known as <strong>word classes</strong> or <strong>lexical categories</strong>. The collection of tags used for a particular task is known as a <strong>tagset</strong>.</p>
<h3 id="using-a-tagger"><a class="markdownIt-Anchor" href="#using-a-tagger"></a> Using a Tagger</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = word_tokenize(<span class="string">"They refuse to permit us to obtain the refuse permit"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>nltk.pos_tag(text)</span><br><span class="line">[(<span class="string">'They'</span>, <span class="string">'PRP'</span>), (<span class="string">'refuse'</span>, <span class="string">'VBP'</span>), (<span class="string">'to'</span>, <span class="string">'TO'</span>), (<span class="string">'permit'</span>, <span class="string">'VB'</span>), (<span class="string">'us'</span>, <span class="string">'PRP'</span>),</span><br><span class="line">(<span class="string">'to'</span>, <span class="string">'TO'</span>), (<span class="string">'obtain'</span>, <span class="string">'VB'</span>), (<span class="string">'the'</span>, <span class="string">'DT'</span>), (<span class="string">'refuse'</span>, <span class="string">'NN'</span>), (<span class="string">'permit'</span>, <span class="string">'NN'</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = nltk.Text(word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> nltk.corpus.brown.words())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text.similar(<span class="string">'woman'</span>)</span><br><span class="line">Building word-context index...</span><br><span class="line">man day time year car moment world family house boy child country job</span><br><span class="line">state girl place war way case question</span><br></pre></td></tr></table></figure>
<h3 id="tagged-corpora"><a class="markdownIt-Anchor" href="#tagged-corpora"></a> Tagged Corpora</h3>
<p><strong>Representing Tagged Tokens</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sent = <span class="string">'''</span><br><span class="line"><span class="meta">... </span>The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN</span><br><span class="line"><span class="meta">... </span>other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC</span><br><span class="line"><span class="meta">... </span>Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS</span><br><span class="line"><span class="meta">... </span>said/VBD ``/`` ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB</span><br><span class="line"><span class="meta">... </span>accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT</span><br><span class="line"><span class="meta">... </span>interest/NN of/IN both/ABX governments/NNS ''/'' ./.</span><br><span class="line"><span class="meta">... </span>'''</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[nltk.tag.str2tuple(t) <span class="keyword">for</span> t <span class="keyword">in</span> sent.split()]</span><br><span class="line">[(<span class="string">'The'</span>, <span class="string">'AT'</span>), (<span class="string">'grand'</span>, <span class="string">'JJ'</span>), (<span class="string">'jury'</span>, <span class="string">'NN'</span>), (<span class="string">'commented'</span>, <span class="string">'VBD'</span>),</span><br><span class="line">(<span class="string">'on'</span>, <span class="string">'IN'</span>), (<span class="string">'a'</span>, <span class="string">'AT'</span>), (<span class="string">'number'</span>, <span class="string">'NN'</span>), ... (<span class="string">'.'</span>, <span class="string">'.'</span>)]</span><br></pre></td></tr></table></figure>
<p><strong>Reading Tagged Corpora</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nltk.corpus.sinica_treebank.tagged_words()</span><br><span class="line">nltk.corpus.brown.tagged_words(tagset=<span class="string">'universal'</span>)</span><br></pre></td></tr></table></figure>
<p>相对应的，<code>tagged_sents()</code> divides up the tagged words into sentences rather than presenting them as one big list.</p>
<p><strong>A Universal Part-of-Speech Tagset</strong></p>
<table>
<thead>
<tr>
<th>Tag</th>
<th>Meaning</th>
<th>English Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>ADJ</td>
<td>adjective</td>
<td>new, good, high, special, big, local</td>
</tr>
<tr>
<td>ADP</td>
<td>adposition</td>
<td>on, of, at, with, by, into, under</td>
</tr>
<tr>
<td>ADV</td>
<td>adverb</td>
<td>really, already, still, early, now</td>
</tr>
<tr>
<td>CONJ</td>
<td>conjunction</td>
<td>and, or, but, if, while, although</td>
</tr>
<tr>
<td>DET</td>
<td>determiner, article</td>
<td>the, a, some, most, every, no, which</td>
</tr>
<tr>
<td>NOUN</td>
<td>noun</td>
<td>year, home, costs, time, Africa</td>
</tr>
<tr>
<td>NUM</td>
<td>numeral</td>
<td>twenty-four, fourth, 1991, 14:24</td>
</tr>
<tr>
<td>PRT</td>
<td>particle</td>
<td>at, on, out, over per, that, up, with</td>
</tr>
<tr>
<td>PRON</td>
<td>pronoun</td>
<td>he, their, her, its, my, I, us</td>
</tr>
<tr>
<td>VERB</td>
<td>verb</td>
<td>is, say, told, given, playing, would</td>
</tr>
<tr>
<td>.</td>
<td>punctuation marks</td>
<td>. , ; !</td>
</tr>
<tr>
<td>X</td>
<td>other</td>
<td>ersatz, esprit, dunno, gr8, univeristy</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># most common tags in the news category of the Brown corpus.</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> brown</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>brown_news_tagged = brown.tagged_words(categories=<span class="string">'news'</span>, tagset=<span class="string">'universal'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tag_fd = nltk.FreqDist(tag <span class="keyword">for</span> (word, tag) <span class="keyword">in</span> brown_news_tagged)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tag_fd.most_common()</span><br><span class="line">[(<span class="string">'NOUN'</span>, <span class="number">30640</span>), (<span class="string">'VERB'</span>, <span class="number">14399</span>), (<span class="string">'ADP'</span>, <span class="number">12355</span>), (<span class="string">'.'</span>, <span class="number">11928</span>), (<span class="string">'DET'</span>, <span class="number">11389</span>),</span><br><span class="line"> (<span class="string">'ADJ'</span>, <span class="number">6706</span>), (<span class="string">'ADV'</span>, <span class="number">3349</span>), (<span class="string">'CONJ'</span>, <span class="number">2717</span>), (<span class="string">'PRON'</span>, <span class="number">2535</span>), (<span class="string">'PRT'</span>, <span class="number">2264</span>),</span><br><span class="line"> (<span class="string">'NUM'</span>, <span class="number">2166</span>), (<span class="string">'X'</span>, <span class="number">106</span>)]</span><br></pre></td></tr></table></figure>
<p><strong>Nouns</strong><br>
Syntactic Patterns involving some Nouns:</p>
<table>
<thead>
<tr>
<th>Word</th>
<th>After a determiner</th>
<th>Subject of the verb</th>
</tr>
</thead>
<tbody>
<tr>
<td>woman</td>
<td><em>the</em> woman who I saw yesterday …</td>
<td>the woman <em>sat</em> down</td>
</tr>
<tr>
<td>Scotland</td>
<td><em>the</em> Scotland I remember as a child …</td>
<td>Scotland <em>has</em> five million people</td>
</tr>
<tr>
<td>book</td>
<td><em>the</em> book I bought yesterday …</td>
<td>this book <em>recounts</em> the colonization of Australia</td>
</tr>
<tr>
<td>intelligence</td>
<td><em>the</em> intelligence displayed by the child …</td>
<td>Mary’s intelligence <em>impressed</em> her teachers</td>
</tr>
</tbody>
</table>
<p><code>N</code> for common nouns like <em>book</em>, <code>NP</code> for proper nouns like <em>Scotland</em>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># what parts of speech occur before a noun</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>word_tag_pairs = nltk.bigrams(brown_news_tagged)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>noun_preceders = [a[<span class="number">1</span>] <span class="keyword">for</span> (a, b) <span class="keyword">in</span> word_tag_pairs <span class="keyword">if</span> b[<span class="number">1</span>] == <span class="string">'NOUN'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fdist = nltk.FreqDist(noun_preceders)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[tag <span class="keyword">for</span> (tag, _) <span class="keyword">in</span> fdist.most_common()]</span><br><span class="line">[<span class="string">'NOUN'</span>, <span class="string">'DET'</span>, <span class="string">'ADJ'</span>, <span class="string">'ADP'</span>, <span class="string">'.'</span>, <span class="string">'VERB'</span>, <span class="string">'CONJ'</span>, <span class="string">'NUM'</span>, <span class="string">'ADV'</span>, <span class="string">'PRT'</span>, <span class="string">'PRON'</span>, <span class="string">'X'</span>]</span><br></pre></td></tr></table></figure>
<p><strong>Verbs</strong><br>
Syntactic Patterns involving some Verbs:</p>
<table>
<thead>
<tr>
<th>Word</th>
<th>Simple</th>
<th>With modifiers and adjuncts (italicized)</th>
</tr>
</thead>
<tbody>
<tr>
<td>fall</td>
<td>Rome fell</td>
<td>Dot com stocks suddenly fell like a stone</td>
</tr>
<tr>
<td>eat</td>
<td>Mice eat cheese</td>
<td>John ate the pizza with gusto</td>
</tr>
</tbody>
</table>
<p><code>VBD</code> past tense, <code>VBN</code> past participle.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">wsj = nltk.corpus.treebank.tagged_words(tagset=<span class="string">'universal'</span>)</span><br><span class="line"><span class="comment"># frequency-ordered list of tags given a word</span></span><br><span class="line">cfd1 = nltk.ConditionalFreqDist(wsj)</span><br><span class="line">cfd1[<span class="string">'yield'</span>].most_common()</span><br><span class="line"><span class="comment"># find words which can be both VBD and VBN</span></span><br><span class="line">[w <span class="keyword">for</span> w <span class="keyword">in</span> cfd1.conditions() <span class="keyword">if</span> <span class="string">'VBD'</span> <span class="keyword">in</span> cfd1[w] <span class="keyword">and</span> <span class="string">'VBN'</span> <span class="keyword">in</span> cfd1[w]]</span><br><span class="line"><span class="comment"># surrounding text</span></span><br><span class="line">idx2 = wsj.index((<span class="string">'kicked'</span>, <span class="string">'VBN'</span>))</span><br><span class="line">wsj[idx2<span class="number">-4</span>:idx2+<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p><strong>Adjectives and Adverbs</strong><br>
Each dictionary and grammar classifies differently.</p>
<p><strong>Unsiplified Tags</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># finds all tags starting with NN, and provides a few example words for each one.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findtags</span><span class="params">(tag_prefix, tagged_text)</span>:</span></span><br><span class="line">    cfd = nltk.ConditionalFreqDist((tag, word) <span class="keyword">for</span> (word, tag) <span class="keyword">in</span> tagged_text</span><br><span class="line">                                  <span class="keyword">if</span> tag.startswith(tag_prefix))</span><br><span class="line">    <span class="keyword">return</span> dict((tag, cfd[tag].most_common(<span class="number">5</span>)) <span class="keyword">for</span> tag <span class="keyword">in</span> cfd.conditions())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tagdict = findtags(<span class="string">'NN'</span>, nltk.corpus.brown.tagged_words(categories=<span class="string">'news'</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> tag <span class="keyword">in</span> sorted(tagdict):</span><br><span class="line"><span class="meta">... </span>    print(tag, tagdict[tag])</span><br><span class="line">NN [(<span class="string">'year'</span>, <span class="number">137</span>), (<span class="string">'time'</span>, <span class="number">97</span>), (<span class="string">'state'</span>, <span class="number">88</span>), (<span class="string">'week'</span>, <span class="number">85</span>), (<span class="string">'man'</span>, <span class="number">72</span>)]</span><br><span class="line">NN$ [(<span class="string">"year's"</span>, <span class="number">13</span>), (<span class="string">"world's"</span>, <span class="number">8</span>), (<span class="string">"state's"</span>, <span class="number">7</span>), (<span class="string">"nation's"</span>, <span class="number">6</span>), (<span class="string">"company's"</span>, <span class="number">6</span>)]</span><br><span class="line">NN$-HL [(<span class="string">"Golf's"</span>, <span class="number">1</span>), (<span class="string">"Navy's"</span>, <span class="number">1</span>)]</span><br><span class="line">NN$-TL [(<span class="string">"President's"</span>, <span class="number">11</span>), (<span class="string">"Army's"</span>, <span class="number">3</span>), (<span class="string">"Gallery's"</span>, <span class="number">3</span>), (<span class="string">"University's"</span>, <span class="number">3</span>), (<span class="string">"League's"</span>, <span class="number">3</span>)]</span><br><span class="line">NN-HL [(<span class="string">'sp.'</span>, <span class="number">2</span>), (<span class="string">'problem'</span>, <span class="number">2</span>), (<span class="string">'Question'</span>, <span class="number">2</span>), (<span class="string">'business'</span>, <span class="number">2</span>), (<span class="string">'Salary'</span>, <span class="number">2</span>)]</span><br><span class="line">NN-NC [(<span class="string">'eva'</span>, <span class="number">1</span>), (<span class="string">'aya'</span>, <span class="number">1</span>), (<span class="string">'ova'</span>, <span class="number">1</span>)]</span><br><span class="line">NN-TL [(<span class="string">'President'</span>, <span class="number">88</span>), (<span class="string">'House'</span>, <span class="number">68</span>), (<span class="string">'State'</span>, <span class="number">59</span>), (<span class="string">'University'</span>, <span class="number">42</span>), (<span class="string">'City'</span>, <span class="number">41</span>)]</span><br><span class="line">NN-TL-HL [(<span class="string">'Fort'</span>, <span class="number">2</span>), (<span class="string">'Dr.'</span>, <span class="number">1</span>), (<span class="string">'Oak'</span>, <span class="number">1</span>), (<span class="string">'Street'</span>, <span class="number">1</span>), (<span class="string">'Basin'</span>, <span class="number">1</span>)]</span><br><span class="line">NNS [(<span class="string">'years'</span>, <span class="number">101</span>), (<span class="string">'members'</span>, <span class="number">69</span>), (<span class="string">'people'</span>, <span class="number">52</span>), (<span class="string">'sales'</span>, <span class="number">51</span>), (<span class="string">'men'</span>, <span class="number">46</span>)]</span><br><span class="line">NNS$ [(<span class="string">"children's"</span>, <span class="number">7</span>), (<span class="string">"women's"</span>, <span class="number">5</span>), (<span class="string">"janitors'"</span>, <span class="number">3</span>), (<span class="string">"men's"</span>, <span class="number">3</span>), (<span class="string">"taxpayers'"</span>, <span class="number">2</span>)]</span><br><span class="line">NNS$-HL [(<span class="string">"Dealers'"</span>, <span class="number">1</span>), (<span class="string">"Idols'"</span>, <span class="number">1</span>)]</span><br><span class="line">NNS$-TL [(<span class="string">"Women's"</span>, <span class="number">4</span>), (<span class="string">"States'"</span>, <span class="number">3</span>), (<span class="string">"Giants'"</span>, <span class="number">2</span>), (<span class="string">"Bros.'"</span>, <span class="number">1</span>), (<span class="string">"Writers'"</span>, <span class="number">1</span>)]</span><br><span class="line">NNS-HL [(<span class="string">'comments'</span>, <span class="number">1</span>), (<span class="string">'Offenses'</span>, <span class="number">1</span>), (<span class="string">'Sacrifices'</span>, <span class="number">1</span>), (<span class="string">'funds'</span>, <span class="number">1</span>), (<span class="string">'Results'</span>, <span class="number">1</span>)]</span><br><span class="line">NNS-TL [(<span class="string">'States'</span>, <span class="number">38</span>), (<span class="string">'Nations'</span>, <span class="number">11</span>), (<span class="string">'Masters'</span>, <span class="number">10</span>), (<span class="string">'Rules'</span>, <span class="number">9</span>), (<span class="string">'Communists'</span>, <span class="number">9</span>)]</span><br><span class="line">NNS-TL-HL [(<span class="string">'Nations'</span>, <span class="number">1</span>)]</span><br></pre></td></tr></table></figure>
<p><strong>Exploring Tagged Corpora</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># "often" used as what?</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>brown_lrnd_tagged = brown.tagged_words(categories=<span class="string">'learned'</span>, tagset=<span class="string">'universal'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tags = [b[<span class="number">1</span>] <span class="keyword">for</span> (a, b) <span class="keyword">in</span> nltk.bigrams(brown_lrnd_tagged) <span class="keyword">if</span> a[<span class="number">0</span>] == <span class="string">'often'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fd = nltk.FreqDist(tags)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fd.tabulate()</span><br><span class="line"> PRT  ADV  ADP    . VERB  ADJ</span><br><span class="line">   <span class="number">2</span>    <span class="number">8</span>    <span class="number">7</span>    <span class="number">4</span>   <span class="number">37</span>    <span class="number">6</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># searching for Three-Word Phrases Using POS Tags</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> brown</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> (w1,t1), (w2,t2), (w3,t3) <span class="keyword">in</span> nltk.trigrams(sentence):</span><br><span class="line">        <span class="keyword">if</span> (t1.startswith(<span class="string">'V'</span>) <span class="keyword">and</span> t2 == <span class="string">'TO'</span> <span class="keyword">and</span> t3.startswith(<span class="string">'V'</span>)):</span><br><span class="line">            print(w1, w2, w3)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> tagged_sent <span class="keyword">in</span> brown.tagged_sents():</span><br><span class="line"><span class="meta">... </span>    process(tagged_sent)</span><br><span class="line">...</span><br><span class="line">combined to achieve</span><br><span class="line"><span class="keyword">continue</span> to place</span><br><span class="line">serve to protect</span><br><span class="line">wanted to wait</span><br><span class="line">allowed to place</span><br><span class="line">expected to become</span><br></pre></td></tr></table></figure>
<h3 id="mapping-words-to-properties-using-python-dictionaries"><a class="markdownIt-Anchor" href="#mapping-words-to-properties-using-python-dictionaries"></a> Mapping Words to Properties Using Python Dictionaries</h3>
<p><strong>Indexing Lists vs Dictionaries</strong><br>
Linguistic Objects as Mappings from Keys to Values:</p>
<table>
<thead>
<tr>
<th>Linguistic Object</th>
<th>Maps From</th>
<th>Maps To</th>
</tr>
</thead>
<tbody>
<tr>
<td>Document Index</td>
<td>Word</td>
<td>List of pages (where word is found)</td>
</tr>
<tr>
<td>Thesaurus</td>
<td>Word sense</td>
<td>List of synonyms</td>
</tr>
<tr>
<td>Dictionary</td>
<td>Headword</td>
<td>Entry (part-of-speech, sense definitions, etymology)</td>
</tr>
<tr>
<td>Comparative Wordlist</td>
<td>Gloss term</td>
<td>Cognates (list of words, one per language)</td>
</tr>
<tr>
<td>Morph Analyzer</td>
<td>Surface form</td>
<td>Morphological analysis (list of component morphemes)</td>
</tr>
</tbody>
</table>
<p><strong>Dictionaries in Python</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pos = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> key, val <span class="keyword">in</span> sorted(pos.items()):</span><br><span class="line">	print(key + <span class="string">":"</span>, val)</span><br><span class="line">pos.keys()</span><br><span class="line">pos.values()</span><br><span class="line">pos.items()</span><br></pre></td></tr></table></figure>
<p><strong>Defining Dictionaries</strong><br>
dictionary keys must be immutable types.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pos = &#123;<span class="string">'colorless'</span>: <span class="string">'ADJ'</span>, <span class="string">'ideas'</span>: <span class="string">'N'</span>, <span class="string">'sleep'</span>: <span class="string">'V'</span>, <span class="string">'furiously'</span>: <span class="string">'ADV'</span>&#125;</span><br><span class="line">pos = dict(colorless=<span class="string">'ADJ'</span>, ideas=<span class="string">'N'</span>, sleep=<span class="string">'V'</span>, furiously=<span class="string">'ADV'</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Default Dictionaries</strong><br>
寻某值不遇时，自动创建一个有默认值的项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pos = defaultdict(list)</span><br><span class="line">pos[<span class="string">'sleep'</span>] = [<span class="string">'NOUN'</span>, <span class="string">'VERB'</span>]</span><br><span class="line">pos[<span class="string">'ideas'</span>]	<span class="comment"># []</span></span><br><span class="line"><span class="comment"># 可以这样设置值</span></span><br><span class="line">pos = defaultdict(<span class="keyword">lambda</span>: <span class="string">'NOUN'</span>)</span><br><span class="line">pos[<span class="string">'colorless'</span>] = <span class="string">'ADJ'</span></span><br><span class="line"> pos[<span class="string">'blog'</span>]	<span class="comment"># 'NOUN'</span></span><br><span class="line">list(pos.items())	<span class="comment"># [('blog', 'NOUN'), ('colorless', 'ADJ')] # [_automatically-added]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 出现太少的词用UNK代替</span></span><br><span class="line">alice = nltk.corpus.gutenberg.words(<span class="string">'carroll-alice.txt'</span>)</span><br><span class="line">vocab = nltk.FreqDist(alice)</span><br><span class="line">v1000 = [word <span class="keyword">for</span> (word, _) <span class="keyword">in</span> vocab.most_common(<span class="number">1000</span>)]</span><br><span class="line">mapping = defaultdict(<span class="keyword">lambda</span>: <span class="string">'UNK'</span>)</span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> v1000:</span><br><span class="line">	mapping[v] = v</span><br><span class="line">alice2 = [mapping[v] <span class="keyword">for</span> v <span class="keyword">in</span> alice]</span><br><span class="line">alice2[:<span class="number">100</span>]</span><br></pre></td></tr></table></figure>
<p><strong>Incrementally Updating a Dictionary</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Incrementally Updating a Dictionary, and Sorting by Value</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">counts = defaultdict(int)</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> brown</span><br><span class="line"><span class="keyword">for</span> (word, tag) <span class="keyword">in</span> brown.tagged_words(categories=<span class="string">'news'</span>, tagset=<span class="string">'universal'</span>):</span><br><span class="line">	counts[tag] += <span class="number">1</span></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line">sorted(counts.items(), key=itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)	<span class="comment"># [('NOUN', 30640), ('VERB', 14399), ('ADP', 12355), ('.', 11928), ...]</span></span><br></pre></td></tr></table></figure>
<p>NLTK中创建defaultdict(list)的一个更方便的形式。其实nltk.FreqDist实际上也是脱胎于defaultdict(int)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">anagrams = nltk.Index((<span class="string">''</span>.join(sorted(w)), w) <span class="keyword">for</span> w <span class="keyword">in</span> words)</span><br><span class="line">anagrams[<span class="string">'aeilnrt'</span>]</span><br></pre></td></tr></table></figure>
<p><strong>Complex Keys and Values</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pos = defaultdict(<span class="keyword">lambda</span>: defaultdict(int))</span><br><span class="line">brown_news_tagged = brown.tagged_words(categories=<span class="string">'news'</span>, tagset=<span class="string">'universal'</span>)</span><br><span class="line"><span class="keyword">for</span> ((w1, t1), (w2, t2)) <span class="keyword">in</span> nltk.bigrams(brown_news_tagged):</span><br><span class="line">	pos[(t1, w2)][t2] += <span class="number">1</span></span><br><span class="line">pos[(<span class="string">'DET'</span>, <span class="string">'right'</span>)]	<span class="comment"># defaultdict(&lt;class 'int'&gt;, &#123;'ADJ': 11, 'NOUN': 5&#125;)</span></span><br></pre></td></tr></table></figure>
<p><strong>Inverting a Dictionary</strong><br>
当想根据value查找key的时候。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pos = &#123;<span class="string">'colorless'</span>: <span class="string">'ADJ'</span>, <span class="string">'ideas'</span>: <span class="string">'N'</span>, <span class="string">'sleep'</span>: <span class="string">'V'</span>, <span class="string">'furiously'</span>: <span class="string">'ADV'</span>&#125;</span><br><span class="line">pos.update(&#123;<span class="string">'cats'</span>: <span class="string">'N'</span>, <span class="string">'scratch'</span>: <span class="string">'V'</span>, <span class="string">'peacefully'</span>: <span class="string">'ADV'</span>, <span class="string">'old'</span>: <span class="string">'ADJ'</span>&#125;)</span><br><span class="line">pos2 = defaultdict(list)</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> pos.items():</span><br><span class="line">	pos2[value].append(key)</span><br></pre></td></tr></table></figure>
<p><strong>Python’s Dictionary Methods</strong>: A summary of commonly-used methods and idioms involving dictionaries.</p>
<table>
<thead>
<tr>
<th>Example</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>d = {}</td>
<td>create an empty dictionary and assign it to d</td>
</tr>
<tr>
<td>d[key] = value</td>
<td>assign a value to a given dictionary key</td>
</tr>
<tr>
<td>d.keys()</td>
<td>the list of keys of the dictionary</td>
</tr>
<tr>
<td>list(d)</td>
<td>the list of keys of the dictionary</td>
</tr>
<tr>
<td>sorted(d)</td>
<td>the keys of the dictionary, sorted</td>
</tr>
<tr>
<td>key in d</td>
<td>test whether a particular key is in the dictionary</td>
</tr>
<tr>
<td>for key in d</td>
<td>iterate over the keys of the dictionary</td>
</tr>
<tr>
<td>d.values()</td>
<td>the list of values in the dictionary</td>
</tr>
<tr>
<td>dict([(k1,v1), (k2,v2), …])</td>
<td>create a dictionary from a list of key-value pairs</td>
</tr>
<tr>
<td>d1.update(d2)</td>
<td>add all items from d2 to d1</td>
</tr>
<tr>
<td>defaultdict(int)</td>
<td>a dictionary whose default value is zero</td>
</tr>
</tbody>
</table>
<h3 id="automatic-tagging"><a class="markdownIt-Anchor" href="#automatic-tagging"></a> Automatic Tagging</h3>
<p>Automatically add part-of-speech tags to text. 因为word的tag在句子中才有意义，所以我们分析tagged过的sentence。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们用这个示例</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> brown</span><br><span class="line">brown_tagged_sents = brown.tagged_sents(categories=<span class="string">'news'</span>)</span><br><span class="line">brown_sents = brown.sents(categories=<span class="string">'news'</span>)</span><br></pre></td></tr></table></figure>
<p><strong>The Default Tagger</strong><br>
把所有词都tag成最可能的那一款。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tags = [tag <span class="keyword">for</span> (word, tag) <span class="keyword">in</span> brown.tagged_words(categories=<span class="string">'news'</span>)]</span><br><span class="line">nltk.FreqDist(tags).max()	<span class="comment"># 'NN'</span></span><br><span class="line">raw = <span class="string">'I do not like green eggs and ham, I do not like them Sam I am!'</span></span><br><span class="line">tokens = word_tokenize(raw)</span><br><span class="line">default_tagger = nltk.DefaultTagger(<span class="string">'NN'</span>)</span><br><span class="line">default_tagger.tag(tokens)</span><br></pre></td></tr></table></figure>
<p><strong>The Regular Expression Tagger</strong><br>
根据一定pattern对word做匹配。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">patterns = [</span><br><span class="line">		(<span class="string">r'.*ing$'</span>, <span class="string">'VBG'</span>),               <span class="comment"># gerunds</span></span><br><span class="line">		(<span class="string">r'.*ed$'</span>, <span class="string">'VBD'</span>),                <span class="comment"># simple past</span></span><br><span class="line">		(<span class="string">r'.*es$'</span>, <span class="string">'VBZ'</span>),                <span class="comment"># 3rd singular present</span></span><br><span class="line">		(<span class="string">r'.*ould$'</span>, <span class="string">'MD'</span>),               <span class="comment"># modals</span></span><br><span class="line">		(<span class="string">r'.*\'s$'</span>, <span class="string">'NN$'</span>),               <span class="comment"># possessive nouns</span></span><br><span class="line">		(<span class="string">r'.*s$'</span>, <span class="string">'NNS'</span>),                 <span class="comment"># plural nouns</span></span><br><span class="line">		(<span class="string">r'^-?[0-9]+(.[0-9]+)?$'</span>, <span class="string">'CD'</span>),  <span class="comment"># cardinal numbers</span></span><br><span class="line">		(<span class="string">r'.*'</span>, <span class="string">'NN'</span>)                     <span class="comment"># nouns (default)</span></span><br><span class="line">	]</span><br><span class="line">regexp_tagger = nltk.RegexpTagger(patterns)</span><br><span class="line">regexp_tagger.tag(brown_sents[<span class="number">3</span>])</span><br><span class="line">regexp_tagger.evaluate(brown_tagged_sents)</span><br></pre></td></tr></table></figure>
<p><strong>The Lookup Tagger</strong><br>
先总结最常用词的tag，然后用他们做表在里面查找，没有就用默认值。<br>
随着model size变大，tagger表现迅速达到一个高值，然后就停滞了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">performance</span><span class="params">(cfd, wordlist)</span>:</span></span><br><span class="line">    lt = dict((word, cfd[word].max()) <span class="keyword">for</span> word <span class="keyword">in</span> wordlist)</span><br><span class="line">    baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.DefaultTagger(<span class="string">'NN'</span>))</span><br><span class="line">    <span class="keyword">return</span> baseline_tagger.evaluate(brown.tagged_sents(categories=<span class="string">'news'</span>))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pylab</span><br><span class="line">    word_freqs = nltk.FreqDist(brown.words(categories=<span class="string">'news'</span>)).most_common()</span><br><span class="line">    words_by_freq = [w <span class="keyword">for</span> (w, _) <span class="keyword">in</span> word_freqs]</span><br><span class="line">    cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories=<span class="string">'news'</span>))</span><br><span class="line">    sizes = <span class="number">2</span> ** pylab.arange(<span class="number">15</span>)</span><br><span class="line">    perfs = [performance(cfd, words_by_freq[:size]) <span class="keyword">for</span> size <span class="keyword">in</span> sizes]</span><br><span class="line">    pylab.plot(sizes, perfs, <span class="string">'-bo'</span>)</span><br><span class="line">    pylab.title(<span class="string">'Lookup Tagger Performance with Varying Model Size'</span>)</span><br><span class="line">    pylab.xlabel(<span class="string">'Model Size'</span>)</span><br><span class="line">    pylab.ylabel(<span class="string">'Performance'</span>)</span><br><span class="line">    pylab.show()</span><br></pre></td></tr></table></figure>
<p><strong>Evaluation</strong><br>
评估表现很重要，因为错误在流水线中会不断放大。</p>
<h3 id="n-gram-tagging"><a class="markdownIt-Anchor" href="#n-gram-tagging"></a> N-Gram Tagging</h3>
<p><strong>Unigram Tagging</strong><br>
给每个词其最可能的tag。有点像lookup tagger，只不过是通过training建立的model。<br>
Separating the Training and Testing Data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> brown</span><br><span class="line">brown_tagged_sents = brown.tagged_sents(categories=<span class="string">'news'</span>)</span><br><span class="line">size = int(len(brown_tagged_sents) * <span class="number">0.9</span>)</span><br><span class="line">train_sents = brown_tagged_sents[:size]</span><br><span class="line">test_sents = brown_tagged_sents[size:]</span><br><span class="line">unigram_tagger = nltk.UnigramTagger(train_sents)</span><br><span class="line">unigram_tagger.evaluate(test_sents)</span><br></pre></td></tr></table></figure>
<p><strong>General N-Gram Tagging</strong><br>
An n-gram tagger is a generalization of a unigram tagger whose context is the current word together with the part-of-speech tags of the n-1 preceding tokens, as shown in 5.1. The tag to be chosen, tn, is circled, and the context is shaded in grey. In the example of an n-gram tagger shown in 5.1, we have n=3; that is, we consider the tags of the two preceding words in addition to the current word. An n-gram tagger picks the tag that is most likely in the given context.<br>
<img src="http://www.nltk.org/images/tag-context.png" alt="Tagger Context"><br>
<code>NgramTagger</code><br>
对见过的句子表现的很好，没见过的就糟糟糟。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bigram_tagger = nltk.BigramTagger(train_sents)</span><br><span class="line">bigram_tagger.tag(brown_sents[<span class="number">2007</span>])</span><br><span class="line">unseen_sent = brown_sents[<span class="number">4203</span>]</span><br><span class="line">bigram_tagger.tag(unseen_sent)</span><br></pre></td></tr></table></figure>
<p><strong>Combining Taggers</strong><br>
Use the more accurate algorithms when we can, but to fall back on algorithms with wider coverage when necessary.<br>
例如：<br>
Try tagging the token with the bigram tagger.<br>
If the bigram tagger is unable to find a tag for the token, try the unigram tagger.<br>
If the unigram tagger is also unable to find a tag, use a default tagger.<br>
用NLTK tagger的backoff参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t0 = nltk.DefaultTagger(<span class="string">'NN'</span>)</span><br><span class="line">t1 = nltk.UnigramTagger(train_sents, backoff=t0)</span><br><span class="line">t2 = nltk.BigramTagger(train_sents, backoff=t1)</span><br><span class="line">t2.evaluate(test_sents)</span><br></pre></td></tr></table></figure>
<p><strong>Tagging Unknown Words</strong><br>
A useful method to tag unknown words based on context is to limit the vocabulary of a tagger to the most frequent n words, and to replace every other word with a special word UNK.<br>
During training, a unigram tagger will probably learn that UNK is usually a noun. However, the n-gram taggers will detect contexts in which it has some other tag.</p>
<p><strong>Storing Taggers</strong><br>
保存训练好的tagger。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save tagger t2 to a file t2.pk1</span></span><br><span class="line"><span class="keyword">from</span> pickle <span class="keyword">import</span> dump</span><br><span class="line">output = open(<span class="string">'t2.pkl'</span>, <span class="string">'wb'</span>)</span><br><span class="line">dump(t2, output, <span class="number">-1</span>)</span><br><span class="line">output.close()</span><br><span class="line"><span class="comment"># load saved tagger</span></span><br><span class="line"><span class="keyword">from</span> pickle <span class="keyword">import</span> load</span><br><span class="line">input = open(<span class="string">'t2.pkl'</span>, <span class="string">'rb'</span>)</span><br><span class="line">tagger = load(input)</span><br><span class="line">input.close()</span><br></pre></td></tr></table></figure>
<p><strong>Performance Limitations</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># part-of-speech ambiguity of a trigram tagger.</span></span><br><span class="line">cfd = nltk.ConditionalFreqDist(</span><br><span class="line">           ((x[<span class="number">1</span>], y[<span class="number">1</span>], z[<span class="number">0</span>]), z[<span class="number">1</span>])	<span class="comment"># curent word and the previous two tags</span></span><br><span class="line">           <span class="keyword">for</span> sent <span class="keyword">in</span> brown_tagged_sents</span><br><span class="line">           <span class="keyword">for</span> x, y, z <span class="keyword">in</span> nltk.trigrams(sent))</span><br><span class="line">ambiguous_contexts = [c <span class="keyword">for</span> c <span class="keyword">in</span> cfd.conditions() <span class="keyword">if</span> len(cfd[c]) &gt; <span class="number">1</span>]</span><br><span class="line">sum(cfd[c].N() <span class="keyword">for</span> c <span class="keyword">in</span> ambiguous_contexts) / cfd.N()	<span class="comment"># 0.049297702068029296</span></span><br></pre></td></tr></table></figure>
<p><em>confusion matrix</em> charts expected tags (the gold standard) against actual tags generated by a tagger.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test_tags = [tag <span class="keyword">for</span> sent <span class="keyword">in</span> brown.sents(categories=<span class="string">'editorial'</span>)</span><br><span class="line">                 <span class="keyword">for</span> (word, tag) <span class="keyword">in</span> t2.tag(sent)]</span><br><span class="line">gold_tags = [tag <span class="keyword">for</span> (word, tag) <span class="keyword">in</span> brown.tagged_words(categories=<span class="string">'editorial'</span>)]</span><br><span class="line">print(nltk.ConfusionMatrix(gold_tags, test_tags))</span><br></pre></td></tr></table></figure>
<h3 id="transformation-based-tagging"><a class="markdownIt-Anchor" href="#transformation-based-tagging"></a> Transformation-Based Tagging</h3>
<p>n-gram tagger会有较大的稀疏散列表存储n-gram table(or language model)。而且只考虑之前word的tagger而忽略word本身的信息。<br>
Brill Tagging，begin with broad brush strokes then fix up the details, with successively finer changes.<br>
比如先用Unigram标记一遍，然后根据Brill Tagging中的每条Rule修改一遍结果，这些Rule也是training出来的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nltk.tag.brill.demo()</span><br></pre></td></tr></table></figure>
<h3 id="how-to-determine-the-category-of-a-word"><a class="markdownIt-Anchor" href="#how-to-determine-the-category-of-a-word"></a> How to Determine the Category of a Word</h3>
<p>In general, linguists use morphological, syntactic, and semantic clues to determine the category of a word.<br>
<strong>Morphological Clues</strong><br>
词的内部结构提供的信息，例如-ness is a suffix that combines with an adjective to produce a noun。<br>
<strong>Syntactic Clues</strong><br>
该词出现的上下文。<br>
<strong>Semantic Clues</strong><br>
根据词的定义。<br>
<strong>New Words</strong><br>
名词多。noun属于open class，而preposition就是closed class。<br>
<strong>Morphology in Part of Speech Tagsets</strong><br>
不同的语法结构暗示该词词性。</p>
<hr>
<h2 id="learning-to-classify-text"><a class="markdownIt-Anchor" href="#learning-to-classify-text"></a> Learning to Classify Text</h2>
<p>本章要解决的问题：</p>
<ol>
<li>How can we identify particular features of language data that are salient for classifying it?</li>
<li>How can we construct models of language that can be used to perform language processing tasks automatically?</li>
<li>What can we learn about language from these models?</li>
</ol>
<h3 id="supervised-classification"><a class="markdownIt-Anchor" href="#supervised-classification"></a> Supervised Classification</h3>
<p><img src="http://www.nltk.org/images/supervised-classification.png" alt="Supervised Classification. (a) During training, a feature extractor is used to convert each input value to a feature set. These feature sets, which capture the basic information about each input that should be used to classify it, are discussed in the next section. Pairs of feature sets and labels are fed into the machine learning algorithm to generate a model. (b) During prediction, the same feature extractor is used to convert unseen inputs to feature sets. These feature sets are then fed into the model, which generates predicted labels."></p>
<p><strong>Gender Identification</strong><br>
首先要想好what features of the input are relevant, and how to encode those features.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feature set</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gender_features</span><span class="params">(word)</span>:</span></span><br><span class="line">	ruturn &#123;<span class="string">'last_letter'</span>:word[<span class="number">-1</span>]&#125;</span><br><span class="line"><span class="comment"># examples</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> names</span><br><span class="line">labeled_names = ([(name, <span class="string">'male'</span>) <span class="keyword">for</span> name <span class="keyword">in</span> names.words(<span class="string">'male.txt'</span>)] + [(name, <span class="string">'female'</span>) <span class="keyword">for</span> name <span class="keyword">in</span> names.words(<span class="string">'female.txt'</span>)])</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random.shuffle(labeled_names)</span><br><span class="line"><span class="comment"># divide into training set and test set and train</span></span><br><span class="line"><span class="comment">#featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]</span></span><br><span class="line"><span class="comment">#train_set, test_set = featuresets[500:], featuresets[:500]</span></span><br><span class="line"><span class="comment"># does not store all the feature sets in memory</span></span><br><span class="line"><span class="keyword">from</span> nltk.classify <span class="keyword">import</span> apply_features</span><br><span class="line">train_set = apply_features(gender_features, labeled_names[<span class="number">500</span>:])</span><br><span class="line">test_set = apply_features(gender_features, labeled_names[:<span class="number">500</span>])</span><br><span class="line">classifier = nltk.NaiveBayesClassifier.train(train_set)</span><br><span class="line"><span class="comment"># test</span></span><br><span class="line">classifier.classify(gender_features(<span class="string">'Neo'</span>))	<span class="comment"># male</span></span><br><span class="line">classifier.classify(gender_features(<span class="string">'Trinity'</span>))	<span class="comment"># female</span></span><br><span class="line"><span class="comment"># evaluate</span></span><br><span class="line">print(nltk.classify.accuracy(classifier, test_set))	<span class="comment"># 0.77</span></span><br><span class="line">classifier.show_most_informative_features(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Choosing the Right Features</strong><br>
充分理解手头任务，好好设计feature。<br>
可以先使用想到的所有可能feature，然后检查哪些才是真正有用的。feature用得太多容易对training data过拟合。<br>
使用一个dev set，用其进行error analysis。看看出错的都是哪些项，总结一下，修改feature。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gender_features2</span><span class="params">(name)</span>:</span></span><br><span class="line">    features = &#123;&#125;</span><br><span class="line">    features[<span class="string">"first_letter"</span>] = name[<span class="number">0</span>].lower()</span><br><span class="line">    features[<span class="string">"last_letter"</span>] = name[<span class="number">-1</span>].lower()</span><br><span class="line">    <span class="keyword">for</span> letter <span class="keyword">in</span> <span class="string">'abcdefghijklmnopqrstuvwxyz'</span>:</span><br><span class="line">        features[<span class="string">"count(&#123;&#125;)"</span>.format(letter)] = name.lower().count(letter)</span><br><span class="line">        features[<span class="string">"has(&#123;&#125;)"</span>.format(letter)] = (letter <span class="keyword">in</span> name.lower())</span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"><span class="comment"># use</span></span><br><span class="line">train_set = [(gender_features(n), gender) <span class="keyword">for</span> (n, gender) <span class="keyword">in</span> train_names]</span><br><span class="line">devtest_set = [(gender_features(n), gender) <span class="keyword">for</span> (n, gender) <span class="keyword">in</span> devtest_names]</span><br><span class="line">test_set = [(gender_features(n), gender) <span class="keyword">for</span> (n, gender) <span class="keyword">in</span> test_names]</span><br><span class="line">classifier = nltk.NaiveBayesClassifier.train(train_set) [<span class="number">1</span>]</span><br><span class="line">print(nltk.classify.accuracy(classifier, devtest_set))	<span class="comment"># 0.75</span></span><br><span class="line">errors = []</span><br><span class="line"><span class="keyword">for</span> (name, tag) <span class="keyword">in</span> devtest_names:</span><br><span class="line">    guess = classifier.classify(gender_features(name))</span><br><span class="line">    <span class="keyword">if</span> guess != tag:</span><br><span class="line">        errors.append( (tag, guess, name) )</span><br><span class="line"><span class="keyword">for</span> (tag, guess, name) <span class="keyword">in</span> sorted(errors):</span><br><span class="line">    print(<span class="string">'correct=&#123;:&lt;8&#125; guess=&#123;:&lt;8s&#125; name=&#123;:&lt;30&#125;'</span>.format(tag, guess, name))</span><br></pre></td></tr></table></figure>
<p><strong>Document Classification</strong><br>
看看一篇影评到底是在肯定还是否定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get movie reviews in a tuple</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> movie_reviews</span><br><span class="line">documents = [(list(movie_reviews.words(fileid)), category)</span><br><span class="line">             <span class="keyword">for</span> category <span class="keyword">in</span> movie_reviews.categories()</span><br><span class="line">             <span class="keyword">for</span> fileid <span class="keyword">in</span> movie_reviews.fileids(category)]</span><br><span class="line">random.shuffle(documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define feature extractor. 2000 most frequent words in the overall corpus, check whether each of these words is present in a given document</span></span><br><span class="line">all_words = nltk.FreqDist(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> movie_reviews.words())</span><br><span class="line">word_features = list(all_words)[:<span class="number">2000</span>] [<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">document_features</span><span class="params">(document)</span>:</span> [<span class="number">2</span>]</span><br><span class="line">    document_words = set(document) [<span class="number">3</span>]</span><br><span class="line">    features = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word_features:</span><br><span class="line">        features[<span class="string">'contains(&#123;&#125;)'</span>.format(word)] = (word <span class="keyword">in</span> document_words)</span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line">print(document_features(movie_reviews.words(<span class="string">'pos/cv957_8737.txt'</span>))) 	<span class="comment"># &#123;'contains(waste)': False, 'contains(lot)': False, ...&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train a classifier to label new movie reviews</span></span><br><span class="line">featuresets = [(document_features(d), c) <span class="keyword">for</span> (d,c) <span class="keyword">in</span> documents]</span><br><span class="line">train_set, test_set = featuresets[<span class="number">100</span>:], featuresets[:<span class="number">100</span>]</span><br><span class="line">classifier = nltk.NaiveBayesClassifier.train(train_set)</span><br><span class="line"></span><br><span class="line">print(nltk.classify.accuracy(classifier, test_set))	<span class="comment"># 0.81</span></span><br><span class="line">classifier.show_most_informative_features(<span class="number">5</span>)</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">Most Informative Features</span><br><span class="line">   contains(outstanding) = <span class="keyword">True</span>              pos : neg    =     <span class="number">11.1</span> : <span class="number">1.0</span></span><br><span class="line">        contains(seagal) = <span class="keyword">True</span>              neg : pos    =      <span class="number">7.7</span> : <span class="number">1.0</span></span><br><span class="line">   contains(wonderfully) = <span class="keyword">True</span>              pos : neg    =      <span class="number">6.8</span> : <span class="number">1.0</span></span><br><span class="line">         contains(damon) = <span class="keyword">True</span>              pos : neg    =      <span class="number">5.9</span> : <span class="number">1.0</span></span><br><span class="line">        contains(wasted) = <span class="keyword">True</span>              neg : pos    =      <span class="number">5.8</span> : <span class="number">1.0</span></span><br><span class="line"><span class="comment">###</span></span><br></pre></td></tr></table></figure>
<p><strong>Part-of-Speech Tagging</strong><br>
通过后缀猜测词性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># find the most common suffixes</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> brown</span><br><span class="line">suffix_fdist = nltk.FreqDist()</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> brown.words():</span><br><span class="line">    word = word.lower()</span><br><span class="line">    suffix_fdist[word[<span class="number">-1</span>:]] += <span class="number">1</span></span><br><span class="line">    suffix_fdist[word[<span class="number">-2</span>:]] += <span class="number">1</span></span><br><span class="line">    suffix_fdist[word[<span class="number">-3</span>:]] += <span class="number">1</span></span><br><span class="line">common_suffixes = [suffix <span class="keyword">for</span> (suffix, count) <span class="keyword">in</span> suffix_fdist.most_common(<span class="number">100</span>)]</span><br><span class="line">print(common_suffixes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define a feature extractor function which checks a given word for these suffixes</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pos_features</span><span class="params">(word)</span>:</span></span><br><span class="line">    features = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> suffix <span class="keyword">in</span> common_suffixes:</span><br><span class="line">        features[<span class="string">'endswith(&#123;&#125;)'</span>.format(suffix)] = word.lower().endswith(suffix)</span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line"><span class="comment"># train</span></span><br><span class="line">tagged_words = brown.tagged_words(categories=<span class="string">'news'</span>)</span><br><span class="line">featuresets = [(pos_features(n), g) <span class="keyword">for</span> (n,g) <span class="keyword">in</span> tagged_words]</span><br><span class="line">size = int(len(featuresets) * <span class="number">0.1</span>)</span><br><span class="line">train_set, test_set = featuresets[size:], featuresets[:size]</span><br><span class="line">classifier = nltk.DecisionTreeClassifier.train(train_set)</span><br><span class="line">nltk.classify.accuracy(classifier, test_set)	<span class="comment"># 0.62705121829935351</span></span><br><span class="line">classifier.classify(pos_features(<span class="string">'cats'</span>))	<span class="comment"># NNS</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># decision tree model is easy to interpret, we can print out the pseudocode</span></span><br><span class="line">print(classifier.pseudocode(depth=<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p><strong>Exploiting Context</strong><br>
不再只是孤立看待每个词，还要看上下文。所以参数不能只是一个词了，而是整句话。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pos_features</span><span class="params">(sentence, i)</span>:</span></span><br><span class="line">    features = &#123;<span class="string">"suffix(1)"</span>: sentence[i][<span class="number">-1</span>:],</span><br><span class="line">                <span class="string">"suffix(2)"</span>: sentence[i][<span class="number">-2</span>:],</span><br><span class="line">                <span class="string">"suffix(3)"</span>: sentence[i][<span class="number">-3</span>:]&#125;</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">        features[<span class="string">"prev-word"</span>] = <span class="string">"&lt;START&gt;"</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        features[<span class="string">"prev-word"</span>] = sentence[i<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"><span class="comment"># use</span></span><br><span class="line">pos_features(brown.sents()[<span class="number">0</span>], <span class="number">8</span>)	<span class="comment"># &#123;'suffix(3)': 'ion', 'prev-word': 'an', 'suffix(2)': 'on', 'suffix(1)': 'n'&#125;</span></span><br><span class="line">tagged_sents = brown.tagged_sents(categories=<span class="string">'news'</span>)</span><br><span class="line">featuresets = []</span><br><span class="line"><span class="keyword">for</span> tagged_sent <span class="keyword">in</span> tagged_sents:</span><br><span class="line">    untagged_sent = nltk.tag.untag(tagged_sent)</span><br><span class="line">    <span class="keyword">for</span> i, (word, tag) <span class="keyword">in</span> enumerate(tagged_sent):</span><br><span class="line">        featuresets.append( (pos_features(untagged_sent, i), tag) )</span><br><span class="line"></span><br><span class="line">size = int(len(featuresets) * <span class="number">0.1</span>)</span><br><span class="line">train_set, test_set = featuresets[size:], featuresets[:size]</span><br><span class="line">classifier = nltk.NaiveBayesClassifier.train(train_set)</span><br><span class="line"></span><br><span class="line">nltk.classify.accuracy(classifier, test_set)	<span class="comment"># 0.78915962207856782</span></span><br></pre></td></tr></table></figure>
<p><strong>Sequence Classification</strong><br>
In order to capture the dependencies between related classification tasks, we can use joint classifier models, which choose an appropriate labeling for a collection of related inputs. In the case of part-of-speech tagging, a variety of different sequence classifier models can be used to jointly choose part-of-speech tags for all the words in a given sentence.<br>
Consecutive classification(greedy sequence classification):  find the most likely class label for the first input, then to use that answer to help find the best label for the next input. The process can then be repeated until all of the inputs have been labeled.  Used by bigram tagger.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Part of Speech Tagging with a Consecutive Classifier</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pos_features</span><span class="params">(sentence, i, history)</span>:</span></span><br><span class="line">     features = &#123;<span class="string">"suffix(1)"</span>: sentence[i][<span class="number">-1</span>:],</span><br><span class="line">                 <span class="string">"suffix(2)"</span>: sentence[i][<span class="number">-2</span>:],</span><br><span class="line">                 <span class="string">"suffix(3)"</span>: sentence[i][<span class="number">-3</span>:]&#125;</span><br><span class="line">     <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">         features[<span class="string">"prev-word"</span>] = <span class="string">"&lt;START&gt;"</span></span><br><span class="line">         features[<span class="string">"prev-tag"</span>] = <span class="string">"&lt;START&gt;"</span></span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         features[<span class="string">"prev-word"</span>] = sentence[i<span class="number">-1</span>]</span><br><span class="line">         features[<span class="string">"prev-tag"</span>] = history[i<span class="number">-1</span>]</span><br><span class="line">     <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConsecutivePosTagger</span><span class="params">(nltk.TaggerI)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_sents)</span>:</span></span><br><span class="line">        train_set = []</span><br><span class="line">        <span class="keyword">for</span> tagged_sent <span class="keyword">in</span> train_sents:</span><br><span class="line">            untagged_sent = nltk.tag.untag(tagged_sent)</span><br><span class="line">            history = []</span><br><span class="line">            <span class="keyword">for</span> i, (word, tag) <span class="keyword">in</span> enumerate(tagged_sent):</span><br><span class="line">                featureset = pos_features(untagged_sent, i, history)</span><br><span class="line">                train_set.append( (featureset, tag) )</span><br><span class="line">                history.append(tag)</span><br><span class="line">        self.classifier = nltk.NaiveBayesClassifier.train(train_set)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tag</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        history = []</span><br><span class="line">        <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(sentence):</span><br><span class="line">            featureset = pos_features(sentence, i, history)</span><br><span class="line">            tag = self.classifier.classify(featureset)</span><br><span class="line">            history.append(tag)</span><br><span class="line">        <span class="keyword">return</span> zip(sentence, history)</span><br><span class="line"><span class="comment"># use</span></span><br><span class="line">tagged_sents = brown.tagged_sents(categories=<span class="string">'news'</span>)</span><br><span class="line">size = int(len(tagged_sents) * <span class="number">0.1</span>)</span><br><span class="line">train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]</span><br><span class="line">tagger = ConsecutivePosTagger(train_sents)</span><br><span class="line">print(tagger.evaluate(test_sents))	<span class="comment"># 0.79796012981</span></span><br></pre></td></tr></table></figure>
<p><strong>Other Methods for Sequence Classification</strong><br>
之前的算法都是定了tag就不能回头。<br>
一种方法是改用一个transformational strategy的classifier。Transformational joint classifiers work by creating an initial assignment of labels for the inputs, and then iteratively refining that assignment in an attempt to repair inconsistencies between related inputs. 比如The Brill tagger.<br>
另一种方法是给每个tag打分，然后比较总分。就是Hidden Markov Models用的方法。还有Maximum Entropy Markov和Linear-Chain Conditional Random Field Models。</p>
<h3 id="further-examples-of-supervised-classification"><a class="markdownIt-Anchor" href="#further-examples-of-supervised-classification"></a> Further Examples of Supervised Classification</h3>
<p><strong>Sentence Segmentation</strong><br>
Sentence Segmentation: whenever we encounter a symbol that could possibly end a sentence, such as a period or a question mark, we have to decide whether it terminates the preceding sentence.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># obtain some data that has already been segmented into sentences and convert it into a form that is suitable for extracting features</span></span><br><span class="line">sents = nltk.corpus.treebank_raw.sents()</span><br><span class="line">tokens = []</span><br><span class="line">boundaries = set()</span><br><span class="line">offset = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> sents:</span><br><span class="line">    tokens.extend(sent)	<span class="comment"># token is a merged list of tokens from the individual sentences</span></span><br><span class="line">    offset += len(sent)</span><br><span class="line">    boundaries.add(offset<span class="number">-1</span>)	<span class="comment"># boundaries is a set containing the indexes of all sentence-boundary tokens.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># specify the features</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">punct_features</span><span class="params">(tokens, i)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'next-word-capitalized'</span>: tokens[i+<span class="number">1</span>][<span class="number">0</span>].isupper(),</span><br><span class="line">            <span class="string">'prev-word'</span>: tokens[i<span class="number">-1</span>].lower(),</span><br><span class="line">            <span class="string">'punct'</span>: tokens[i],</span><br><span class="line">            <span class="string">'prev-word-is-one-char'</span>: len(tokens[i<span class="number">-1</span>]) == <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a list of labeled featuresets by selecting all the punctuation tokens, and tagging whether they are boundary tokens or not.</span></span><br><span class="line">featuresets = [(punct_features(tokens, i), (i <span class="keyword">in</span> boundaries))</span><br><span class="line">               <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(tokens)<span class="number">-1</span>)</span><br><span class="line">               <span class="keyword">if</span> tokens[i] <span class="keyword">in</span> <span class="string">'.?!'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># train and evaluate a punctuation classifier</span></span><br><span class="line">size = int(len(featuresets) * <span class="number">0.1</span>)</span><br><span class="line">train_set, test_set = featuresets[size:], featuresets[:size]</span><br><span class="line">classifier = nltk.NaiveBayesClassifier.train(train_set)</span><br><span class="line">nltk.classify.accuracy(classifier, test_set)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">segment_sentences</span><span class="params">(words)</span>:</span></span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    sents = []</span><br><span class="line">    <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(words):</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> <span class="string">'.?!'</span> <span class="keyword">and</span> classifier.classify(punct_features(words, i)) == <span class="keyword">True</span>:</span><br><span class="line">            sents.append(words[start:i+<span class="number">1</span>])</span><br><span class="line">            start = i+<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> start &lt; len(words):</span><br><span class="line">        sents.append(words[start:])</span><br><span class="line">    <span class="keyword">return</span> sents</span><br></pre></td></tr></table></figure>
<p><strong>Identifying Dialogue Act Types</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># extract the basic messaging data</span></span><br><span class="line">posts = nltk.corpus.nps_chat.xml_posts()[:<span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># define a feature extractor that checks what words the post contains</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dialogue_act_features</span><span class="params">(post)</span>:</span></span><br><span class="line">    features = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> nltk.word_tokenize(post):</span><br><span class="line">        features[<span class="string">'contains(&#123;&#125;)'</span>.format(word.lower())] = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct the training and testing data by applying the feature extractor to each post and create a new classifier</span></span><br><span class="line">featuresets = [(dialogue_act_features(post.text), post.get(<span class="string">'class'</span>))</span><br><span class="line">               <span class="keyword">for</span> post <span class="keyword">in</span> posts]</span><br><span class="line">size = int(len(featuresets) * <span class="number">0.1</span>)</span><br><span class="line">train_set, test_set = featuresets[size:], featuresets[:size]</span><br><span class="line">classifier = nltk.NaiveBayesClassifier.train(train_set)</span><br><span class="line">print(nltk.classify.accuracy(classifier, test_set))	<span class="comment"># 0.67</span></span><br></pre></td></tr></table></figure>
<p><strong>Recognizing Textual Entailment</strong><br>
Recognizing Textual Entailment: the task of determining whether a given piece of text T entails another text called the “hypothesis”.<br>
两句话是否是合理的上下文对话。注意这个合理并不是逻辑上，而是whether a human would conclude that the text provides reasonable evidence for taking the hypothesis to be true.也就是从text中能否合理推论出hypothesis的论点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># word types serve as proxies for information, and out features count the degree of word overlap, and the degree to which there are words in the hypothesis but not in the text</span></span><br><span class="line"><span class="comment"># not all words are equally important, Named Entity is more significant. some high frequency function function words are filtered out as stopwords.</span></span><br><span class="line"><span class="comment"># "Recognizing Text Entailment" Feature Extractor. The RTEFeatureExtractor class builds a bag of words for both the text and the hypothesis after throwing away some stopwords, then calculates overlap and difference.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rte_features</span><span class="params">(rtepair)</span>:</span></span><br><span class="line">    extractor = nltk.RTEFeatureExtractor(rtepair)</span><br><span class="line">    features = &#123;&#125;</span><br><span class="line">    features[<span class="string">'word_overlap'</span>] = len(extractor.overlap(<span class="string">'word'</span>))</span><br><span class="line">    features[<span class="string">'word_hyp_extra'</span>] = len(extractor.hyp_extra(<span class="string">'word'</span>))</span><br><span class="line">    features[<span class="string">'ne_overlap'</span>] = len(extractor.overlap(<span class="string">'ne'</span>))</span><br><span class="line">    features[<span class="string">'ne_hyp_extra'</span>] = len(extractor.hyp_extra(<span class="string">'ne'</span>))</span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line">rtepair = nltk.corpus.rte.pairs([<span class="string">'rte3_dev.xml'</span>])[<span class="number">33</span>]</span><br><span class="line">extractor = nltk.RTEFeatureExtractor(rtepair)</span><br><span class="line">print(extractor.text_words)	<span class="comment"># &#123;'Russia', 'Organisation', 'Shanghai', 'Asia', 'four', 'at', 'operation', 'SCO', ...&#125;</span></span><br><span class="line">print(extractor.hyp_words)	<span class="comment"># &#123;'member', 'SCO', 'China'&#125;</span></span><br><span class="line">print(extractor.overlap(<span class="string">'word'</span>))	<span class="comment"># set()</span></span><br><span class="line">print(extractor.overlap(<span class="string">'ne'</span>))	<span class="comment"># &#123;'SCO', 'China'&#125;</span></span><br><span class="line">print(extractor.hyp_extra(<span class="string">'word'</span>))	<span class="comment"># &#123;'member'&#125;</span></span><br></pre></td></tr></table></figure>
<p><strong>Scaling Up to Large Datasets</strong><br>
python的效率比较低，如果数据量大的话看看NLTK’s facilities for interfacing with external machine learning packages.</p>
<h3 id="evaluation"><a class="markdownIt-Anchor" href="#evaluation"></a> Evaluation</h3>
<p>看看classification model到底可靠否。</p>
<p><strong>The Test Set</strong><br>
即evaluation set。training data和test data不要太相似，别是同一个种类同一个文件的。</p>
<p><strong>Accuracy</strong><br>
nltk.classify.accuracy()</p>
<p><strong>Precision and Recall</strong></p>
<ul>
<li><strong>True positives</strong> are relevant items that we correctly identified as relevant.</li>
<li><strong>True negatives</strong> are irrelevant items that we correctly identified as irrelevant.</li>
<li><strong>False positives</strong> (or Type I errors) are irrelevant items that we incorrectly identified as relevant.</li>
<li><strong>False negatives</strong> (or Type II errors) are relevant items that we incorrectly identified as irrelevant.</li>
</ul>
<p>于是</p>
<ul>
<li><strong>Precision</strong>, which indicates how many of the items that we identified were relevant, is TP/(TP+FP).</li>
<li><strong>Recall</strong>, which indicates how many of the relevant items that we identified, is TP/(TP+FN).</li>
<li><strong>The F-Measure</strong> (or F-Score), which combines the precision and recall to give a single score, is defined to be the harmonic mean of the precision and recall: (2 × Precision × Recall) / (Precision + Recall).</li>
</ul>
<p><strong>Confusion Matrices</strong><br>
A confusion matrix is a table where each cell [i,j] indicates how often label j was predicted when the correct label was i.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">tag_list</span><span class="params">(tagged_sents)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> [tag <span class="keyword">for</span> sent <span class="keyword">in</span> tagged_sents <span class="keyword">for</span> (word, tag) <span class="keyword">in</span> sent]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">apply_tagger</span><span class="params">(tagger, corpus)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> [tagger.tag(nltk.tag.untag(sent)) <span class="keyword">for</span> sent <span class="keyword">in</span> corpus]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>gold = tag_list(brown.tagged_sents(categories=<span class="string">'editorial'</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>test = tag_list(apply_tagger(t2, brown.tagged_sents(categories=<span class="string">'editorial'</span>)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cm = nltk.ConfusionMatrix(gold, test)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(cm.pretty_format(sort_by_count=<span class="keyword">True</span>, show_percents=<span class="keyword">True</span>, truncate=<span class="number">9</span>))</span><br><span class="line">    |                                         N                      |</span><br><span class="line">    |      N      I      A      J             N             V      N |</span><br><span class="line">    |      N      N      T      J      .      S      ,      B      P |</span><br><span class="line">----+----------------------------------------------------------------+</span><br><span class="line"> NN | &lt;<span class="number">11.8</span>%&gt;  <span class="number">0.0</span>%      .   <span class="number">0.2</span>%      .   <span class="number">0.0</span>%      .   <span class="number">0.3</span>%   <span class="number">0.0</span>% |</span><br><span class="line"> IN |   <span class="number">0.0</span>%  &lt;<span class="number">9.0</span>%&gt;     .      .      .   <span class="number">0.0</span>%      .      .      . |</span><br><span class="line"> AT |      .      .  &lt;<span class="number">8.6</span>%&gt;     .      .      .      .      .      . |</span><br><span class="line"> JJ |   <span class="number">1.7</span>%      .      .  &lt;<span class="number">3.9</span>%&gt;     .      .      .   <span class="number">0.0</span>%   <span class="number">0.0</span>% |</span><br><span class="line">  . |      .      .      .      .  &lt;<span class="number">4.8</span>%&gt;     .      .      .      . |</span><br><span class="line">NNS |   <span class="number">1.5</span>%      .      .      .      .  &lt;<span class="number">3.2</span>%&gt;     .      .   <span class="number">0.0</span>% |</span><br><span class="line">  , |      .      .      .      .      .      .  &lt;<span class="number">4.4</span>%&gt;     .      . |</span><br><span class="line"> VB |   <span class="number">0.9</span>%      .      .   <span class="number">0.0</span>%      .      .      .  &lt;<span class="number">2.4</span>%&gt;     . |</span><br><span class="line"> NP |   <span class="number">1.0</span>%      .      .   <span class="number">0.0</span>%      .      .      .      .  &lt;<span class="number">1.8</span>%&gt;|</span><br><span class="line">----+----------------------------------------------------------------+</span><br><span class="line">(row = reference; col = test)</span><br></pre></td></tr></table></figure>
<p><strong>Cross-Validation</strong><br>
perform multiple evaluations on different test sets, then to combine the scores from those evaluations, a technique known as cross-validation.</p>
<h3 id="decision-trees"><a class="markdownIt-Anchor" href="#decision-trees"></a> Decision Trees</h3>
<p>Three machine learning methods that can be used to automatically build classification models: <strong>decision trees, naive Bayes classifiers, and Maximum Entropy classifiers</strong>.<br>
<img src="http://www.nltk.org/images/decision-tree.png" alt="Decision Tree model for the name gender task. Note that tree diagrams are conventionally drawn &quot;upside down,&quot; with the root at the top, and the leaves at the bottom."></p>
<p><strong>Entropy and Information Gain</strong><br>
information gain, measures how much more organized the input values become when we divide them up using a given feature. To measure how disorganized the original set of input values are, we calculate entropy of their labels, which will be high if the input values have highly varied labels, and low if many input values all have the same label. In particular, entropy is defined as the sum of the probability of each label times the log probability of that same label. 每个label都要有一定分量才好，不要只有一个label最多。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span><span class="params">(labels)</span>:</span></span><br><span class="line">    freqdist = nltk.FreqDist(labels)</span><br><span class="line">    probs = [freqdist.freq(l) <span class="keyword">for</span> l <span class="keyword">in</span> freqdist]</span><br><span class="line">    <span class="keyword">return</span> -sum(p * math.log(p,<span class="number">2</span>) <span class="keyword">for</span> p <span class="keyword">in</span> probs)</span><br><span class="line"><span class="comment"># use</span></span><br><span class="line">print(entropy([<span class="string">'male'</span>, <span class="string">'male'</span>, <span class="string">'male'</span>, <span class="string">'male'</span>]))	<span class="comment"># 0.0</span></span><br><span class="line">print(entropy([<span class="string">'female'</span>, <span class="string">'male'</span>, <span class="string">'female'</span>, <span class="string">'male'</span>]))	<span class="comment"># 1.0</span></span><br><span class="line">print(entropy([<span class="string">'female'</span>, <span class="string">'female'</span>, <span class="string">'male'</span>, <span class="string">'female'</span>]))	<span class="comment"># 0.811</span></span><br></pre></td></tr></table></figure>
<p>Decision Tree的缺点是分支多就需要更大的training data，branches几何级数增长，影响力小的feature很难用上，被迫按照一定顺序进行判断。</p>
<h3 id="naive-bayes-classifiers"><a class="markdownIt-Anchor" href="#naive-bayes-classifiers"></a> Naive Bayes Classifiers</h3>
<p>In naive Bayes classifiers, every feature gets a say in determining which label should be assigned to a given input value. To choose a label for an input value, the naive Bayes classifier begins by calculating the prior probability of each label, which is determined by checking frequency of each label in the training set. The contribution from each feature is then combined with this prior probability, to arrive at a likelihood estimate for each label. The label whose likelihood estimate is the highest is then assigned to the input value.<br>
<img src="http://www.nltk.org/images/naive-bayes-triangle.png" alt="An abstract illustration of the procedure used by the naive Bayes classifier to choose the topic for a document. In the training corpus, most documents are automotive, so the classifier starts out at a point closer to the &quot;automotive&quot; label. But it then considers the effect of each feature. In this example, the input document contains the word &quot;dark,&quot; which is a weak indicator for murder mysteries, but it also contains the word &quot;football,&quot; which is a strong indicator for sports documents. After every feature has made its contribution, the classifier checks which label it is closest to, and assigns that label to the input."><br>
每个feature在每种label里的可能性自己乘起来。是假设feature之间是完全独立的，所以naive。<br>
<img src="http://www.nltk.org/images/naive_bayes_bargraph.png" alt="Calculating label likelihoods with naive Bayes. Naive Bayes begins by calculating the prior probability of each label, based on how frequently each label occurs in the training data. Every feature then contributes to the likelihood estimate for each label, by multiplying it by the probability that input values with that label will have that feature. The resulting likelihood score can be thought of as an estimate of the probability that a randomly selected value from the training set would have both the given label and the set of features, assuming that the feature probabilities are all independent."></p>
<h3 id="maximum-entropy-classifiers"><a class="markdownIt-Anchor" href="#maximum-entropy-classifiers"></a> Maximum Entropy Classifiers</h3>
<p>不用probabilites，而是用search techniques to find a set of parameters that will maximize the performance of the classifier.<br>
In particular, it looks for the set of parameters that maximizes the total likelihood of the training corpus, which is defined as:<br>
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo>)</mo><mo>=</mo><msub><mo>∑</mo><mrow><mi>x</mi><mi mathvariant="normal">∣</mi><mi>i</mi><mi>n</mi><mi mathvariant="normal">∣</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>p</mi><mi>u</mi><mi>s</mi></mrow></msub><mi>P</mi><mo>(</mo><mi>l</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mo>(</mo><mi>x</mi><mo>)</mo><mi mathvariant="normal">∣</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">P(features) = \sum_{x|in|corpus}P(label(x)|features(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.22501em;vertical-align:-0.47501em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mord mathit">e</span><span class="mord mathit">a</span><span class="mord mathit">t</span><span class="mord mathit">u</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">e</span><span class="mord mathit">s</span><span class="mclose">)</span><span class="mrel">=</span><span class="mop"><span class="op-symbol small-op mop" style="top:-0.0000050000000000050004em;">∑</span><span class="vlist"><span style="top:0.30001em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">x</span><span class="mord mathrm">∣</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathrm">∣</span><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">p</span><span class="mord mathit">u</span><span class="mord mathit">s</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">a</span><span class="mord mathit">b</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mord mathit">e</span><span class="mord mathit">a</span><span class="mord mathit">t</span><span class="mord mathit">u</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">e</span><span class="mord mathit">s</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>l</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi mathvariant="normal">∣</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo>)</mo><mo>=</mo><mi>P</mi><mo>(</mo><mi>l</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mo separator="true">,</mo><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo>)</mo><mi mathvariant="normal">/</mi><msub><mo>∑</mo><mrow><mi>l</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi></mrow></msub><mi>P</mi><mo>(</mo><mi>l</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mo separator="true">,</mo><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(label|features) = P(label, features) / \sum_{label}P(label, features)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.0500099999999999em;vertical-align:-0.30001em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">a</span><span class="mord mathit">b</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mord mathit">e</span><span class="mord mathit">a</span><span class="mord mathit">t</span><span class="mord mathit">u</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">e</span><span class="mord mathit">s</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">a</span><span class="mord mathit">b</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mord mathit">e</span><span class="mord mathit">a</span><span class="mord mathit">t</span><span class="mord mathit">u</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">e</span><span class="mord mathit">s</span><span class="mclose">)</span><span class="mord mathrm">/</span><span class="mop"><span class="op-symbol small-op mop" style="top:-0.0000050000000000050004em;">∑</span><span class="vlist"><span style="top:0.30001em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">a</span><span class="mord mathit">b</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">a</span><span class="mord mathit">b</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mord mathit">e</span><span class="mord mathit">a</span><span class="mord mathit">t</span><span class="mord mathit">u</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">e</span><span class="mord mathit">s</span><span class="mclose">)</span></span></span></span><br>
使用iterative optimization，从随机参数开始，逐步迭代改进。算法可能会比较慢。</p>
<h3 id="generative-vs-conditional-classifiers"><a class="markdownIt-Anchor" href="#generative-vs-conditional-classifiers"></a> Generative vs Conditional Classifiers</h3>
<p><strong>Naive Bayes Classifier</strong> is an example of a generative classifier, which builds a model that predicts <code>P(input, label)</code>, the joint probability of a (input, label) pair.可以回答以下问题：</p>
<ol>
<li>What is the most likely label for a given input?</li>
<li>How likely is a given label for a given input?</li>
<li>What is the most likely input value?</li>
<li>How likely is a given input value?</li>
<li>How likely is a given input value with a given label?</li>
<li>What is the most likely label for an input that might have one of two values (but we don’t know which)?</li>
</ol>
<p><strong>Maximum Entropy Classifier</strong> is an example of a conditional classifier. Conditional classifiers build models that predict <code>P(label|input)</code> — the probability of a label given the input value. Thus, conditional models can still be used to answer questions 1 and 2. However, conditional models can not be used to answer the remaining questions 3-6.</p>
<h3 id="modeling-linguistic-patterns"><a class="markdownIt-Anchor" href="#modeling-linguistic-patterns"></a> Modeling Linguistic Patterns</h3>
<p>Classifier的存在是为了更好地分析Linguistic Patterns。<br>
descriptive models and explanatory models.<br>
Descriptive models capture patterns in the data but they don’t provide any information about why the data contains those patterns.<br>
Explanatory models attempt to capture properties and relationships that cause the linguistic patterns.<br>
从corpus直接分析出来的一般都是descriptive models。</p>
<hr>
<h2 id="extracting-information-from-text"><a class="markdownIt-Anchor" href="#extracting-information-from-text"></a> Extracting Information from Text</h2>
<p>本章要解决的问题：</p>
<ol>
<li>How can we build a system that extracts structured data, such as tables, from unstructured text?</li>
<li>What are some robust methods for identifying the entities and relationships described in a text?</li>
<li>Which corpora are appropriate for this work, and how do we use them for training and evaluating our models?</li>
</ol>
<h3 id="information-extraction"><a class="markdownIt-Anchor" href="#information-extraction"></a> Information Extraction</h3>
<p>First convert the unstructured data of natural language sentences into the structured data of the text. Then we reap the benefits of powerful query tools such as SQL.<br>
使用此技术的应用有：business intelligence, resume harvesting, media analysis, sentiment detection, patent search, and email scanning.</p>
<p><strong>Information Extraction Architecture</strong><br>
<img src="http://www.nltk.org/images/ie-architecture.png" alt="Simple Pipeline Architecture for an Information Extraction System. This system takes the raw text of a document as its input, and generates a list of (entity, relation, entity) tuples as its output. For example, given a document that indicates that the company Georgia-Pacific is located in Atlanta, it might generate the tuple ([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta'])."></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## first 3 steps</span></span><br><span class="line"><span class="keyword">import</span> nltk, re, pprint</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ie_preprocess</span><span class="params">(document)</span>:</span></span><br><span class="line">   sentences = nltk.sent_tokenize(document)</span><br><span class="line">   sentences = [nltk.word_tokenize(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> sentences]</span><br><span class="line">   sentences = [nltk.pos_tag(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> sentences]</span><br></pre></td></tr></table></figure>
<p>Next, in named entity detection, we segment and label the entities that might participate in interesting relations with one another.<br>
Finally, in relation extraction, we search for specific patterns between pairs of entities that occur near one another in the text, and use those patterns to build tuples recording the relationships between the entities.</p>
<h3 id="chunking"><a class="markdownIt-Anchor" href="#chunking"></a> Chunking</h3>
<p>Entity Detection中一个基础技术Chunking。小方块表示word-level tokenization和part-of-speech tagging，大方块表示higher-level chunking。大方块就是Chunking。chunk们不会重叠。<br>
<img src="http://www.nltk.org/images/chunk-segmentation.png" alt="Segmentation and Labeling at both the Token and Chunk Levels"></p>
<p><strong>Noun Phrase Chunking</strong><br>
NP-chunking。找individual noun phrases。part-of-speech tags在这里很有用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a simple regular expression based NP Chunker</span></span><br><span class="line">sentence = [(<span class="string">"the"</span>, <span class="string">"DT"</span>), (<span class="string">"little"</span>, <span class="string">"JJ"</span>), (<span class="string">"yellow"</span>, <span class="string">"JJ"</span>), (<span class="string">"dog"</span>, <span class="string">"NN"</span>), (<span class="string">"barked"</span>, <span class="string">"VBD"</span>), (<span class="string">"at"</span>, <span class="string">"IN"</span>),  (<span class="string">"the"</span>, <span class="string">"DT"</span>), (<span class="string">"cat"</span>, <span class="string">"NN"</span>)]</span><br><span class="line">grammar = <span class="string">"NP: &#123;&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;&#125;"</span></span><br><span class="line">cp = nltk.RegexpParser(grammar)</span><br><span class="line">result = cp.parse(sentence)</span><br><span class="line">print(result)</span><br><span class="line"><span class="string">'''</span><br><span class="line">(S</span><br><span class="line">  (NP the/DT little/JJ yellow/JJ dog/NN)</span><br><span class="line">  barked/VBD</span><br><span class="line">  at/IN</span><br><span class="line">  (NP the/DT cat/NN))</span><br><span class="line">'''</span></span><br><span class="line">result.draw()</span><br></pre></td></tr></table></figure>
<p><img src="http://www.nltk.org/book/tree_images/ch07-tree-1.png" alt="Image Loading"></p>
<p><strong>Tag Patterns</strong><br>
如<code>&lt;DT&gt;?&lt;JJ.*&gt;*&lt;NN.*&gt;+</code>表示 any sequence of tokens beginning with an optional determiner, followed by zero or more adjectives of any type (including relative adjectives like earlier/JJR), followed by one or more nouns of any type.</p>
<p><strong>Chunking with Regular Expressions</strong><br>
可以同时指定多条chunking rule。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">grammar = <span class="string">r"""</span><br><span class="line">  NP: &#123;&lt;DT|PP\$&gt;?&lt;JJ&gt;*&lt;NN&gt;&#125;   # chunk determiner/possessive, adjectives and noun</span><br><span class="line">      &#123;&lt;NNP&gt;+&#125;                # chunk sequences of proper nouns</span><br><span class="line">"""</span></span><br><span class="line">cp = nltk.RegexpParser(grammar)</span><br><span class="line">sentence = [(<span class="string">"Rapunzel"</span>, <span class="string">"NNP"</span>), (<span class="string">"let"</span>, <span class="string">"VBD"</span>), (<span class="string">"down"</span>, <span class="string">"RP"</span>), [<span class="number">1</span>]</span><br><span class="line">                 (<span class="string">"her"</span>, <span class="string">"PP$"</span>), (<span class="string">"long"</span>, <span class="string">"JJ"</span>), (<span class="string">"golden"</span>, <span class="string">"JJ"</span>), (<span class="string">"hair"</span>, <span class="string">"NN"</span>)]</span><br><span class="line">print(cp.parse(sentence))</span><br><span class="line"><span class="string">'''</span><br><span class="line">(S</span><br><span class="line">  (NP Rapunzel/NNP)</span><br><span class="line">  let/VBD</span><br><span class="line">  down/RP</span><br><span class="line">  (NP her/PP$ long/JJ golden/JJ hair/NN))</span><br><span class="line">'''</span></span><br></pre></td></tr></table></figure>
<p><strong>Exploring Text Corpora</strong><br>
extract phrases matching a particular sequence of part-of-speech tags.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cp = nltk.RegexpParser(<span class="string">'CHUNK: &#123;&lt;V.*&gt; &lt;TO&gt; &lt;V.*&gt;&#125;'</span>)</span><br><span class="line">brown = nltk.corpus.brown</span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> brown.tagged_sents():</span><br><span class="line">    tree = cp.parse(sent)</span><br><span class="line">    <span class="keyword">for</span> subtree <span class="keyword">in</span> tree.subtrees():</span><br><span class="line">        <span class="keyword">if</span> subtree.label() == <span class="string">'CHUNK'</span>: print(subtree)</span><br><span class="line"><span class="string">'''</span><br><span class="line">(CHUNK combined/VBN to/TO achieve/VB)</span><br><span class="line">(CHUNK continue/VB to/TO place/VB)</span><br><span class="line">(CHUNK serve/VB to/TO protect/VB)</span><br><span class="line">(CHUNK wanted/VBD to/TO wait/VB)</span><br><span class="line">(CHUNK allowed/VBN to/TO place/VB)</span><br><span class="line">(CHUNK expected/VBN to/TO become/VB)</span><br><span class="line">...</span><br><span class="line">(CHUNK seems/VBZ to/TO overtake/VB)</span><br><span class="line">(CHUNK want/VB to/TO buy/VB)</span><br><span class="line">'''</span></span><br></pre></td></tr></table></figure>
<p><strong>Chinking</strong><br>
Chink is a sequence of tokens that is not included in a chunk.</p>
<p>||Entire chunk|	Middle of a chunk|	End of a chunk|<br>
|—|---|—|<br>
|Input|	[a/DT little/JJ dog/NN]|	[a/DT little/JJ dog/NN]|	[a/DT little/JJ dog/NN]<br>
|Operation|	Chink “DT JJ NN”|	Chink “JJ”|	Chink “NN”<br>
|Pattern|	}DT JJ NN{|	}JJ{|	}NN{<br>
|Output|	a/DT little/JJ dog/NN|	[a/DT] little/JJ [dog/NN]|	[a/DT little/JJ] dog/NN</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># put the entire sentence into a single chunk, then excise the chinks.</span></span><br><span class="line">grammar = <span class="string">r"""</span><br><span class="line">  NP:</span><br><span class="line">    &#123;&lt;.*&gt;+&#125;          # Chunk everything</span><br><span class="line">    &#125;&lt;VBD|IN&gt;+&#123;      # Chink sequences of VBD and IN</span><br><span class="line">  """</span></span><br><span class="line">sentence = [(<span class="string">"the"</span>, <span class="string">"DT"</span>), (<span class="string">"little"</span>, <span class="string">"JJ"</span>), (<span class="string">"yellow"</span>, <span class="string">"JJ"</span>),</span><br><span class="line">       (<span class="string">"dog"</span>, <span class="string">"NN"</span>), (<span class="string">"barked"</span>, <span class="string">"VBD"</span>), (<span class="string">"at"</span>, <span class="string">"IN"</span>),  (<span class="string">"the"</span>, <span class="string">"DT"</span>), (<span class="string">"cat"</span>, <span class="string">"NN"</span>)]</span><br><span class="line">cp = nltk.RegexpParser(grammar)</span><br><span class="line">print(cp.parse(sentence))</span><br><span class="line"><span class="string">'''</span><br><span class="line">(S</span><br><span class="line">   (NP the/DT little/JJ yellow/JJ dog/NN)</span><br><span class="line">   barked/VBD</span><br><span class="line">   at/IN</span><br><span class="line">   (NP the/DT cat/NN))</span><br><span class="line">'''</span></span><br></pre></td></tr></table></figure>
<p><strong>Representing Chunks: Tags vs Trees</strong><br>
最广泛使用的文件表达方式用了IOB tags。In this scheme, each token is tagged with one of three special chunk tags, I (inside), O (outside), or B (begin). A token is tagged as B if it marks the beginning of a chunk. Subsequent tokens within the chunk are tagged I. All other tokens are tagged O.<br>
表现成Tag是这样的：<br>
<img src="http://www.nltk.org/images/chunk-tagrep.png" alt="Tag Representation of Chunk Structures"><br>
在文件中表示是这样的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">We PRP B-NP</span><br><span class="line">saw VBD O</span><br><span class="line">the DT B-NP</span><br><span class="line">yellow JJ I-NP</span><br><span class="line">dog NN I-NP</span><br></pre></td></tr></table></figure>
<p>表示成Tree是这样的：<br>
<img src="http://www.nltk.org/images/chunk-treerep.png" alt="Tree Representation of Chunk Structures"></p>
<h3 id="developing-and-evaluating-chunkers"><a class="markdownIt-Anchor" href="#developing-and-evaluating-chunkers"></a> Developing and Evaluating Chunkers</h3>
<p><strong>Reading IOB Format and the CoNLL 2000 Corpus</strong><br>
<code>nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()</code><br>
<img src="http://www.nltk.org/book/tree_images/ch07-tree-2.png" alt="Image Loading"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从CoNLL 2000里拿出100句NP chunk的句子。</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> conll2000</span><br><span class="line">print(conll2000.chunked_sents(<span class="string">'train.txt'</span>, chunk_types=[<span class="string">'NP'</span>])[<span class="number">99</span>])</span><br></pre></td></tr></table></figure>
<p><strong>Simple Evaluation and Baselines</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Noun Phrase Chunking with a Unigram Tagger</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UnigramChunker</span><span class="params">(nltk.ChunkParserI)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_sents)</span>:</span>	<span class="comment"># train_sents is chunk tree</span></span><br><span class="line">        train_data = [[(t,c) <span class="keyword">for</span> w,t,c <span class="keyword">in</span> nltk.chunk.tree2conlltags(sent)]	<span class="comment"># word, tag, chunk triples</span></span><br><span class="line">                      <span class="keyword">for</span> sent <span class="keyword">in</span> train_sents]</span><br><span class="line">        self.tagger = nltk.UnigramTagger(train_data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        pos_tags = [pos <span class="keyword">for</span> (word,pos) <span class="keyword">in</span> sentence]	<span class="comment"># sentence is a tagged sentence, extract the part-of-speech tag from that sentence</span></span><br><span class="line">        tagged_pos_tags = self.tagger.tag(pos_tags)	<span class="comment"># tag the part-of-speech tags with IOB chunk tags</span></span><br><span class="line">        chunktags = [chunktag <span class="keyword">for</span> (pos, chunktag) <span class="keyword">in</span> tagged_pos_tags]</span><br><span class="line">        conlltags = [(word, pos, chunktag) <span class="keyword">for</span> ((word,pos),chunktag)</span><br><span class="line">                     <span class="keyword">in</span> zip(sentence, chunktags)]</span><br><span class="line">        <span class="keyword">return</span> nltk.chunk.conlltags2tree(conlltags)	<span class="comment"># convert result back into a chunk tree</span></span><br><span class="line"><span class="comment"># use</span></span><br><span class="line">test_sents = conll2000.chunked_sents(<span class="string">'test.txt'</span>, chunk_types=[<span class="string">'NP'</span>])</span><br><span class="line">train_sents = conll2000.chunked_sents(<span class="string">'train.txt'</span>, chunk_types=[<span class="string">'NP'</span>])</span><br><span class="line">unigram_chunker = UnigramChunker(train_sents)</span><br><span class="line">print(unigram_chunker.evaluate(test_sents))</span><br><span class="line"><span class="string">'''</span><br><span class="line">ChunkParse score:</span><br><span class="line">    IOB Accuracy:  92.9%</span><br><span class="line">    Precision:     79.9%</span><br><span class="line">    Recall:        86.8%</span><br><span class="line">    F-Measure:     83.2%</span><br><span class="line">'''</span></span><br><span class="line"><span class="comment"># check, assign the unigram tagger to each of the part-of-speech tags that appear in the corpus</span></span><br><span class="line">postags = sorted(set(pos <span class="keyword">for</span> sent <span class="keyword">in</span> train_sents</span><br><span class="line">                     <span class="keyword">for</span> (word,pos) <span class="keyword">in</span> sent.leaves()))</span><br><span class="line">print(unigram_chunker.tagger.tag(postags))</span><br><span class="line"><span class="string">'''</span><br><span class="line">[('#', 'B-NP'), ('$', 'B-NP'), ("''", 'O'), ('(', 'O'), (')', 'O'),</span><br><span class="line"> (',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'),</span><br><span class="line"> ('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'),</span><br><span class="line"> ('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'),</span><br><span class="line"> ('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'),</span><br><span class="line"> ('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'),</span><br><span class="line"> ('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'),</span><br><span class="line"> ('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'),</span><br><span class="line"> ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'),</span><br><span class="line"> ('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]</span><br><span class="line">'''</span></span><br></pre></td></tr></table></figure>
<p><strong>Training Classifier-Based Chunkers</strong><br>
Regular-expression based chunkers和N-gram chunkers都是完全根据part-of-speech tags来判断如何分chunk的。有时这个信息并不够，还应该用word本身有的信息。<br>
例如下面的句子，chunk分法应完全不同。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a.	Joey/NN sold/VBD the/DT farmer/NN rice/NN ./.</span><br><span class="line">b.	Nick/NN broke/VBD my/DT computer/NN monitor/NN ./.</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Noun Phrase Chunking with a Consecutive Classifier</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConsecutiveNPChunkTagger</span><span class="params">(nltk.TaggerI)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_sents)</span>:</span></span><br><span class="line">        train_set = []</span><br><span class="line">        <span class="keyword">for</span> tagged_sent <span class="keyword">in</span> train_sents:</span><br><span class="line">            untagged_sent = nltk.tag.untag(tagged_sent)</span><br><span class="line">            history = []</span><br><span class="line">            <span class="keyword">for</span> i, (word, tag) <span class="keyword">in</span> enumerate(tagged_sent):</span><br><span class="line">                featureset = npchunk_features(untagged_sent, i, history)</span><br><span class="line">                train_set.append( (featureset, tag) )</span><br><span class="line">                history.append(tag)</span><br><span class="line">        self.classifier = nltk.MaxentClassifier.train(</span><br><span class="line">            train_set, algorithm=<span class="string">'megam'</span>, trace=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tag</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        history = []</span><br><span class="line">        <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(sentence):</span><br><span class="line">            featureset = npchunk_features(sentence, i, history)</span><br><span class="line">            tag = self.classifier.classify(featureset)</span><br><span class="line">            history.append(tag)</span><br><span class="line">        <span class="keyword">return</span> zip(sentence, history)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConsecutiveNPChunker</span><span class="params">(nltk.ChunkParserI)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_sents)</span>:</span></span><br><span class="line">        tagged_sents = [[((w,t),c) <span class="keyword">for</span> (w,t,c) <span class="keyword">in</span></span><br><span class="line">                         nltk.chunk.tree2conlltags(sent)]</span><br><span class="line">                        <span class="keyword">for</span> sent <span class="keyword">in</span> train_sents]</span><br><span class="line">        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        tagged_sents = self.tagger.tag(sentence)</span><br><span class="line">        conlltags = [(w,t,c) <span class="keyword">for</span> ((w,t),c) <span class="keyword">in</span> tagged_sents]</span><br><span class="line">        <span class="keyword">return</span> nltk.chunk.conlltags2tree(conlltags)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">npchunk_features</span><span class="params">(sentence, i, history)</span>:</span></span><br><span class="line">    word, pos = sentence[i]</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">        prevword, prevpos = <span class="string">"&lt;START&gt;"</span>, <span class="string">"&lt;START&gt;"</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        prevword, prevpos = sentence[i<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">if</span> i == len(sentence)<span class="number">-1</span>:</span><br><span class="line">        nextword, nextpos = <span class="string">"&lt;END&gt;"</span>, <span class="string">"&lt;END&gt;"</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        nextword, nextpos = sentence[i+<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">"pos"</span>: pos,</span><br><span class="line">            <span class="string">"word"</span>: word,</span><br><span class="line">            <span class="string">"prevpos"</span>: prevpos,</span><br><span class="line">            <span class="string">"nextpos"</span>: nextpos,</span><br><span class="line">            <span class="string">"prevpos+pos"</span>: <span class="string">"%s+%s"</span> % (prevpos, pos),</span><br><span class="line">            <span class="string">"pos+nextpos"</span>: <span class="string">"%s+%s"</span> % (pos, nextpos),</span><br><span class="line">            <span class="string">"tags-since-dt"</span>: tags_since_dt(sentence, i)&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tags_since_dt</span><span class="params">(sentence, i)</span>:</span></span><br><span class="line">    tags = set()</span><br><span class="line">    <span class="keyword">for</span> word, pos <span class="keyword">in</span> sentence[:i]:</span><br><span class="line">        <span class="keyword">if</span> pos == <span class="string">'DT'</span>:</span><br><span class="line">            tags = set()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tags.add(pos)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'+'</span>.join(sorted(tags))</span><br><span class="line"></span><br><span class="line">chunker = ConsecutiveNPChunker(train_sents)</span><br><span class="line">print(chunker.evaluate(test_sents))</span><br><span class="line"><span class="string">'''</span><br><span class="line">ChunkParse score:</span><br><span class="line">    IOB Accuracy:  96.0%</span><br><span class="line">    Precision:     88.6%</span><br><span class="line">    Recall:        91.0%</span><br><span class="line">    F-Measure:     89.8%</span><br><span class="line">'''</span></span><br></pre></td></tr></table></figure>
<h3 id="recursion-in-linguistic-structure"><a class="markdownIt-Anchor" href="#recursion-in-linguistic-structure"></a> Recursion in Linguistic Structure</h3>
<p><strong>Building Nested Structure with Cascaded Chunkers</strong><br>
build chunk structures of arbitrary depth, simply by creating a multi-stage chunk grammar containing recursive rules.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a chunker that handles NP, PP, VP and S</span></span><br><span class="line">grammar = <span class="string">r"""</span><br><span class="line">  NP: &#123;&lt;DT|JJ|NN.*&gt;+&#125;          # Chunk sequences of DT, JJ, NN</span><br><span class="line">  PP: &#123;&lt;IN&gt;&lt;NP&gt;&#125;               # Chunk prepositions followed by NP</span><br><span class="line">  VP: &#123;&lt;VB.*&gt;&lt;NP|PP|CLAUSE&gt;+$&#125; # Chunk verbs and their arguments</span><br><span class="line">  CLAUSE: &#123;&lt;NP&gt;&lt;VP&gt;&#125;           # Chunk NP, VP</span><br><span class="line">  """</span></span><br><span class="line">cp = nltk.RegexpParser(grammar, loop=<span class="number">2</span>)	<span class="comment"># after the chunker trying all of the rules, it repeats the process.</span></span><br><span class="line">sentence = [(<span class="string">"John"</span>, <span class="string">"NNP"</span>), (<span class="string">"thinks"</span>, <span class="string">"VBZ"</span>), (<span class="string">"Mary"</span>, <span class="string">"NN"</span>),</span><br><span class="line">    (<span class="string">"saw"</span>, <span class="string">"VBD"</span>), (<span class="string">"the"</span>, <span class="string">"DT"</span>), (<span class="string">"cat"</span>, <span class="string">"NN"</span>), (<span class="string">"sit"</span>, <span class="string">"VB"</span>),</span><br><span class="line">    (<span class="string">"on"</span>, <span class="string">"IN"</span>), (<span class="string">"the"</span>, <span class="string">"DT"</span>), (<span class="string">"mat"</span>, <span class="string">"NN"</span>)]</span><br><span class="line"></span><br><span class="line">print(cp.parse(sentence))</span><br><span class="line"><span class="string">'''</span><br><span class="line">(S</span><br><span class="line">  (NP John/NNP)</span><br><span class="line">  thinks/VBZ</span><br><span class="line">  (CLAUSE</span><br><span class="line">    (NP Mary/NN)</span><br><span class="line">    (VP</span><br><span class="line">      saw/VBD</span><br><span class="line">      (CLAUSE</span><br><span class="line">        (NP the/DT cat/NN)</span><br><span class="line">        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))</span><br><span class="line">'''</span></span><br></pre></td></tr></table></figure>
<p>并不是最好的方法，期待第8章学的full parsing。</p>
<p><strong>Trees</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tree1 = nltk.Tree(<span class="string">'NP'</span>, [<span class="string">'Alice'</span>])</span><br><span class="line">tree2 = nltk.Tree(<span class="string">'NP'</span>, [<span class="string">'the'</span>, <span class="string">'rabbit'</span>])</span><br><span class="line">tree3 = nltk.Tree(<span class="string">'VP'</span>, [<span class="string">'chased'</span>, tree2])</span><br><span class="line">tree4 = nltk.Tree(<span class="string">'S'</span>, [tree1, tree3])</span><br><span class="line">print(tree4)	<span class="comment"># (S (NP Alice) (VP chased (NP the rabbit)))</span></span><br><span class="line">tree4[<span class="number">1</span>].label()	<span class="comment"># 'VP'</span></span><br><span class="line">tree4.leaves()	<span class="comment"># ['Alice', 'chased', 'the', 'rabbit']</span></span><br><span class="line">tree4[<span class="number">1</span>][<span class="number">1</span>][<span class="number">1</span>]	<span class="comment"># 'rabbit'</span></span><br><span class="line">tree3.draw()</span><br></pre></td></tr></table></figure>
<p><strong>Tree Traversal</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a recursive function to traverse a tree</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">traverse</span><span class="params">(t)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        t.label()	<span class="comment"># duck typing, to detect that t is a tree</span></span><br><span class="line">    <span class="keyword">except</span> AttributeError:</span><br><span class="line">        print(t, end=<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Now we know that t.node is defined</span></span><br><span class="line">        print(<span class="string">'('</span>, t.label(), end=<span class="string">" "</span>)</span><br><span class="line">        <span class="keyword">for</span> child <span class="keyword">in</span> t:</span><br><span class="line">            traverse(child)</span><br><span class="line">        print(<span class="string">')'</span>, end=<span class="string">" "</span>)</span><br><span class="line"></span><br><span class="line"> &gt;&gt;&gt; t = nltk.Tree(<span class="string">'(S (NP Alice) (VP chased (NP the rabbit)))'</span>)</span><br><span class="line"> &gt;&gt;&gt; traverse(t)</span><br><span class="line"> ( S ( NP Alice ) ( VP chased ( NP the rabbit ) ) )</span><br></pre></td></tr></table></figure>
<h3 id="named-entity-recognition"><a class="markdownIt-Anchor" href="#named-entity-recognition"></a> Named Entity Recognition</h3>
<p>“Facility”: human-made artifacts in the domains of architecture and civil engineering; and “GPE”: geo-political entities such as city, state/province, and country.</p>
<p>Commonly Used Types of Named Entity</p>
<table>
<thead>
<tr>
<th>NE Type</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>ORGANIZATION</td>
<td>Georgia-Pacific Corp., WHO</td>
</tr>
<tr>
<td>PERSON</td>
<td>Eddy Bonte, President Obama</td>
</tr>
<tr>
<td>LOCATION</td>
<td>Murray River, Mount Everest</td>
</tr>
<tr>
<td>DATE</td>
<td>June, 2008-06-29</td>
</tr>
<tr>
<td>TIME</td>
<td>two fifty a m, 1:30 p.m.</td>
</tr>
<tr>
<td>MONEY</td>
<td>175 million Canadian Dollars, GBP 10.40</td>
</tr>
<tr>
<td>PERCENT</td>
<td>twenty pct, 18.75 %</td>
</tr>
<tr>
<td>FACILITY</td>
<td>Washington Monument, Stonehenge</td>
</tr>
<tr>
<td>GPE</td>
<td>South East Asia, Midlothian</td>
</tr>
</tbody>
</table>
<p>NER的目标就是identifying the boundaries of the NE, and identifying its type.</p>
<p>NLTK已经有一个训练好的classifer来识别named entities，<code>nltk.ne_chunk()</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(nltk.ne_chunk(sent, binary=<span class="keyword">False</span>)) <span class="comment"># binary为True则只识别出NE，不区分出子类。 </span></span><br><span class="line">(S</span><br><span class="line">  The/DT</span><br><span class="line">  (GPE U.S./NNP)</span><br><span class="line">  <span class="keyword">is</span>/VBZ</span><br><span class="line">  one/CD</span><br><span class="line">  ...</span><br><span class="line">  according/VBG</span><br><span class="line">  to/TO</span><br><span class="line">  (PERSON Brooke/NNP T./NNP Mossman/NNP)</span><br><span class="line">  ...)</span><br></pre></td></tr></table></figure>
<h3 id="relation-extraction"><a class="markdownIt-Anchor" href="#relation-extraction"></a> Relation Extraction</h3>
<p>当named entites确定了以后就要找关系了。<br>
一个简单的方法，找(X, a, Y)的关系，其中a是一串表达此关系的词汇。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>IN = re.compile(<span class="string">r'.*\bin\b(?!\b.+ing)'</span>)  <span class="comment"># 其中的(?!\b.+ing\b)是删掉类似success in supervising the transition of这样的结构。</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> doc <span class="keyword">in</span> nltk.corpus.ieer.parsed_docs(<span class="string">'NYT_19980315'</span>):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">for</span> rel <span class="keyword">in</span> nltk.sem.extract_rels(<span class="string">'ORG'</span>, <span class="string">'LOC'</span>, doc,</span><br><span class="line"><span class="meta">... </span>                                     corpus=<span class="string">'ieer'</span>, pattern = IN):</span><br><span class="line"><span class="meta">... </span>        print(nltk.sem.rtuple(rel))</span><br><span class="line">[ORG: <span class="string">'WHYY'</span>] <span class="string">'in'</span> [LOC: <span class="string">'Philadelphia'</span>]</span><br><span class="line">[ORG: <span class="string">'McGlashan &amp;AMP; Sarrail'</span>] <span class="string">'firm in'</span> [LOC: <span class="string">'San Mateo'</span>]</span><br><span class="line">[ORG: <span class="string">'Freedom Forum'</span>] <span class="string">'in'</span> [LOC: <span class="string">'Arlington'</span>]</span><br><span class="line">[ORG: <span class="string">'Brookings Institution'</span>] <span class="string">', the research group in'</span> [LOC: <span class="string">'Washington'</span>]</span><br><span class="line">[ORG: <span class="string">'Idealab'</span>] <span class="string">', a self-described business incubator based in'</span> [LOC: <span class="string">'Los Angeles'</span>]</span><br><span class="line">[ORG: <span class="string">'Open Text'</span>] <span class="string">', based in'</span> [LOC: <span class="string">'Waterloo'</span>]</span><br><span class="line">[ORG: <span class="string">'WGBH'</span>] <span class="string">'in'</span> [LOC: <span class="string">'Boston'</span>]</span><br><span class="line">[ORG: <span class="string">'Bastille Opera'</span>] <span class="string">'in'</span> [LOC: <span class="string">'Paris'</span>]</span><br><span class="line">[ORG: <span class="string">'Omnicom'</span>] <span class="string">'in'</span> [LOC: <span class="string">'New York'</span>]</span><br><span class="line">[ORG: <span class="string">'DDB Needham'</span>] <span class="string">'in'</span> [LOC: <span class="string">'New York'</span>]</span><br><span class="line">[ORG: <span class="string">'Kaplan Thaler Group'</span>] <span class="string">'in'</span> [LOC: <span class="string">'New York'</span>]</span><br><span class="line">[ORG: <span class="string">'BBDO South'</span>] <span class="string">'in'</span> [LOC: <span class="string">'Atlanta'</span>]</span><br><span class="line">[ORG: <span class="string">'Georgia-Pacific'</span>] <span class="string">'in'</span> [LOC: <span class="string">'Atlanta'</span>]</span><br></pre></td></tr></table></figure>
<p>用NLTK库里自带的方法，使用part-of-speech tags。例子是Dutch的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> conll2002</span><br><span class="line">vnv = <span class="string">"""</span><br><span class="line">(</span><br><span class="line">is/V|    # 3rd sing present and</span><br><span class="line">was/V|   # past forms of the verb zijn ('be')</span><br><span class="line">werd/V|  # and also present</span><br><span class="line">wordt/V  # past of worden ('become)</span><br><span class="line">)</span><br><span class="line">.*       # followed by anything</span><br><span class="line">van/Prep # followed by van ('of')</span><br><span class="line">"""</span></span><br><span class="line">VAN = re.compile(vnv, re.VERBOSE)</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> conll2002.chunked_sents(<span class="string">'ned.train'</span>):</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> nltk.sem.extract_rels(<span class="string">'PER'</span>, <span class="string">'ORG'</span>, doc,</span><br><span class="line">                                   corpus=<span class="string">'conll2002'</span>, pattern=VAN):</span><br><span class="line">        print(nltk.sem.clause(r, relsym=<span class="string">"VAN"</span>))</span><br><span class="line"><span class="string">'''</span><br><span class="line">VAN("cornet_d'elzius", 'buitenlandse_handel')</span><br><span class="line">VAN('johan_rottiers', 'kardinaal_van_roey_instituut')</span><br><span class="line">VAN('annie_lennox', 'eurythmics')</span><br><span class="line">'''</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="analyzing-sentence-structure"><a class="markdownIt-Anchor" href="#analyzing-sentence-structure"></a> Analyzing Sentence Structure</h2>
<p>本章要解决的问题：</p>
<ol>
<li>How can we use a formal grammar to describe the structure of an unlimited set of sentences?</li>
<li>How do we represent the structure of sentences using syntax trees?</li>
<li>How do parsers analyze a sentence and automatically build a syntax tree?</li>
</ol>
<h3 id="some-grammatical-dilemmas"><a class="markdownIt-Anchor" href="#some-grammatical-dilemmas"></a> Some Grammatical Dilemmas</h3>
<p><strong>Linguistic Data and Unlimited Possibilities</strong><br>
使用一些template可以将句子组合成无限长，由许多grammatical sentences组成。<br>
Grammars use recursive productions of the form <em>S → S and S</em>.</p>
<p><strong>Ubiquitous Ambiguity</strong><br>
例句：<em>While hunting in Africa, I shot an elephant in my pajamas. How he got into my pajamas, I don’t know.</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">groucho_grammar = nltk.CFG.fromstring(<span class="string">"""</span><br><span class="line">	S -&gt; NP VP</span><br><span class="line">	PP -&gt; P NP</span><br><span class="line">	NP -&gt; Det N | Det N PP | 'I'</span><br><span class="line">	VP -&gt; V NP | VP PP</span><br><span class="line">	Det -&gt; 'an' | 'my'</span><br><span class="line">	N -&gt; 'elephant' | 'pajamas'</span><br><span class="line">	V -&gt; 'shot'</span><br><span class="line">	P -&gt; 'in'</span><br><span class="line">	"""</span>)</span><br><span class="line"></span><br><span class="line">sent = [<span class="string">'I'</span>, <span class="string">'shot'</span>, <span class="string">'an'</span>, <span class="string">'elephant'</span>, <span class="string">'in'</span>, <span class="string">'my'</span>, <span class="string">'pajamas'</span>]</span><br><span class="line">parser = nltk.ChartParser(groucho_grammar)</span><br><span class="line"><span class="keyword">for</span> tree <span class="keyword">in</span> parser.parse(sent):</span><br><span class="line">    print(tree)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span><br><span class="line">(S</span><br><span class="line">  (NP I)</span><br><span class="line">  (VP</span><br><span class="line">    (VP (V shot) (NP (Det an) (N elephant)))</span><br><span class="line">    (PP (P in) (NP (Det my) (N pajamas)))))</span><br><span class="line">(S</span><br><span class="line">  (NP I)</span><br><span class="line">  (VP</span><br><span class="line">    (V shot)</span><br><span class="line">    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))</span><br><span class="line">"""</span></span><br></pre></td></tr></table></figure>
<p>这个句子可以被解读为这样两棵树。<br>
<img src="http://www.nltk.org/book/tree_images/ch08-tree-1.png" alt="Image Loading"><br>
<img src="http://www.nltk.org/book/tree_images/ch08-tree-2.png" alt="Image Loading"></p>
<h3 id="whats-the-use-of-syntax"><a class="markdownIt-Anchor" href="#whats-the-use-of-syntax"></a> What’s the Use of Syntax?</h3>
<p><strong>Beyond n-grams</strong></p>

  </div>
</article>



    </div>
    
      <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/archives/">博文</a></li>
         
          <li><a href="/categories/">分类</a></li>
         
          <li><a href="/tags/">标签</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#preface"><span class="toc-number">1.</span> <span class="toc-text"> Preface</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#language-processing-and-python"><span class="toc-number">2.</span> <span class="toc-text"> Language Processing and Python</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#在ubuntu里设置时可能的坑"><span class="toc-number">2.1.</span> <span class="toc-text"> 在Ubuntu里设置时可能的坑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#computing-with-language-texts-and-words"><span class="toc-number">2.2.</span> <span class="toc-text"> Computing with Language: Texts and Words</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-closer-look-at-python-texts-as-lists-of-words"><span class="toc-number">2.3.</span> <span class="toc-text"> A Closer Look at Python: Texts as Lists of Words</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#computing-with-language-simple-statistics"><span class="toc-number">2.4.</span> <span class="toc-text"> Computing with Language: Simple Statistics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#back-to-python-making-decisions-and-taking-control"><span class="toc-number">2.5.</span> <span class="toc-text"> Back to Python: Making Decisions and Taking Control</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#automatic-natural-language-understanding"><span class="toc-number">2.6.</span> <span class="toc-text"> Automatic Natural Language Understanding</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#accessing-text-corpora-and-lexical-resources"><span class="toc-number">3.</span> <span class="toc-text"> Accessing Text Corpora and Lexical Resources</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#accessing-text-corpora"><span class="toc-number">3.1.</span> <span class="toc-text"> Accessing Text Corpora</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#corporas"><span class="toc-number">3.1.1.</span> <span class="toc-text"> Corporas</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#corpus-functionality"><span class="toc-number">3.1.2.</span> <span class="toc-text"> Corpus Functionality</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#loading-your-own-corpus"><span class="toc-number">3.1.3.</span> <span class="toc-text"> Loading your own Corpus</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#conditional-frequency-distributions"><span class="toc-number">3.2.</span> <span class="toc-text"> Conditional Frequency Distributions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#more-python-reusing-code"><span class="toc-number">3.3.</span> <span class="toc-text"> More Python: Reusing Code</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lexical-resources"><span class="toc-number">3.4.</span> <span class="toc-text"> Lexical Resources</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#wordnet"><span class="toc-number">3.5.</span> <span class="toc-text"> WordNet</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#processing-raw-text"><span class="toc-number">4.</span> <span class="toc-text"> Processing Raw Text</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#accessing-text-from-the-web-and-from-disk"><span class="toc-number">4.1.</span> <span class="toc-text"> Accessing Text from the Web and from Disk</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#strings-text-processing-at-the-lowest-level"><span class="toc-number">4.2.</span> <span class="toc-text"> Strings: Text Processing at the Lowest Level</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#text-processing-with-unicode"><span class="toc-number">4.3.</span> <span class="toc-text"> Text Processing with Unicode</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#regular-expressions-for-detecting-word-patterns"><span class="toc-number">4.4.</span> <span class="toc-text"> Regular Expressions for Detecting Word Patterns</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#useful-applications-of-regular-expressions"><span class="toc-number">4.5.</span> <span class="toc-text"> Useful Applications of Regular Expressions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#normalizeing-text"><span class="toc-number">4.6.</span> <span class="toc-text"> Normalizeing Text</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#regular-expressions-for-tokenizing-text"><span class="toc-number">4.7.</span> <span class="toc-text"> Regular Expressions for Tokenizing Text</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#segmentation"><span class="toc-number">4.8.</span> <span class="toc-text"> Segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#formatting-from-lists-to-strings"><span class="toc-number">4.9.</span> <span class="toc-text"> Formatting: From Lists to Strings</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#writing-structured-programs"><span class="toc-number">5.</span> <span class="toc-text"> Writing Structured Programs</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#back-to-the-basics"><span class="toc-number">5.1.</span> <span class="toc-text"> Back to the Basics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sequences"><span class="toc-number">5.2.</span> <span class="toc-text"> Sequences</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#questions-of-style"><span class="toc-number">5.3.</span> <span class="toc-text"> Questions of Style</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#functions-the-foundation-of-structured-programming"><span class="toc-number">5.4.</span> <span class="toc-text"> Functions: The Foundation of Structured Programming</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#doing-more-with-functions"><span class="toc-number">5.5.</span> <span class="toc-text"> Doing More with Functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#program-development"><span class="toc-number">5.6.</span> <span class="toc-text"> Program Development</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#algorithm-design"><span class="toc-number">5.7.</span> <span class="toc-text"> Algorithm Design</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-sample-of-python-libraries"><span class="toc-number">5.8.</span> <span class="toc-text"> A Sample of Python Libraries</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#categorizing-and-tagging-words"><span class="toc-number">6.</span> <span class="toc-text"> Categorizing and Tagging Words</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#using-a-tagger"><span class="toc-number">6.1.</span> <span class="toc-text"> Using a Tagger</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tagged-corpora"><span class="toc-number">6.2.</span> <span class="toc-text"> Tagged Corpora</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapping-words-to-properties-using-python-dictionaries"><span class="toc-number">6.3.</span> <span class="toc-text"> Mapping Words to Properties Using Python Dictionaries</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#automatic-tagging"><span class="toc-number">6.4.</span> <span class="toc-text"> Automatic Tagging</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#n-gram-tagging"><span class="toc-number">6.5.</span> <span class="toc-text"> N-Gram Tagging</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transformation-based-tagging"><span class="toc-number">6.6.</span> <span class="toc-text"> Transformation-Based Tagging</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-to-determine-the-category-of-a-word"><span class="toc-number">6.7.</span> <span class="toc-text"> How to Determine the Category of a Word</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#learning-to-classify-text"><span class="toc-number">7.</span> <span class="toc-text"> Learning to Classify Text</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#supervised-classification"><span class="toc-number">7.1.</span> <span class="toc-text"> Supervised Classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#further-examples-of-supervised-classification"><span class="toc-number">7.2.</span> <span class="toc-text"> Further Examples of Supervised Classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#evaluation"><span class="toc-number">7.3.</span> <span class="toc-text"> Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#decision-trees"><span class="toc-number">7.4.</span> <span class="toc-text"> Decision Trees</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#naive-bayes-classifiers"><span class="toc-number">7.5.</span> <span class="toc-text"> Naive Bayes Classifiers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#maximum-entropy-classifiers"><span class="toc-number">7.6.</span> <span class="toc-text"> Maximum Entropy Classifiers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#generative-vs-conditional-classifiers"><span class="toc-number">7.7.</span> <span class="toc-text"> Generative vs Conditional Classifiers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#modeling-linguistic-patterns"><span class="toc-number">7.8.</span> <span class="toc-text"> Modeling Linguistic Patterns</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#extracting-information-from-text"><span class="toc-number">8.</span> <span class="toc-text"> Extracting Information from Text</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#information-extraction"><span class="toc-number">8.1.</span> <span class="toc-text"> Information Extraction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#chunking"><span class="toc-number">8.2.</span> <span class="toc-text"> Chunking</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#developing-and-evaluating-chunkers"><span class="toc-number">8.3.</span> <span class="toc-text"> Developing and Evaluating Chunkers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#recursion-in-linguistic-structure"><span class="toc-number">8.4.</span> <span class="toc-text"> Recursion in Linguistic Structure</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#named-entity-recognition"><span class="toc-number">8.5.</span> <span class="toc-text"> Named Entity Recognition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#relation-extraction"><span class="toc-number">8.6.</span> <span class="toc-text"> Relation Extraction</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#analyzing-sentence-structure"><span class="toc-number">9.</span> <span class="toc-text"> Analyzing Sentence Structure</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#some-grammatical-dilemmas"><span class="toc-number">9.1.</span> <span class="toc-text"> Some Grammatical Dilemmas</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#whats-the-use-of-syntax"><span class="toc-number">9.2.</span> <span class="toc-text"> What’s the Use of Syntax?</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&text=Natural Language Processing with Python 笔记"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&title=Natural Language Processing with Python 笔记"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&is_video=false&description=Natural Language Processing with Python 笔记"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Natural Language Processing with Python 笔记&body=Check out this article: http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&title=Natural Language Processing with Python 笔记"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&title=Natural Language Processing with Python 笔记"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&title=Natural Language Processing with Python 笔记"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&title=Natural Language Processing with Python 笔记"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://conglang.github.io/2016/02/05/note-natural-language-processing-with-python/&name=Natural Language Processing with Python 笔记&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

    
    <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2018 聪
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/archives/">博文</a></li>
         
          <li><a href="/categories/">分类</a></li>
         
          <li><a href="/tags/">标签</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>

<!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

<!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-74786593-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?4e074986ce7bd4c6c94338ce1a49c4be";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->



