<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Coursera上Natural Language Processing课程的笔记。主要讲解了Parsing, Tagging, Machine Translation等问题; Hidden Markov Models, Probabilistic Context-free Grammars, Log-linear Models, and Statistical Models for Machin">
<meta property="og:type" content="article">
<meta property="og:title" content="Natural Language Processing 课程笔记">
<meta property="og:url" content="http://conglang.github.io/2015/09/05/note-natural-language-processing/index.html">
<meta property="og:site_name" content="A Stellar Hiker">
<meta property="og:description" content="Coursera上Natural Language Processing课程的笔记。主要讲解了Parsing, Tagging, Machine Translation等问题; Hidden Markov Models, Probabilistic Context-free Grammars, Log-linear Models, and Statistical Models for Machin">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_what_is_natural_language_processing.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_information_extraction1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_information_extraction2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_text_summarization.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_dialogue_systems.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_problem_tagging.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_problem_parsing.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_ambiguity_at_syntactic_level.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_more_syntactic_ambiguity.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_language_modeling_problem1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_language_modeling_problem2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_naive_method.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_first_order_markov_processes.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_second_order_markov_processes.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_modeling_variable_length_sequences.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_trigram_language_models.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_trigram_language_models_an_example.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_trigram_estimation_problem.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_sparse_data_problems.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_perplexity.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_some_intuition_about_perplexity.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_typical_values_of_perplexity.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_bias_variance_trade_off.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_linear_interpolation1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_linear_interpolation2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_how_to_estimate_the_lambda_values.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_allowing_the_lambdas_to_vary.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_discounting_methods.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_discounting_methods1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_discounting_methods2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_katz_back_off_models_bigrams.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_katz_back_off_models_trigrams.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_week1_summary.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_part_of_speech_tagging.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_named_entity_recognition.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_named_entity_extraction_as_tagging.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_tagging_problem_out_goal.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_two_types_of_constraints.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_supervised_learning_problems.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_generative_models.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_decoding_with_generative_models.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_hidden_markov_models.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_trigram_hidden_markov_models.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_trigram_hmms_example.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_trigram_hmms_why_the_name.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_smoothed_estimation.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_dealing_with_low_frequency_words.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_dealing_with_low_frequency_words_example1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_dealing_with_low_frequency_words_example2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_viterbi_algorithm_problem.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_viterbi_algorithm.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_viterbi_algorithm_example.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_viterbi_algorithm_recursive_definition.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_justification_for_the_recursive_definition.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_viterbi_algorithm_detail.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_viterbi_algorithm_with_backpointers.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_viterbi_algorithm_running_time.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_hidden_markov_models_pros_and_cons.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_parsing_syntactic_structure.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_data_for_parsing_experiments.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_information_conveyed_by_parse_trees1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_information_conveyed_by_parse_trees2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_information_conveyed_by_parse_trees3.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_an_example_application_machine_translation.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_an_example_application_machine_translation_tree.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_context_free_grammars_definition.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_context_free_grammar_for_english.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_left_most_derivations.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_left_most_derivations_example.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_properties_of_cfgs.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_cfg_example_of_ambiguity1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_cfg_example_of_ambiguity2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_problem_with_parsing_ambiguity.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_brief_overview_of_english_syntax.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_fragment_of_a_noun_phrase_grammar.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_prepositions_and_prepositional_phrases.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_an_extended_grammar.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_verbs_verb_phrases_and_sentences.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_pps_modifying_verb_phrases.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_complementizers_and_sbars.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_more_verbs.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_coordination.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_we_have_only_scratched_the_surface.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_sources_of_ambuity_part_of_speech_ambiguity.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_sources_of_ambuity_prepositional_phrase_attachment1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_sources_of_ambuity_prepositional_phrase_attachment2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_sources_of_ambuity_noun_premodifiers.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_probabilistic_context_free_grammar.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_probabilistic_context_free_grammar_example.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_properties_of_pcfgs.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_data_for_parsing_experiments_treebanks.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_deriving_a_pcfg_from_a_treebank.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_parsing_with_a_pcfg.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_chomsky_normal_form.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_dynamic_programming_algorithm1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_dynamic_programming_algorithm2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_full_dynamic_programming_algorithm.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_dynamic_programming_algorithm_for_the_sum.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_pp_attachment_ambiguity1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_pp_attachment_ambiguity2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_coordination_ambiguity1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_coordination_ambiguity2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_structural_preferences_close_attachment.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_heads_in_context_free_rules.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_heads_in_context_free_rules_more.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_rules_which_recover_heads_an_example_for_nps.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_rules_which_recover_heads_an_example_for_vps.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_adding_headwords_to_trees.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_chomsky_normal_form.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_lexicalized_context_free_grammars_in_chomsky_normal_form.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_lexicalized_context_free_grammars_in_chomsky_normal_form_example.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_parameters_in_a_lexicalized_pcfg.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_parsing_with_lexicalized_cfgs.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_parameter_estimation_in_lexicalized_probabilistic_context_free_grammars_example.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_model_from_charniak1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_model_from_charniak2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_lexicalized_probabilistic_context_free_grammars_other_important_details.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_evaluation_representing_trees_as_constituents.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_precision_and_recall.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_accuracy_of_lexicalized_pcfgs_results.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_dependency_accuracies_m2_example.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_dependency_accuracies.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_strengths_and_weaknesses_of_modern_parsers.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_introduction_to_machine_translation.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_challenges_lexical_ambiguity.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_challenges_differing_word_orders.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_syntactic_structure_is_not_preserved_across_translations.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_syntactic_ambiguity_causes_problems.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_pronoun_resolution.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_an_example_of_a_set_of_direct_translation_rules.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_some_problems_with_direct_machine_translation.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_transfer_based_approaches1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_transfer_based_approaches2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_classical_machine_translation_pyramid.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_noisy_channel_model.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_noisy_channel_model2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_example_from_koehn_and_knight_tutorial1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_example_from_koehn_and_knight_tutorial2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_alighments_in_the_ibm_models.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_by_product_most_likely_alignments.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_an_example_alignment.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_ibm_model_1_alignments.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_ibm_model_1_translation_probabilities.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_ibm_model_1_translation_probabilities_example.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_ibm_model_1_the_generative_process.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_an_example_lexical_entry.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_ibm_model_2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_ibm_model_2_example.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_ibm_model_2_example2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_ibm_model_2_the_generative_process.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_recovering_alignments.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_parameter_estimation_problem.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_parameter_estimation_if_the_alignments_are_observed1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_parameter_estimation_if_the_alignments_are_observed2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_parameter_estimation_with_the_em_algorithm1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_parameter_estimation_with_the_em_algorithm2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_parameter_estimation_with_the_em_algorithm3.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_justification_for_the_algorithm.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_ibm_translation_models_summary.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_phrased_based_models.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_pb_lexicon_pairs_example.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_pb_lexicon_pairs_example_ibm_model_2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_representation_as_alignment_matrix.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_finding_alignment_matrices2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_finding_alignment_matrices3.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_heuristics_for_growing_alignments.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_extracting_phrase_pairs_from_the_alignment_matrix.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_probabilities_for_phrase_pairs.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_an_example_phrase_translation_table.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_phrased_based_system_a_sketch_step1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_phrased_based_system_a_sketch_step2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_phrased_based_system_a_sketch_step3.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_phrased_based_system_a_sketch_step4.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_phrased_based_system_a_sketch_step5.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_phrased_based_translation.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_phrased_based_models_definitions.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_phrased_based_translation_definitions.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_definitions.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_valid_derivations.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_scoring_derivations.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_scoring_derivations_example.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_decoding_algorithm_definitions.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_states_and_the_search_space.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_transitions.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_next_function.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_equality_function.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_decoding_algorithm.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_definition_of_add.png">
<meta property="og:image" content="http://conglang.github.io/img/note_definition_of_beam.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_language_modeling_problem.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_trigram_models.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_trigram_models2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_naive_approach.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_second_example_part_of_speech_tagging.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_second_example_part_of_speech_tagging2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_second_example_part_of_speech_tagging3.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_feature_vector_representations.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_language_modeling1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_language_modeling2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_defining_features_in_practice.png">
<meta property="og:image" content="http://conglang.github.io/img/note_the_pos_tagging_example.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_full_set_of_features_in_ratnaparkhi_1996_1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_full_set_of_features_in_ratnaparkhi_1996_2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_final_result.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_parameter_vectors.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_language_modeling.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_log_linear_models.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_why_the_name.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_maximum_likelihood_estimation.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_calculating_the_maximum_likelihood_estimates.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_gradient_ascent_methods.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_conjugate_gradient_methods.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_smoothing_in_log_linear_models.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_regularization.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_experiments_with_regularization.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_experiments_with_gaussian_priors.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_part_of_speech_tagging.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_named_entity_recognition.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_named_entity_extraction_as_tagging.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_log_linear_tagging_our_goal.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_log_linear_models_for_tagging.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_how_to_model_ptw.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_representation_histories.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_feature_vector_representations_in_log_linear_models.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_an_example_log_linear_taggers.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_an_example_log_linear_taggers_feature.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_full_set_of_features_in_ratnaparkhi_1996_1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_full_set_of_features_in_ratnaparkhi_1996_2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_log_linear_models_recap.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_training_the_log_linear_model.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_viterbi_algorithm1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_viterbi_algorithm2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_recursive_definition.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_viterbi_algorithm_with_backpointers_llt.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_faq_segmentation_an_hmm_tagger1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_faq_segmentation_an_hmm_tagger2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_log_linear_models_for_tagging_summary.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_recap_log_linear_taggers.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_general_approach_conditional_history_based_models.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_an_example_tree.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_layer1_part_of_speech_tags.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_layer2_chunks1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_layer2_chunks2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_layer3_remaining_structure.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_layer3_1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_layer3_2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_layer3_3.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_layer3_4.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_layer3_5.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_layer3_6.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_layer3_7.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_layer3_8.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_layer3_9.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_final_sequence_of_decisions.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_applying_a_log_linear_model1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_applying_a_log_linear_model2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_search_problem.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_brow_clustering_algorithm_formulation.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_brown_example.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_brown_clustering_model.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_measuring_the_quality_of_c.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_first_algorithm.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_second_algorithm.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_miller_et_al1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_miller_et_al2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_supervised_learning_in_natural_language.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_models_so_far.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_example1_pcfgs.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_example2_log_linear_taggers.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_need_for_flexible_features_example1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_need_for_flexible_features_example2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_three_components_of_global_linear_models.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_component1_f.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_component1_features.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_component1_feature_vectors.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_component2_gen.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_component3_v.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_putting_it_all_together.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_putting_it_all_together1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_reranking_approaches_to_parsing.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_the_representation_f.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_features_in_paper_rules.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_features_in_paper_bigrams.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_features_in_paper_grandparent_rules.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_features_in_paper_two_level_rules.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_variant_of_the_perceptron_algorithm.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_tagging_using_global_linear_models.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_representation_histories1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_local_feature_vector_representations.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_history_tag_pairs.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_global_and_local_features.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_putting_it_all_together_glm_tagging.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_variant_of_the_perceptron_algorithm.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_training_a_tagger_using_the_perceptron_algorithm.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_example_glm_tagging.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_unlabeled_dependency_parses.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_all_dependency_parses_for_john_saw_mary.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_a_more_complex_example.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_non_projective_structures.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_dependency_parsing_resources.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_glms_for_dependency_parsing1.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_glms_for_dependency_parsing2.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_definition_of_local_feature_vectors.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_results_from_mcdonald.png">
<meta property="og:image" content="http://conglang.github.io/img/note_nlp_glms_for_dependency_parsing_summary.png">
<meta property="og:updated_time" content="2018-07-31T15:04:59.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Natural Language Processing 课程笔记">
<meta name="twitter:description" content="Coursera上Natural Language Processing课程的笔记。主要讲解了Parsing, Tagging, Machine Translation等问题; Hidden Markov Models, Probabilistic Context-free Grammars, Log-linear Models, and Statistical Models for Machin">
<meta name="twitter:image" content="http://conglang.github.io/img/note_nlp_what_is_natural_language_processing.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/astro.png">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/astro.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/astro.png">
          
        
    
    <!-- title -->
    <title>Natural Language Processing 课程笔记</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
  	<link href="https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css" rel="stylesheet">
</head>

<body class="max-width mx-auto px3 ltr">    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/archives/">博文</a></li>
         
          <li><a href="/categories/">分类</a></li>
         
          <li><a href="/tags/">标签</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2015/09/20/note-crucial-conversations/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2015/08/19/note-work-smarter-not-harder-coursera/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://conglang.github.io/2015/09/05/note-natural-language-processing/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&text=Natural Language Processing 课程笔记"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&title=Natural Language Processing 课程笔记"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&is_video=false&description=Natural Language Processing 课程笔记"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Natural Language Processing 课程笔记&body=Check out this article: http://conglang.github.io/2015/09/05/note-natural-language-processing/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&title=Natural Language Processing 课程笔记"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&title=Natural Language Processing 课程笔记"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&title=Natural Language Processing 课程笔记"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&title=Natural Language Processing 课程笔记"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&name=Natural Language Processing 课程笔记&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction-to-natural-language-processing"><span class="toc-number">1.</span> <span class="toc-text"> Introduction to Natural Language Processing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#what-is-natural-language-processingnlp"><span class="toc-number">1.1.</span> <span class="toc-text"> What is Natural Language Processing(NLP)?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#why-is-nlp-hard"><span class="toc-number">1.2.</span> <span class="toc-text"> Why is NLP Hard?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-will-this-course-be-about"><span class="toc-number">1.3.</span> <span class="toc-text"> What will this course be about?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-language-modeling-problem"><span class="toc-number">2.</span> <span class="toc-text"> The Language Modeling Problem</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#introduction-to-the-language-modeling-problem"><span class="toc-number">2.1.</span> <span class="toc-text"> Introduction to the language modeling problem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#markov-processes"><span class="toc-number">2.2.</span> <span class="toc-text"> Markov Processes</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#trigram-language-models"><span class="toc-number">2.3.</span> <span class="toc-text"> Trigram Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#evaluating-language-models-perplexity"><span class="toc-number">2.4.</span> <span class="toc-text"> Evaluating language models: perplexity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#parameter-estimation-in-language-models"><span class="toc-number">2.5.</span> <span class="toc-text"> Parameter Estimation in Language Models</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#linear-interpolation"><span class="toc-number">2.5.1.</span> <span class="toc-text"> Linear Interpolation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#discounting-methods"><span class="toc-number">2.5.2.</span> <span class="toc-text"> Discounting Methods</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary"><span class="toc-number">2.6.</span> <span class="toc-text"> Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tagging-problems-and-hidden-markov-models"><span class="toc-number">3.</span> <span class="toc-text"> Tagging Problems, and Hidden Markov Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#the-tagging-problem"><span class="toc-number">3.1.</span> <span class="toc-text"> The Tagging Problem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#generative-models-and-the-noisy-channel-model-for-supervised-learning"><span class="toc-number">3.2.</span> <span class="toc-text"> Generative models, and the noisy-channel model, for supervised learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hidden-markov-model-hmm-taggers"><span class="toc-number">3.3.</span> <span class="toc-text"> Hidden Markov Model (HMM) taggers</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#basic-definitions"><span class="toc-number">3.3.1.</span> <span class="toc-text"> Basic definitions</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#parameter-estimation"><span class="toc-number">3.3.2.</span> <span class="toc-text"> Parameter estimation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-viterbi-algorithm"><span class="toc-number">3.3.3.</span> <span class="toc-text"> The Viterbi algorithm</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary-2"><span class="toc-number">3.4.</span> <span class="toc-text"> Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#parsing-and-context-free-grammars"><span class="toc-number">4.</span> <span class="toc-text"> Parsing, and Context-Free Grammars</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#an-introduction-to-the-parsing-problem"><span class="toc-number">4.1.</span> <span class="toc-text"> An Introduction to the Parsing Problem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#context-free-grammars"><span class="toc-number">4.2.</span> <span class="toc-text"> Context Free Grammars</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-brief-sketch-of-the-syntax-of-english"><span class="toc-number">4.3.</span> <span class="toc-text"> A Brief(!) Sketch of the Syntax of English</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#examples-of-ambiguous-structures"><span class="toc-number">4.4.</span> <span class="toc-text"> Examples of Ambiguous Structures</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#probabilistic-context-free-grammars-pcfgs"><span class="toc-number">5.</span> <span class="toc-text"> Probabilistic Context-Free Grammars (PCFGs)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#probabilistic-context-free-grammars-pcfgs-2"><span class="toc-number">5.1.</span> <span class="toc-text"> Probabilistic Context-Free Grammars (PCFGs)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-cky-algorithm-for-parsing-with-pcfgs"><span class="toc-number">5.2.</span> <span class="toc-text"> The CKY Algorithm for parsing with PCFGs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary-3"><span class="toc-number">5.3.</span> <span class="toc-text"> Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#weeknesses-of-pcfgs"><span class="toc-number">6.</span> <span class="toc-text"> Weeknesses of PCFGs</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#lack-of-sensitivity-to-lexical-information"><span class="toc-number">6.1.</span> <span class="toc-text"> Lack of Sensitivity to Lexical Information</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lack-of-sensitivity-to-structural-frequencies"><span class="toc-number">6.2.</span> <span class="toc-text"> Lack of Sensitivity to Structural Frequencies</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lexicalized-pcfgs"><span class="toc-number">7.</span> <span class="toc-text"> Lexicalized PCFGs</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#lexicalization-of-a-treebank"><span class="toc-number">7.1.</span> <span class="toc-text"> Lexicalization of a treebank</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lexicalized-probabilistic-context-free-grammars"><span class="toc-number">7.2.</span> <span class="toc-text"> Lexicalized probabilistic context-free grammars</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#parameter-estimation-in-lexicalized-probabilistic-context-free-grammars"><span class="toc-number">7.3.</span> <span class="toc-text"> Parameter estimation in lexicalized probabilistic context-free grammars</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#accuracy-of-lexicalized-probabilistic-context-free-grammars"><span class="toc-number">7.4.</span> <span class="toc-text"> Accuracy of lexicalized probabilistic context-free grammars</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary-4"><span class="toc-number">7.5.</span> <span class="toc-text"> Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction-to-machine-translationmt"><span class="toc-number">8.</span> <span class="toc-text"> Introduction to Machine Translation(MT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#challenges-in-machine-translation"><span class="toc-number">8.1.</span> <span class="toc-text"> Challenges in machine translation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#classical-machine-translation"><span class="toc-number">8.2.</span> <span class="toc-text"> Classical machine translation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-brief-introduction-to-statistical-mt"><span class="toc-number">8.3.</span> <span class="toc-text"> A brief introduction to statistical MT</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-ibm-translation-models"><span class="toc-number">9.</span> <span class="toc-text"> The IBM Translation Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ibm-model-1"><span class="toc-number">9.1.</span> <span class="toc-text"> IBM Model 1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ibm-model-2"><span class="toc-number">9.2.</span> <span class="toc-text"> IBM Model 2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#em-training-of-models-1-and-2"><span class="toc-number">9.3.</span> <span class="toc-text"> EM Training of Models 1 and 2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary-5"><span class="toc-number">9.4.</span> <span class="toc-text"> Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#phrase-based-translation-models"><span class="toc-number">10.</span> <span class="toc-text"> Phrase-based Translation Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#learning-phrases-from-alignments"><span class="toc-number">10.1.</span> <span class="toc-text"> Learning Phrases From Alignments</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-phrase-based-model"><span class="toc-number">10.2.</span> <span class="toc-text"> A Phrase-Based Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#decoding-of-phrase-based-translation-models"><span class="toc-number">10.3.</span> <span class="toc-text"> Decoding of Phrase-based Translation Models</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#definition-of-the-decoding-problem"><span class="toc-number">10.3.1.</span> <span class="toc-text"> Definition of the Decoding Problem</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-decoding-algorithm"><span class="toc-number">10.3.2.</span> <span class="toc-text"> The Decoding Algorithm</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#log-linear-models"><span class="toc-number">11.</span> <span class="toc-text"> Log-linear Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#two-example-problems"><span class="toc-number">11.1.</span> <span class="toc-text"> Two Example Problems</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#features-in-log-linear-models"><span class="toc-number">11.2.</span> <span class="toc-text"> Features in Log-Linear Models</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#for-problem-1"><span class="toc-number">11.2.1.</span> <span class="toc-text"> For Problem 1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#for-problem-2"><span class="toc-number">11.2.2.</span> <span class="toc-text"> For Problem 2</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#definition-of-log-linear-models"><span class="toc-number">11.3.</span> <span class="toc-text"> Definition of Log-linear Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#parameter-estimation-in-log-linear-models"><span class="toc-number">11.4.</span> <span class="toc-text"> Parameter Estimation in Log-linear Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#smoothingregularization-in-log-linear-models"><span class="toc-number">11.5.</span> <span class="toc-text"> Smoothing/Regularization in Log-linear Models</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#log-linear-models-for-taggingmemms"><span class="toc-number">12.</span> <span class="toc-text"> Log-linear Models for Tagging(MEMMs)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#recap-the-tagging-problem"><span class="toc-number">12.1.</span> <span class="toc-text"> Recap: The Tagging Problem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#log-linear-taggers"><span class="toc-number">12.2.</span> <span class="toc-text"> Log-Linear Taggers</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#independence-assumptions-in-log-linear-taggers"><span class="toc-number">12.2.1.</span> <span class="toc-text"> Independence Assumptions in Log-linear Taggers</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#features-in-log-linear-taggers"><span class="toc-number">12.2.2.</span> <span class="toc-text"> Features in Log-Linear Taggers</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#parameters-in-log-linear-models"><span class="toc-number">12.2.3.</span> <span class="toc-text"> Parameters in Log-linear Models</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-viterbi-algorithm-for-log-linear-taggers"><span class="toc-number">12.2.4.</span> <span class="toc-text"> The Viterbi Algorithm for Log-linear Taggers</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#an-example-application"><span class="toc-number">12.2.5.</span> <span class="toc-text"> An Example Application</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary-6"><span class="toc-number">12.3.</span> <span class="toc-text"> Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#log-linear-models-for-history-based-parsing"><span class="toc-number">13.</span> <span class="toc-text"> Log-Linear Models for History-based Parsing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#conditional-history-based-models"><span class="toc-number">13.1.</span> <span class="toc-text"> Conditional History-based Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#representing-trees-as-decision-sequences关于step-1"><span class="toc-number">13.2.</span> <span class="toc-text"> Representing Trees as Decision Sequences(关于Step 1)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#features关于step-3"><span class="toc-number">13.3.</span> <span class="toc-text"> Features(关于Step 3)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#beam-search关于step-4"><span class="toc-number">13.4.</span> <span class="toc-text"> Beam Search(关于Step 4)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#unsupervised-learning-brown-clustering"><span class="toc-number">14.</span> <span class="toc-text"> Unsupervised Learning: Brown Clustering</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#word-cluster-representations"><span class="toc-number">14.1.</span> <span class="toc-text"> Word Cluster Representations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-brown-clustering-algorithm"><span class="toc-number">14.2.</span> <span class="toc-text"> The Brown Clustering Algorithm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#clusters-in-ne-recognition"><span class="toc-number">14.3.</span> <span class="toc-text"> Clusters in NE Recognition</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#global-linear-modelsglms"><span class="toc-number">15.</span> <span class="toc-text"> Global Linear Models(GLMs)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#a-brief-review-of-history-based-methods"><span class="toc-number">15.1.</span> <span class="toc-text"> A Brief Review of History-based Methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-new-set-of-techniques-global-linear-models"><span class="toc-number">15.2.</span> <span class="toc-text"> A New Set of Techniques: Global Linear Models</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#a-new-framework-global-linear-models"><span class="toc-number">15.2.1.</span> <span class="toc-text"> A New Framework: Global Linear Models</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#intuition"><span class="toc-number">15.2.1.1.</span> <span class="toc-text"> Intuition</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#three-components-of-global-linear-models"><span class="toc-number">15.2.1.2.</span> <span class="toc-text"> Three Components of Global Linear Models</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#parsing-problems-in-this-framework-reranking-problems"><span class="toc-number">15.2.2.</span> <span class="toc-text"> Parsing Problems in This Framework: Reranking Problems</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#parameter-estimation-method-1-a-variant-of-the-perceptron-algorithm"><span class="toc-number">15.2.3.</span> <span class="toc-text"> Parameter Estimation Method 1: A Variant of the Perceptron Algorithm</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary-7"><span class="toc-number">15.3.</span> <span class="toc-text"> Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#glms-for-tagging"><span class="toc-number">16.</span> <span class="toc-text"> GLMs for Tagging</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#glms-for-dependency-parsing"><span class="toc-number">17.</span> <span class="toc-text"> GLMs for Dependency Parsing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#dependency-parsing"><span class="toc-number">17.1.</span> <span class="toc-text"> Dependency Parsing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#glms-for-dependency-parsing-2"><span class="toc-number">17.2.</span> <span class="toc-text"> GLMs for Dependency Parsing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#results-from-mcdonald2005"><span class="toc-number">17.3.</span> <span class="toc-text"> Results from McDonald(2005)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary-8"><span class="toc-number">17.4.</span> <span class="toc-text"> Summary</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index my4">
        
        
        
          <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Natural Language Processing 课程笔记
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">A Stellar Hiker</span>
      </span>
      
    <div class="postdate">
        <time datetime="2015-09-05T13:47:21.000Z" itemprop="datePublished">2015-09-05</time>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/Courses/">Courses</a>, <a class="tag-link" href="/tags/Natural-Language-Processing/">Natural Language Processing</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>Coursera上<a href="https://www.coursera.org/course/nlangp" target="_blank" rel="external">Natural Language Processing课程</a>的笔记。<br>
Michael Collins, Columbia University<br>
PS. 老师的PPT做得好有品位！简单、有效、节制美。声音也好听！</p>
<p>中间停了3个月，重新开始学习。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>†</mo></mrow><annotation encoding="application/x-tex">\dagger</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord">†</span></span></span></span></p>
<p>授课大纲为：<br>
Topics covered include:</p>
<ol>
<li>Language modeling.</li>
<li>Hidden Markov models, and tagging problems.</li>
<li>Probabilistic context-free grammars, and the parsing problem.</li>
<li>Statistical approaches to machine translation.</li>
<li>Log-linear models, and their application to NLP problems.</li>
<li>Unsupervised and semi-supervised learning in NLP.</li>
</ol>
<hr>
<h2 id="introduction-to-natural-language-processing"><a class="markdownIt-Anchor" href="#introduction-to-natural-language-processing"></a> Introduction to Natural Language Processing</h2>
<h3 id="what-is-natural-language-processingnlp"><a class="markdownIt-Anchor" href="#what-is-natural-language-processingnlp"></a> What is Natural Language Processing(NLP)?</h3>
<p><img src="/img/note_nlp_what_is_natural_language_processing.png" alt="Image Loading"></p>
<p><strong>Key Applications</strong></p>
<ul>
<li>Machine Translation: e.g., Google Translation from Arabic</li>
<li>Information Extraction:<br>
<img src="/img/note_nlp_information_extraction1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_information_extraction2.png" alt="Image Loading"></li>
<li>Text Summarization:<br>
<img src="/img/note_nlp_text_summarization.png" alt="Image Loading"></li>
<li>Dialogue Systems<br>
<img src="/img/note_nlp_dialogue_systems.png" alt="Image Loading"></li>
</ul>
<p><strong>Basic NLP Problems</strong></p>
<ul>
<li>Tagging<br>
<img src="/img/note_nlp_problem_tagging.png" alt="Image Loading"><br>
n是none，v是verb，等等。<br>
na不是entity，sc是start company，cc是continued company，sl是start location，cl是continued location。</li>
<li>Parsing<br>
<img src="/img/note_nlp_problem_parsing.png" alt="Image Loading"></li>
</ul>
<h3 id="why-is-nlp-hard"><a class="markdownIt-Anchor" href="#why-is-nlp-hard"></a> Why is NLP Hard?</h3>
<p>[example from L.Lee]<br>
“At last, a computer that understands you like your mother”</p>
<p><strong>Ambiguity</strong></p>
<ol>
<li>(*) It understands you as well as your mother understands you</li>
<li>It understands (that) you like your mother</li>
<li>It understands you as well as it understands your mother</li>
</ol>
<p>1 and 3: Does this mean well, or poorly?</p>
<p><strong>Ambiguity at Many Levels</strong><br>
At the <strong>acoustic</strong> level (speech recognition):</p>
<ol>
<li>“…a computer that understands you like your mother”</li>
<li>“…a computer that understands you lie cured mother”</li>
</ol>
<p>At the <strong>syntactic</strong> level:<br>
<img src="/img/note_nlp_ambiguity_at_syntactic_level.png" alt="Image Loading"><br>
Different structures lead to different interpretations.<br>
<img src="/img/note_nlp_more_syntactic_ambiguity.png" alt="More Syntactic Ambiguity"></p>
<p>At the <strong>semantic</strong> (meaning) level:<br>
Two definitions of “mother”</p>
<ul>
<li>a woman who has given birth to a child</li>
<li>a stringy slimy substance consisting of yeast cells and bacteria; is added to cider or wine to produce vinegar</li>
</ul>
<p>This is an instance of <strong>word sense ambiguity</strong></p>
<ul>
<li>They put money in the <em>bank</em> = buried in mud?</li>
<li>I saw her <em>duck</em> with a telescope</li>
</ul>
<p>At the <strong>discourse</strong> (multi-clause) level:</p>
<ul>
<li>Alice says they’ve built a computer that understands you like your mother</li>
<li>But <em>she</em> … 有两种选择<br>
… doesn’t know any details<br>
… doesn’t understand me at all</li>
</ul>
<p>This is an instance of <strong>anaphora</strong>, where she co-referees to some other discourse entity.</p>
<h3 id="what-will-this-course-be-about"><a class="markdownIt-Anchor" href="#what-will-this-course-be-about"></a> What will this course be about?</h3>
<p><strong>Course Coverage</strong></p>
<ul>
<li>NLP sub-problems: part-of-speech tagging, parsing, word-sense disambiguation, etc.</li>
<li>Machine learning techniques: probabilistic context-free grammars, hidden markov models, estimation/smoothing techniques, the EM algorithm, log-linear models, etc.</li>
<li>Applications: information extraction, machine translation, natural language interfaces…</li>
</ul>
<p><strong>A Syllabus</strong></p>
<ul>
<li>Language modeling, smoothed estimation</li>
<li>Tagging, hidden Markov models</li>
<li>Statistical parsing</li>
<li>Machine translation</li>
<li>Log-linear models, discriminative methods</li>
<li>Semi-supervised and unsupervised learning for NLP</li>
</ul>
<p><strong>Prerequisites</strong></p>
<ul>
<li>Basic linear algebra, probability, algorithms</li>
<li>Programming skills 本课需要Python或Java</li>
</ul>
<p><strong>Books</strong><br>
Comprehensive notes for the course will be provided at<br>
<a href="http://www.cs.columbia.edu/~mcollins" target="_blank" rel="external">http://www.cs.columbia.edu/~mcollins</a></p>
<p>Additional useful background:<br>
Jurafsky and Martin:<br>
Speech and Language Processing (2nd Edition)</p>
<hr>
<h2 id="the-language-modeling-problem"><a class="markdownIt-Anchor" href="#the-language-modeling-problem"></a> The Language Modeling Problem</h2>
<h3 id="introduction-to-the-language-modeling-problem"><a class="markdownIt-Anchor" href="#introduction-to-the-language-modeling-problem"></a> Introduction to the language modeling problem</h3>
<p><img src="/img/note_nlp_the_language_modeling_problem1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_the_language_modeling_problem2.png" alt="Image Loading"></p>
<p><strong>Why on earth would we want to do this?</strong></p>
<ul>
<li><strong>Speech recognition</strong> was the original motivation. (Related problems are optical character recognition, handwriting recognition.)<br>
比如识别有两种可能，recognize speech和wreck a nice beach。如果有language model，我们就知道哪个的可能性大一些。</li>
<li>The estimation techniques developed for this problem will be <strong>VERY</strong> useful for other problems in NLP.</li>
</ul>
<p><strong>A Naive Method</strong><br>
<img src="/img/note_nlp_a_naive_method.png" alt="Image Loading"><br>
最简单的一个方法，统计每句出现次数，求概率。</p>
<h3 id="markov-processes"><a class="markdownIt-Anchor" href="#markov-processes"></a> Markov Processes</h3>
<p><strong>Markov Processes</strong><br>
Trigram models的实现基于Markov Processes。</p>
<ul>
<li>Consider a sequence of random variables <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>X</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>X</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">X_1, X_2, ..., X_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>. Each random variable can take any value in a finite set <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.22222em;">V</span></span></span></span>. For now we assume the length <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span> is fixed (e.g., <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>=</mo><mn>1</mn><mn>0</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">n = 100</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span><span class="mrel">=</span><span class="mord mathrm">1</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span></span></span></span>).</li>
<li>Our goal: model<br>
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><msub><mi>X</mi><mn>1</mn></msub><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>X</mi><mn>2</mn></msub><mo>=</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>X</mi><mi>n</mi></msub><mo>=</mo><msub><mi>x</mi><mi>n</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X_1 = x_1, X_2 = x_2, ..., X_n = x_n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span></li>
</ul>
<p><strong>First-Order Markov Processes</strong><br>
<img src="/img/note_nlp_first_order_markov_processes.png" alt="Image Loading"><br>
第一步是exact的，例如：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>A</mi><mo separator="true">,</mo><mi>B</mi><mo separator="true">,</mo><mi>C</mi><mo>)</mo><mo>=</mo><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo><mo>×</mo><mi>P</mi><mo>(</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo>)</mo><mo>∗</mo><mi>P</mi><mo>(</mo><mi>C</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo separator="true">,</mo><mi>B</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(A,B,C) = P(A) \times P(B|A) * P(C|A,B)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit">A</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.05017em;">B</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.07153em;">C</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit">A</span><span class="mclose">)</span><span class="mbin">×</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.05017em;">B</span><span class="mord mathrm">∣</span><span class="mord mathit">A</span><span class="mclose">)</span><span class="mbin">∗</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.07153em;">C</span><span class="mord mathrm">∣</span><span class="mord mathit">A</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.05017em;">B</span><span class="mclose">)</span></span></span></span>。<br>
第二步是markov assumption。</p>
<p><strong>Second-Order Markov Processes</strong><br>
<img src="/img/note_nlp_second_order_markov_processes.png" alt="Image Loading"></p>
<p><strong>Modeling Variable Length Sequences</strong><br>
<img src="/img/note_nlp_modeling_variable_length_sequences.png" alt="Image Loading"></p>
<h3 id="trigram-language-models"><a class="markdownIt-Anchor" href="#trigram-language-models"></a> Trigram Language Models</h3>
<p>很好用又易于实现。<br>
<strong>Trigram Language Models</strong><br>
<img src="/img/note_nlp_trigram_language_models.png" alt="Image Loading"></p>
<p><strong>An Example</strong><br>
<img src="/img/note_nlp_trigram_language_models_an_example.png" alt="Image Loading"></p>
<p><strong>The Trigram Estimation Problem</strong><br>
<img src="/img/note_nlp_the_trigram_estimation_problem.png" alt="Image Loading"><br>
它的问题是<strong>Sparse Data Problems</strong>，parameters太多。<br>
<img src="/img/note_nlp_sparse_data_problems.png" alt="Image Loading"></p>
<h3 id="evaluating-language-models-perplexity"><a class="markdownIt-Anchor" href="#evaluating-language-models-perplexity"></a> Evaluating language models: perplexity</h3>
<p><strong>Perplexity</strong><br>
<img src="/img/note_nlp_perplexity.png" alt="Image Loading"></p>
<p><strong>Some Intuition about Perplexity</strong><br>
<img src="/img/note_nlp_some_intuition_about_perplexity.png" alt="Image Loading"><br>
Perplexity有点像所有这些词中有效的词有多少。从而可以看出这个model好不好，值越小越好。</p>
<p><strong>Typical Values of Perplexity</strong><br>
<img src="/img/note_nlp_typical_values_of_perplexity.png" alt="Image Loading"></p>
<p>Some History: “Syntactic Structures”</p>
<h3 id="parameter-estimation-in-language-models"><a class="markdownIt-Anchor" href="#parameter-estimation-in-language-models"></a> Parameter Estimation in Language Models</h3>
<h4 id="linear-interpolation"><a class="markdownIt-Anchor" href="#linear-interpolation"></a> Linear Interpolation</h4>
<p><strong>The Bias-Variance Trade-Off</strong><br>
<img src="/img/note_nlp_the_bias_variance_trade_off.png" alt="Image Loading"><br>
<strong>Linear Interpolation</strong><br>
<img src="/img/note_nlp_linear_interpolation1.png" alt="Image Loading"><br>
证明：<br>
<img src="/img/note_nlp_linear_interpolation2.png" alt="Image Loading"></p>
<p><strong>How to estimate the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">λ</span></span></span></span> values?</strong><br>
<img src="/img/note_nlp_how_to_estimate_the_lambda_values.png" alt="Image Loading"></p>
<p><strong>Allowing the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">λ</span></span></span></span>'s to vary</strong><br>
<img src="/img/note_nlp_allowing_the_lambdas_to_vary.png" alt="Image Loading"><br>
不同情况下用不同<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">λ</span></span></span></span>。</p>
<h4 id="discounting-methods"><a class="markdownIt-Anchor" href="#discounting-methods"></a> Discounting Methods</h4>
<p><strong>Discounting Methods</strong><br>
<img src="/img/note_nlp_discounting_methods.png" alt="Image Loading"><br>
<img src="/img/note_nlp_discounting_methods1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_discounting_methods2.png" alt="Image Loading"></p>
<p><strong>Katz Back-Off Models (Bigrams)</strong><br>
<img src="/img/note_nlp_katz_back_off_models_bigrams.png" alt="Image Loading"></p>
<p><strong>Katz Back-Off Models (Trigrams)</strong><br>
<img src="/img/note_nlp_katz_back_off_models_trigrams.png" alt="Image Loading"></p>
<h3 id="summary"><a class="markdownIt-Anchor" href="#summary"></a> Summary</h3>
<p><img src="/img/note_nlp_week1_summary.png" alt="Image Loading"></p>
<hr>
<h2 id="tagging-problems-and-hidden-markov-models"><a class="markdownIt-Anchor" href="#tagging-problems-and-hidden-markov-models"></a> Tagging Problems, and Hidden Markov Models</h2>
<h3 id="the-tagging-problem"><a class="markdownIt-Anchor" href="#the-tagging-problem"></a> The Tagging Problem</h3>
<p><strong>Part-of-Speech Tagging</strong><br>
<img src="/img/note_nlp_part_of_speech_tagging.png" alt="Image Loading"><br>
主要问题还是ambiguity。</p>
<p><strong>Named Entity Recognition</strong><br>
<img src="/img/note_nlp_named_entity_recognition.png" alt="Image Loading"></p>
<p><strong>Named Entity Extraction as Tagging</strong><br>
<img src="/img/note_nlp_named_entity_extraction_as_tagging.png" alt="Image Loading"></p>
<p><strong>Our Goal</strong><br>
<img src="/img/note_nlp_tagging_problem_out_goal.png" alt="Image Loading"></p>
<p><strong>Two Types of Constraints</strong><br>
<img src="/img/note_nlp_two_types_of_constraints.png" alt="Image Loading"></p>
<h3 id="generative-models-and-the-noisy-channel-model-for-supervised-learning"><a class="markdownIt-Anchor" href="#generative-models-and-the-noisy-channel-model-for-supervised-learning"></a> Generative models, and the noisy-channel model, for supervised learning</h3>
<p><strong>Supervised Learning Problems - Conditional models</strong><br>
<img src="/img/note_nlp_supervised_learning_problems.png" alt="Image Loading"><br>
本例中x,y大概为：<br>
x(i) = the dog laughs, y(i) = DT NN VB<br>
也叫Discriminative Model。</p>
<p><strong>Generative Models</strong><br>
<img src="/img/note_nlp_generative_models.png" alt="Image Loading"><br>
Note那一行就是Bayes rule。<br>
Generative Model比也叫Discriminative Model更灵活通用。</p>
<p><strong>Decoding with Generative Models</strong><br>
<img src="/img/note_nlp_decoding_with_generative_models.png" alt="Image Loading"><br>
由于p(x)并不受y的变化影响，被约去。</p>
<h3 id="hidden-markov-model-hmm-taggers"><a class="markdownIt-Anchor" href="#hidden-markov-model-hmm-taggers"></a> Hidden Markov Model (HMM) taggers</h3>
<h4 id="basic-definitions"><a class="markdownIt-Anchor" href="#basic-definitions"></a> Basic definitions</h4>
<p><strong>Hidden Markov Models</strong><br>
<img src="/img/note_nlp_hidden_markov_models.png" alt="Image Loading"></p>
<p><strong>Trigram Hidden Markov Models(Trigram HMMs)</strong><br>
<img src="/img/note_nlp_trigram_hidden_markov_models.png" alt="Image Loading"><br>
q代表Trigram Parameters，e代表Emission Parameters。</p>
<p><strong>An Example</strong><br>
<img src="/img/note_nlp_trigram_hmms_example.png" alt="Image Loading"></p>
<p><strong>Why the Name?</strong><br>
<img src="/img/note_nlp_trigram_hmms_why_the_name.png" alt="Image Loading"><br>
即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo>)</mo><mo>=</mo><mi>p</mi><mo>(</mo><mi>y</mi><mo>)</mo><mo>×</mo><mi>p</mi><mo>(</mo><mi>x</mi><mi mathvariant="normal">∣</mi><mi>y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">p(x,y)=p(y) \times p(x|y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mbin">×</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></p>
<h4 id="parameter-estimation"><a class="markdownIt-Anchor" href="#parameter-estimation"></a> Parameter estimation</h4>
<p><strong>Smoothed Estimation</strong><br>
<img src="/img/note_nlp_smoothed_estimation.png" alt="Image Loading"><br>
回顾前面的linear interprelation method。<br>
q包含trigram ml estimate/bigram ml estimate/unigram ml estimate。<br>
这个方式的问题是：e(x|y)=0 for all y if x is never seen in the training data.有一个词没有出现过的话，整个p都是0了。</p>
<p><strong>Dealing with Low-Frequency Words</strong><br>
Example:</p>
<blockquote>
<p>Profits soared at Boeing Co. , easily topping forecasts on Wall Street, as their CEO Alan Mulally announced first quarter results.</p>
</blockquote>
<p>其中Mulally这个词在training set中可能没有出现过。那么<br>
e(Mulally|y)=0 for all tags y 所以 p(x1, …, xn, y1, …, yn+1) = 0 for all tag sequences y1, …, yn+1。</p>
<p>解决方法：<br>
<img src="/img/note_nlp_dealing_with_low_frequency_words.png" alt="Image Loading"></p>
<p>Example:<br>
<img src="/img/note_nlp_dealing_with_low_frequency_words_example1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_dealing_with_low_frequency_words_example2.png" alt="Image Loading"><br>
这样就有类似e(firstword|NA)或e(initcap|SC)这样的e了。缺点是需要人手动整理分类。</p>
<h4 id="the-viterbi-algorithm"><a class="markdownIt-Anchor" href="#the-viterbi-algorithm"></a> The Viterbi algorithm</h4>
<p><strong>The Problem</strong><br>
<img src="/img/note_nlp_the_viterbi_algorithm_problem.png" alt="Image Loading"><br>
Brute Force Search is Hopelessly Inefficient</p>
<p><strong>The Viterbi Algorithm Defination</strong><br>
<img src="/img/note_nlp_the_viterbi_algorithm.png" alt="Image Loading"><br>
Example:<br>
<img src="/img/note_nlp_viterbi_algorithm_example.png" alt="Image Loading"><br>
k是1到n。u是Sk-1。v是Sk。</p>
<p><strong>A Recursive Definition</strong><br>
<img src="/img/note_nlp_viterbi_algorithm_recursive_definition.png" alt="Image Loading"><br>
动态规划。<br>
Justification for the Recursive Definition:<br>
<img src="/img/note_nlp_justification_for_the_recursive_definition.png" alt="Image Loading"></p>
<p><strong>The Viterbi Algorithm</strong><br>
<img src="/img/note_nlp_the_viterbi_algorithm_detail.png" alt="Image Loading"></p>
<p><strong>The Viterbi Algorithm with Backpointers</strong><br>
<img src="/img/note_nlp_the_viterbi_algorithm_with_backpointers.png" alt="Image Loading"><br>
只是动态规划的具体实现，记录tag。</p>
<p><strong>The Viterbi Algorithm: Running Time</strong><br>
<img src="/img/note_nlp_the_viterbi_algorithm_running_time.png" alt="Image Loading"></p>
<h3 id="summary-2"><a class="markdownIt-Anchor" href="#summary-2"></a> Summary</h3>
<p><strong>Pros and Cons</strong><br>
<img src="/img/note_nlp_hidden_markov_models_pros_and_cons.png" alt="Image Loading"></p>
<hr>
<h2 id="parsing-and-context-free-grammars"><a class="markdownIt-Anchor" href="#parsing-and-context-free-grammars"></a> Parsing, and Context-Free Grammars</h2>
<h3 id="an-introduction-to-the-parsing-problem"><a class="markdownIt-Anchor" href="#an-introduction-to-the-parsing-problem"></a> An Introduction to the Parsing Problem</h3>
<p><strong>Parsing(Syntactic Structure)</strong><br>
<img src="/img/note_nlp_parsing_syntactic_structure.png" alt="Image Loading"></p>
<p><strong>Syntactic Formalisms</strong></p>
<ul>
<li>Work in formal syntax goes back to Chomsky’s PhD thesis in the 1950s.<br>
Book “Syntactic Structures”</li>
<li>Examples of current formalisms: minimalism, lexical functional grammer(LFG), head-driven phrase-structure grammar(HPSG), tree adjoining grammars(TAG), categorial grammars.<br>
今天学的context-free grammar是这些modern grammar的基础。</li>
</ul>
<p><strong>Data for Parsing Experiments</strong><br>
<img src="/img/note_nlp_data_for_parsing_experiments.png" alt="Image Loading"><br>
是supervised learning，Training data是人手动标的，Penn WSJ Treebank。</p>
<p><strong>The Information Conveyed by Parse Trees</strong><br>
1.Part of speech for each word<br>
<img src="/img/note_nlp_the_information_conveyed_by_parse_trees1.png" alt="Image Loading"><br>
2.Phrases<br>
<img src="/img/note_nlp_the_information_conveyed_by_parse_trees2.png" alt="Image Loading"><br>
3.Useful Relationships<br>
<img src="/img/note_nlp_the_information_conveyed_by_parse_trees3.png" alt="Image Loading"></p>
<p><strong>An Example Application: Machine Translation</strong><br>
<img src="/img/note_nlp_an_example_application_machine_translation.png" alt="Image Loading"><br>
<img src="/img/note_nlp_an_example_application_machine_translation_tree.png" alt="Image Loading"><br>
有Syntactic Structure后任务容易了很多。</p>
<h3 id="context-free-grammars"><a class="markdownIt-Anchor" href="#context-free-grammars"></a> Context Free Grammars</h3>
<p><strong>Context-Free Grammars Definition</strong><br>
<img src="/img/note_nlp_context_free_grammars_definition.png" alt="Image Loading"></p>
<p><strong>A Context-Free Grammar for English</strong><br>
<img src="/img/note_nlp_a_context_free_grammar_for_english.png" alt="Image Loading"></p>
<p><strong>Left-Most Derivations</strong><br>
<img src="/img/note_nlp_left_most_derivations.png" alt="Image Loading"><br>
An Example:<br>
<img src="/img/note_nlp_left_most_derivations_example.png" alt="Image Loading"></p>
<p><strong>Properties of CFGs</strong><br>
<img src="/img/note_nlp_properties_of_cfgs.png" alt="Image Loading"><br>
CFG - Context Free Grammars</p>
<p><strong>An Example of Ambiguity</strong><br>
<img src="/img/note_nlp_cfg_example_of_ambiguity1.png" alt="in the car修饰drove。"><br>
<img src="/img/note_nlp_cfg_example_of_ambiguity2.png" alt="in the car修饰the street。意义不同了。"></p>
<p><strong>The Problem with Parsing: Ambiguity</strong><br>
<img src="/img/note_nlp_the_problem_with_parsing_ambiguity.png" alt="Image Loading"><br>
有14种解释。</p>
<h3 id="a-brief-sketch-of-the-syntax-of-english"><a class="markdownIt-Anchor" href="#a-brief-sketch-of-the-syntax-of-english"></a> A Brief(!) Sketch of the Syntax of English</h3>
<p><strong>A Brief Overview of English Syntax</strong><br>
<img src="/img/note_nlp_a_brief_overview_of_english_syntax.png" alt="Image Loading"></p>
<p><strong>A Fragment of a Noun Phrase Grammar</strong><br>
<img src="/img/note_nlp_a_fragment_of_a_noun_phrase_grammar.png" alt="Image Loading"><br>
可以试着组合出很多种tree。</p>
<p><strong>Prepositions, and Prepositional Phrases</strong><br>
<img src="/img/note_nlp_prepositions_and_prepositional_phrases.png" alt="Image Loading"></p>
<p><strong>An Extended Grammar</strong><br>
<img src="/img/note_nlp_an_extended_grammar.png" alt="Image Loading"></p>
<p><strong>Verbs, Verb Phrases, and Sentences</strong><br>
<img src="/img/note_nlp_verbs_verb_phrases_and_sentences.png" alt="Image Loading"></p>
<p><strong>PPs Modifying Verb Phrases</strong><br>
<img src="/img/note_nlp_pps_modifying_verb_phrases.png" alt="Image Loading"></p>
<p><strong>Complementizers, and SBARs</strong><br>
<img src="/img/note_nlp_complementizers_and_sbars.png" alt="Image Loading"></p>
<p><strong>More Verbs</strong><br>
<img src="/img/note_nlp_more_verbs.png" alt="Image Loading"></p>
<p><strong>Coordination</strong><br>
<img src="/img/note_nlp_coordination.png" alt="Image Loading"></p>
<p><strong>We’ve Only Scratched the Surface…</strong><br>
<img src="/img/note_nlp_we_have_only_scratched_the_surface.png" alt="Image Loading"></p>
<h3 id="examples-of-ambiguous-structures"><a class="markdownIt-Anchor" href="#examples-of-ambiguous-structures"></a> Examples of Ambiguous Structures</h3>
<p><strong>Sources of Ambiguity - Part-of-Speech ambiguity</strong><br>
<img src="/img/note_nlp_sources_of_ambuity_part_of_speech_ambiguity.png" alt="Image Loading"></p>
<p><strong>Sources of Ambiguity - Prepositional Phrase Attachment</strong><br>
<img src="/img/note_nlp_sources_of_ambuity_prepositional_phrase_attachment1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_sources_of_ambuity_prepositional_phrase_attachment2.png" alt="Image Loading"><br>
前文有提到。<br>
另一个例子：Two analyses for: John was believed to have been shot by Bill。by Bill修饰shot或believed。<br>
人类会更倾向于让prepositions去修饰the most recent verb in a sentence.</p>
<p><strong>Sources of Ambiguity - Noun Premodifiers</strong><br>
<img src="/img/note_nlp_sources_of_ambuity_noun_premodifiers.png" alt="Image Loading"><br>
左边那个fast相当于修饰car mechanic了。右边是fast car修饰mechanic。</p>
<hr>
<h2 id="probabilistic-context-free-grammars-pcfgs"><a class="markdownIt-Anchor" href="#probabilistic-context-free-grammars-pcfgs"></a> Probabilistic Context-Free Grammars (PCFGs)</h2>
<p>为每个rule添加概率，这样生成不同parse tree的几率也不一样了。<br>
又旧又简单效果也不好，但是别人的基础，稍加改进之后表现就很好了。</p>
<h3 id="probabilistic-context-free-grammars-pcfgs-2"><a class="markdownIt-Anchor" href="#probabilistic-context-free-grammars-pcfgs-2"></a> Probabilistic Context-Free Grammars (PCFGs)</h3>
<p><strong>A Probabilistic Context-Free Grammar (PCFG)</strong><br>
<img src="/img/note_nlp_a_probabilistic_context_free_grammar.png" alt="Image Loading"><br>
An Example:<br>
<img src="/img/note_nlp_a_probabilistic_context_free_grammar_example.png" alt="Image Loading"></p>
<p><strong>Properties of PCFGs</strong><br>
<img src="/img/note_nlp_properties_of_pcfgs.png" alt="Image Loading"></p>
<p><strong>Deriving a PCFG from a Treebank</strong><br>
Treebank是什么：<br>
<img src="/img/note_nlp_data_for_parsing_experiments_treebanks.png" alt="Image Loading"><br>
用Treebank计算PCFG<br>
<img src="/img/note_nlp_deriving_a_pcfg_from_a_treebank.png" alt="Image Loading"></p>
<p><strong>PCFGs</strong><br>
Booth and Thompson (1973) showed that a CFG with rule probabilities correctly defines a distribution over the set of derivations provided that:</p>
<ol>
<li>The rule probabilities define conditional distributions over the different ways of rewriting each non-terminal.</li>
<li>A technical condition on the rule probabilities ensuring that the probability of the derivation terminating in a finite number of steps is 1. (This condition is not really a practical concern.)</li>
</ol>
<h3 id="the-cky-algorithm-for-parsing-with-pcfgs"><a class="markdownIt-Anchor" href="#the-cky-algorithm-for-parsing-with-pcfgs"></a> The CKY Algorithm for parsing with PCFGs</h3>
<p>** Parsing with a PCFG**<br>
<img src="/img/note_nlp_parsing_with_a_pcfg.png" alt="Image Loading"><br>
Brute Force还是完全不可能的。</p>
<p><strong>Chomsky Normal Form</strong><br>
<img src="/img/note_nlp_chomsky_normal_form.png" alt="Image Loading"><br>
R那行的意思就是要么分成两个非终端节点，要么就是终端节点。如果多于两个，就定义变量分成几条Rule，每条都是两个。<br>
有了这个标准形式就可以用动态规划了。</p>
<p><strong>A Dynamic Programming Algorithm</strong><br>
<img src="/img/note_nlp_a_dynamic_programming_algorithm1.png" alt="Image Loading"><br>
词i到词j之间形成X形式的可能组成方式中概率最大的那个。<br>
<img src="/img/note_nlp_a_dynamic_programming_algorithm2.png" alt="Image Loading"><br>
s是split point。</p>
<p><strong>The Full Dynamic Programming Algorithm</strong><br>
<img src="/img/note_nlp_the_full_dynamic_programming_algorithm.png" alt="Image Loading"></p>
<p><strong>A Dynamic Programming Algoritm for the Sum</strong><br>
<img src="/img/note_nlp_a_dynamic_programming_algorithm_for_the_sum.png" alt="Image Loading"></p>
<h3 id="summary-3"><a class="markdownIt-Anchor" href="#summary-3"></a> Summary</h3>
<ul>
<li>PCFGs augments CFGs by including a probability for each rule in the grammar.</li>
<li>The probability for a parse tree is the product of probabilities for the rules in the tree.</li>
<li>To build a PCFG-parsed parser:<br>
1.Learn a PCFG from a treebank.<br>
2.Given a test data sentence, use the CKY algorithm to compute the highest probability tree for the sentence under the PCFG.</li>
</ul>
<hr>
<h2 id="weeknesses-of-pcfgs"><a class="markdownIt-Anchor" href="#weeknesses-of-pcfgs"></a> Weeknesses of PCFGs</h2>
<p>PCFGs = Probabilistic Context-Free Grammars<br>
只有大约72%的准确率。而现代parser大约有92%的准确率。</p>
<ul>
<li>Lack of sensitivity to lexical information</li>
<li>Lack of sensitivity to structural frequencies</li>
</ul>
<h3 id="lack-of-sensitivity-to-lexical-information"><a class="markdownIt-Anchor" href="#lack-of-sensitivity-to-lexical-information"></a> Lack of Sensitivity to Lexical Information</h3>
<p>每个假设都是独立的。且和word完全无关。</p>
<p><strong>PP Attachment Ambiguity</strong><br>
<img src="/img/note_nlp_pp_attachment_ambiguity1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_pp_attachment_ambiguity2.png" alt="Image Loading"><br>
本例中粗体那条一条就决定了整个树的成败。</p>
<p><strong>Coordination Ambiguity</strong><br>
<img src="/img/note_nlp_coordination_ambiguity1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_coordination_ambiguity2.png" alt="Image Loading"><br>
本例中两棵树的概率完全相等，不分伯仲，因为只是顺序不同而已。</p>
<h3 id="lack-of-sensitivity-to-structural-frequencies"><a class="markdownIt-Anchor" href="#lack-of-sensitivity-to-structural-frequencies"></a> Lack of Sensitivity to Structural Frequencies</h3>
<p><strong>Structural Preferences: Close Attachment</strong><br>
<img src="/img/note_nlp_structural_preferences_close_attachment.png" alt="Image Loading"><br>
in Africa可以修饰president或company。就近原则。</p>
<p>Previous example: John was believed to have been shot by Bill<br>
Here the low attachment analysis (Bill does the shooting) contains same rules as the high attachment analysis (Bill does the believing), so the two analyses receive same probability.</p>
<hr>
<h2 id="lexicalized-pcfgs"><a class="markdownIt-Anchor" href="#lexicalized-pcfgs"></a> Lexicalized PCFGs</h2>
<h3 id="lexicalization-of-a-treebank"><a class="markdownIt-Anchor" href="#lexicalization-of-a-treebank"></a> Lexicalization of a treebank</h3>
<p><strong>Heads in Context-Free Rules</strong><br>
<img src="/img/note_nlp_heads_in_context_free_rules.png" alt="Image Loading"><br>
<img src="/img/note_nlp_heads_in_context_free_rules_more.png" alt="Image Loading"></p>
<p><strong>Rules which Recover Heads</strong><br>
原始的Treebank里没有Head信息，要用这些Rule来Recover。<br>
An Example for NPs:<br>
<img src="/img/note_nlp_rules_which_recover_heads_an_example_for_nps.png" alt="Image Loading"><br>
An Example for VPs:<br>
<img src="/img/note_nlp_rules_which_recover_heads_an_example_for_vps.png" alt="Image Loading"></p>
<p><strong>Adding Headwords to Trees</strong><br>
<img src="/img/note_nlp_adding_headwords_to_trees.png" alt="Image Loading"></p>
<h3 id="lexicalized-probabilistic-context-free-grammars"><a class="markdownIt-Anchor" href="#lexicalized-probabilistic-context-free-grammars"></a> Lexicalized probabilistic context-free grammars</h3>
<p><strong>Chomsky Normal Form</strong><br>
前文有讲过。<br>
<img src="/img/note_nlp_chomsky_normal_form.png" alt="Image Loading"></p>
<p><strong>Lexicalized Context-Free Grammars in Chomsky Normal Form</strong><br>
<img src="/img/note_nlp_lexicalized_context_free_grammars_in_chomsky_normal_form.png" alt="Image Loading"><br>
增加了标明head从哪儿来。<br>
Example:<br>
<img src="/img/note_nlp_lexicalized_context_free_grammars_in_chomsky_normal_form_example.png" alt="Image Loading"></p>
<p><strong>Parameters in a Lexicalized PCFG</strong><br>
<img src="/img/note_nlp_parameters_in_a_lexicalized_pcfg.png" alt="Image Loading"><br>
参数变多。</p>
<p><strong>Parsing with Lexicalized CFGs</strong><br>
<img src="/img/note_nlp_parsing_with_lexicalized_cfgs.png" alt="Image Loading"></p>
<h3 id="parameter-estimation-in-lexicalized-probabilistic-context-free-grammars"><a class="markdownIt-Anchor" href="#parameter-estimation-in-lexicalized-probabilistic-context-free-grammars"></a> Parameter estimation in lexicalized probabilistic context-free grammars</h3>
<p>An Example:<br>
<img src="/img/note_nlp_parameter_estimation_in_lexicalized_probabilistic_context_free_grammars_example.png" alt="Image Loading"></p>
<p><strong>A Model from Charniak (1997)</strong><br>
<img src="/img/note_nlp_a_model_from_charniak1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_a_model_from_charniak2.png" alt="Image Loading"><br>
ML = maximum likelihood<br>
qML = count(A) / count(B)<br>
一组<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">λ</span></span></span></span>之和为1。<br>
逐步一般化以平滑。</p>
<p><strong>Other Important Details</strong><br>
<img src="/img/note_nlp_lexicalized_probabilistic_context_free_grammars_other_important_details.png" alt="Image Loading"><br>
binalize。</p>
<h3 id="accuracy-of-lexicalized-probabilistic-context-free-grammars"><a class="markdownIt-Anchor" href="#accuracy-of-lexicalized-probabilistic-context-free-grammars"></a> Accuracy of lexicalized probabilistic context-free grammars</h3>
<p><strong>Method 1: Constituents/Precision and Recall</strong><br>
Evaluation: Representing Trees as Constituents<br>
<img src="/img/note_nlp_evaluation_representing_trees_as_constituents.png" alt="Image Loading"><br>
Precision and Recall<br>
<img src="/img/note_nlp_precision_and_recall.png" alt="Image Loading"><br>
Results<br>
<img src="/img/note_nlp_accuracy_of_lexicalized_pcfgs_results.png" alt="Image Loading"></p>
<p><strong>Method 2: Dependency Accuracies</strong><br>
Example:<br>
<img src="/img/note_nlp_dependency_accuracies_m2_example.png" alt="Image Loading"><br>
三列分别是：head word(h)/modifier word(w)/rule。<br>
Dependency Accuracies:<br>
<img src="/img/note_nlp_dependency_accuracies.png" alt="Image Loading"><br>
Strengths and Weaknesses of Modern Parsers:<br>
<img src="/img/note_nlp_strengths_and_weaknesses_of_modern_parsers.png" alt="Image Loading"></p>
<h3 id="summary-4"><a class="markdownIt-Anchor" href="#summary-4"></a> Summary</h3>
<ul>
<li>Key weakness of PCFGs: lack of sensitivity to lexical information.</li>
<li>Lexicalized PCFGs:<br>
Lexicalize a treebank using head rules.<br>
Estimate the parameters of a lexicalized PCFG using smoothed estimation.</li>
<li>Accuracy of lexicalized PCFGs: around 88% in recovering constituents or dependencies.</li>
</ul>
<hr>
<h2 id="introduction-to-machine-translationmt"><a class="markdownIt-Anchor" href="#introduction-to-machine-translationmt"></a> Introduction to Machine Translation(MT)</h2>
<p><img src="/img/note_nlp_introduction_to_machine_translation.png" alt="Image Loading"></p>
<h3 id="challenges-in-machine-translation"><a class="markdownIt-Anchor" href="#challenges-in-machine-translation"></a> Challenges in machine translation</h3>
<p><strong>Challenges: Lexical Ambiguity</strong><br>
<img src="/img/note_nlp_challenges_lexical_ambiguity.png" alt="Image Loading"><br>
从英语翻译到西班牙语，英语同词不同义在翻译时的不同选择。</p>
<p><strong>Challenges: Differing Word Orders</strong><br>
<img src="/img/note_nlp_challenges_differing_word_orders.png" alt="Image Loading"><br>
老例子，英语与日语的语法中词的顺序不同。</p>
<p><strong>Syntactics Structure is not Preserved Across Translations</strong><br>
Example from Dorr et. al, 1999<br>
<img src="/img/note_nlp_syntactic_structure_is_not_preserved_across_translations.png" alt="Image Loading"><br>
float这个动词在下面不再那么动词了。此例中floated对应floating，into对应entered。</p>
<p><strong>Syntactic Ambiguity Causes Problems</strong><br>
Example from Dorr et. al, 1999<br>
<img src="/img/note_nlp_syntactic_ambiguity_causes_problems.png" alt="Image Loading"><br>
是John用棍子打狗，还是狗自己有棍子。with the stick是修饰hit还是dog。</p>
<p><strong>Pronoun Resolution</strong><br>
Example from Dorr et. al, 1999<br>
<img src="/img/note_nlp_pronoun_resolution.png" alt="Image Loading"><br>
it可以是The computer，也可以是the data。</p>
<h3 id="classical-machine-translation"><a class="markdownIt-Anchor" href="#classical-machine-translation"></a> Classical machine translation</h3>
<p>都是Rule-based。<br>
<strong>Direct Machine Translation</strong></p>
<ul>
<li>Translation is word-by-word.</li>
<li>Very little analysis of the source text(e.g., no syntactic or semantic analysis).</li>
<li>Relies on a large bilingual directionary. For each word in the source language, the dictionary specifies a set of rules for translating that word.</li>
<li>After the words are translated, simple reordering rules are applied (e.g., move adjectives after nouns when translating from English to French).</li>
</ul>
<p>An Example:<br>
<img src="/img/note_nlp_an_example_of_a_set_of_direct_translation_rules.png" alt="Image Loading"></p>
<p>Some Problems with Direct Machine Translation:<br>
<img src="/img/note_nlp_some_problems_with_direct_machine_translation.png" alt="Image Loading"></p>
<p><strong>Transfer-Based Approaches</strong><br>
Three phases in translation:</p>
<blockquote>
<p>Analysis: Analyze the source language sentence; for example, build a syntactic analysis of the source language sentence.<br>
Transfer: Convert the source-language parse tree to a target-language parse tree.<br>
Generation: Convert the target-language parse tree to an output sentence.</p>
</blockquote>
<ul>
<li>The “parse trees” involved can vary from shallow analyses to much deeper analyses (even semantic representations).</li>
<li>The transfer rules might look quite similar to the rules for direct translation systems. But they can now operate on syntactic structures.</li>
<li>It’s easier with these approaches to handle long-distance reorderings.</li>
<li>The <em>Systran</em> systems are a classic example of this approach.</li>
</ul>
<p><img src="/img/note_nlp_transfer_based_approaches1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_transfer_based_approaches2.png" alt="Image Loading"></p>
<p><strong>Interlingua-Based Translation</strong><br>
Two phases in translation:</p>
<ul>
<li>Analysis: Analyze the source language sentence into a (language-independent) representataion of its meaning.</li>
<li>Generation: Convert the meaning representation into an output sentence.</li>
</ul>
<p>One Advantage: If we want to build a translation system that translates between <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span> languages, we need to develop <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span> analysis and generation systems. With a transfer based system, we’d need to develop <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">n</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span> sets of translation rules.<br>
Disadvantage: What would a language-independent representation look like?</p>
<ul>
<li>How to represent different concepts in a interlingua?</li>
<li>Different languages break down concepts in quite different ways:<br>
German has two words for <em>wall</em>: one for an internal wall, one for a wall that is outside.<br>
Japanese has two words for <em>brother</em>: one for an elder brother, one for a younger brother.<br>
Spanish has two words for <em>leg</em>: pierna for a human’s leg, pata for an animal’s leg, or the leg of a table.</li>
<li>An interlingua might end up simple being an intersection of these different ways of breaking down concepts, but that doesn’t seem very satisfactory…</li>
</ul>
<p><strong>A Pyramid Represnts These Three</strong><br>
<img src="/img/note_nlp_classical_machine_translation_pyramid.png" alt="Image Loading"></p>
<h3 id="a-brief-introduction-to-statistical-mt"><a class="markdownIt-Anchor" href="#a-brief-introduction-to-statistical-mt"></a> A brief introduction to statistical MT</h3>
<ul>
<li>Parallel corpora are available in several language pairs.</li>
<li>Basic idea: use a parallel corpus as a training set of translation examples.</li>
<li>Classic example: IBM work on French-English translation, using the Canadian Hansards. (1.7 million sentences of 30 words or less in length). (Early 1990s)</li>
<li>Google Translation用的就是这个。</li>
<li>Idea goes back to Warren Weaver (1949): suggested applying statistical and cryptanalytic techniques to translation.<br>
Warren Weaver, 1949, in a letter to Norbert Wiener:<br>
&quot;… one naturally wonders if the problem of translation could conceivably be treated as a problem in cryptography. When I look at an article in Russian, I say: “This is really written in English, but it has been coded in some strange symbols. I will now proceed to decode.”</li>
</ul>
<p><strong>The Noisy Channel Model</strong><br>
<img src="/img/note_nlp_noisy_channel_model.png" alt="Image Loading"><br>
e是一个英文句子。f是一个法文句子。<br>
p(e): a probability to any sentence in English.<br>
p(f|e): the conditional probability of any French sentence given an English sentence.<br>
<img src="/img/note_nlp_noisy_channel_model2.png" alt="Image Loading"><br>
Example from Koehn and Knight tutorial:<br>
<img src="/img/note_nlp_example_from_koehn_and_knight_tutorial1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_example_from_koehn_and_knight_tutorial2.png" alt="Image Loading"></p>
<hr>
<h2 id="the-ibm-translation-models"><a class="markdownIt-Anchor" href="#the-ibm-translation-models"></a> The IBM Translation Models</h2>
<p>First Generation Translation Systems.<br>
IBM Model做到了1-5，2就已经很不错了。</p>
<h3 id="ibm-model-1"><a class="markdownIt-Anchor" href="#ibm-model-1"></a> IBM Model 1</h3>
<p><strong>IBM Model 1: Alignments - Introduction</strong></p>
<ul>
<li>How do we model <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mi>f</mi><mi mathvariant="normal">∣</mi><mi>e</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">p(f|e)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mord mathrm">∣</span><span class="mord mathit">e</span><span class="mclose">)</span></span></span></span>?</li>
<li>English sentence <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>e</mi></mrow><annotation encoding="application/x-tex">e</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">e</span></span></span></span> has <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.01968em;">l</span></span></span></span> words <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>e</mi><mn>1</mn></msub><mo>⋯</mo><msub><mi>e</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">e_1 \cdots e_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="minner">⋯</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.01968em;">l</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>，French sentence <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span> has <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">m</span></span></span></span> words <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>f</mi><mn>1</mn></msub><mo>⋯</mo><msub><mi>f</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">f_1 \cdots f_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.10764em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="minner">⋯</span><span class="mord"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.10764em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">m</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>.</li>
<li>An alignment <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span></span></span></span> identifies which English word each French word originated from.</li>
<li>Formally, an alignment <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span></span></span></span> is <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>{</mo><msub><mi>a</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><msub><mi>a</mi><mi>m</mi></msub><mo>}</mo></mrow><annotation encoding="application/x-tex">\{ a_1, \cdots a_m \}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">{</span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="minner">⋯</span><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">m</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">}</span></span></span></span>, where each <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>j</mi></msub><mo>∈</mo><mo>{</mo><mn>0</mn><mo>⋯</mo><mi>l</mi><mo>}</mo></mrow><annotation encoding="application/x-tex">a_j \in \{ 0 \cdots l \}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">a</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∈</span><span class="mopen">{</span><span class="mord mathrm">0</span><span class="minner">⋯</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mclose">}</span></span></span></span>.</li>
<li>There are <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>l</mi><mo>+</mo><mn>1</mn><msup><mo>)</mo><mi>m</mi></msup></mrow><annotation encoding="application/x-tex">(l+1)^m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mbin">+</span><span class="mord mathrm">1</span><span class="mclose"><span class="mclose">)</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">m</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> possible alignments.</li>
<li>e.g., <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">l=6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mrel">=</span><span class="mord mathrm">6</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi><mo>=</mo><mn>7</mn></mrow><annotation encoding="application/x-tex">m=7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">m</span><span class="mrel">=</span><span class="mord mathrm">7</span></span></span></span><br>
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>e</mi></mrow><annotation encoding="application/x-tex">e</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">e</span></span></span></span> = And the program has been implemented<br>
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span> = Le programme a ete mis en application<br>
One alignment is {2,3,4,5,6,6,6}<br>
Another (bad!) alignment is {1,1,1,1,1,1,1}</li>
</ul>
<p><strong>Alignments in the IBM Models</strong><br>
<img src="/img/note_nlp_alighments_in_the_ibm_models.png" alt="Image Loading"><br>
例如 p(le chien aboie,{1,2,3} | the dog barks, 3)</p>
<p><strong>A By-Product: Most Likely Alignments</strong><br>
<img src="/img/note_nlp_a_by_product_most_likely_alignments.png" alt="Image Loading"><br>
IBM Model现在不做翻译，反而主要来找最可能的Alignment了。</p>
<p><strong>An Example Alignment</strong><br>
<img src="/img/note_nlp_an_example_alignment.png" alt="Image Loading"></p>
<p><strong>IBM Model 1: Alignments</strong><br>
Step 1:<br>
<img src="/img/note_nlp_ibm_model_1_alignments.png" alt="Image Loading"></p>
<p><strong>IBM Model 1: Translation Probabilities</strong><br>
Step 2:<br>
<img src="/img/note_nlp_ibm_model_1_translation_probabilities.png" alt="Image Loading"><br>
例如：<br>
e = NULL the dog barks 于是l=3<br>
a = {1, 2, 3}<br>
f = le chian aboie 于是m=3<br>
p(f|a,e,m) = t(le|the) x t(chien|dog) x t(aboie|barks)</p>
<p>又一个例子：<br>
<img src="/img/note_nlp_ibm_model_1_translation_probabilities_example.png" alt="Image Loading"></p>
<p><strong>IBM Model 1: The Generative Process</strong><br>
Generation:<br>
<img src="/img/note_nlp_ibm_model_1_the_generative_process.png" alt="Image Loading"></p>
<p><strong>An Example Lexical Entry</strong><br>
<img src="/img/note_nlp_an_example_lexical_entry.png" alt="Image Loading"></p>
<h3 id="ibm-model-2"><a class="markdownIt-Anchor" href="#ibm-model-2"></a> IBM Model 2</h3>
<p>** IBM Model 2 - Introduction**<br>
<img src="/img/note_nlp_ibm_model_2.png" alt="Image Loading"><br>
只是a部分的模型不同。</p>
<p><strong>An Example</strong><br>
<img src="/img/note_nlp_ibm_model_2_example.png" alt="Image Loading"><br>
<img src="/img/note_nlp_ibm_model_2_example2.png" alt="Image Loading"></p>
<p><strong>IBM Model 2: The Generative Process</strong><br>
<img src="/img/note_nlp_ibm_model_2_the_generative_process.png" alt="Image Loading"></p>
<p><strong>Recovering Alignments</strong><br>
<img src="/img/note_nlp_recovering_alignments.png" alt="Image Loading"></p>
<h3 id="em-training-of-models-1-and-2"><a class="markdownIt-Anchor" href="#em-training-of-models-1-and-2"></a> EM Training of Models 1 and 2</h3>
<p>Parameter estimation in models 1 and 2.<br>
<strong>The Parameter Estimation Problem</strong><br>
<img src="/img/note_nlp_the_parameter_estimation_problem.png" alt="Image Loading"></p>
<p><strong>Parameter Estimation if the Alignments are Observed</strong><br>
<img src="/img/note_nlp_parameter_estimation_if_the_alignments_are_observed1.png" alt="Image Loading"><br>
在已有a关系的情况下，只需要计算count相除就可以。<br>
<img src="/img/note_nlp_parameter_estimation_if_the_alignments_are_observed2.png" alt="Image Loading"></p>
<p><strong>Parameter Estimation with the EM Algorithm</strong><br>
<img src="/img/note_nlp_parameter_estimation_with_the_em_algorithm1.png" alt="Image Loading"><br>
迭代改进。<br>
<img src="/img/note_nlp_parameter_estimation_with_the_em_algorithm2.png" alt="Image Loading"><br>
<img src="/img/note_nlp_parameter_estimation_with_the_em_algorithm3.png" alt="Image Loading"></p>
<p><strong>Justification for the Algorithm</strong><br>
<img src="/img/note_nlp_justification_for_the_algorithm.png" alt="Image Loading"></p>
<h3 id="summary-5"><a class="markdownIt-Anchor" href="#summary-5"></a> Summary</h3>
<p><img src="/img/note_nlp_the_ibm_translation_models_summary.png" alt="Image Loading"></p>
<hr>
<h2 id="phrase-based-translation-models"><a class="markdownIt-Anchor" href="#phrase-based-translation-models"></a> Phrase-based Translation Models</h2>
<p>Second Generation Translation Systems.<br>
Late 1990s.<br>
Google Translate.</p>
<h3 id="learning-phrases-from-alignments"><a class="markdownIt-Anchor" href="#learning-phrases-from-alignments"></a> Learning Phrases From Alignments</h3>
<p><strong>Phrase-Based Models</strong><br>
<img src="/img/note_nlp_phrased_based_models.png" alt="Image Loading"></p>
<p><strong>An Example (from tutorial by Koehn and Knight):</strong><br>
Calculate PB Lexicon Pairs using IBM Model 2.<br>
<img src="/img/note_nlp_pb_lexicon_pairs_example.png" alt="Image Loading"><br>
<img src="/img/note_nlp_pb_lexicon_pairs_example_ibm_model_2.png" alt="Image Loading"></p>
<p><strong>Representation as Alignment Matrix</strong><br>
<img src="/img/note_nlp_representation_as_alignment_matrix.png" alt="Image Loading"></p>
<p><strong>Finding Alignment Matrices</strong><br>
问题：</p>
<ul>
<li>Alignments are often “noisy”.</li>
<li>“Many to One” instead of “Many to Many”.</li>
</ul>
<p>解决方法：</p>
<ul>
<li>Step 1: train IBM model 2 for p(f|e), and come up with most likely alignment for each (e,f) pair.</li>
<li>Step 2: train IBM model 2 for p(e|f) and come up with most likely alignment for each (e,f) pair.</li>
<li>We now have two alignments:<br>
take intersection of the two alignments as a starting point.</li>
</ul>
<p><img src="/img/note_nlp_finding_alignment_matrices2.png" alt="Image Loading"><br>
<img src="/img/note_nlp_finding_alignment_matrices3.png" alt="Image Loading"></p>
<p><strong>Heuristics for Growing Alignments</strong></p>
<ul>
<li>Only explore alignment in <strong>union</strong> of p(f|e) and p(e|f) alignments.</li>
<li>Add one alignment point at a time.</li>
<li>Only add alignment points which align a word that currently has no alignment.</li>
<li>At first, restrict ourselves to alignment points that are “neighbors” (adjacent or diagonal) of current alignment points.</li>
<li>Later, consider other alignment points.</li>
</ul>
<p><img src="/img/note_nlp_heuristics_for_growing_alignments.png" alt="Image Loading"></p>
<p><strong>Extracting Phrase Pairs from the Alignment Matrix</strong><br>
<img src="/img/note_nlp_extracting_phrase_pairs_from_the_alignment_matrix.png" alt="Image Loading"></p>
<p><strong>Probabilities for Phrase Pairs</strong><br>
<img src="/img/note_nlp_probabilities_for_phrase_pairs.png" alt="Image Loading"></p>
<p><strong>An Example Phrase Translation Table</strong><br>
<img src="/img/note_nlp_an_example_phrase_translation_table.png" alt="Image Loading"></p>
<h3 id="a-phrase-based-model"><a class="markdownIt-Anchor" href="#a-phrase-based-model"></a> A Phrase-Based Model</h3>
<p><strong>A Sketch</strong><br>
<img src="/img/note_nlp_phrased_based_system_a_sketch_step1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_phrased_based_system_a_sketch_step2.png" alt="Image Loading"><br>
<img src="/img/note_nlp_phrased_based_system_a_sketch_step3.png" alt="Image Loading"><br>
6是因为跳过了6个词，那是一个负值，为了不让两种语言句子结构跳跃太大的penalty。<br>
<img src="/img/note_nlp_phrased_based_system_a_sketch_step4.png" alt="Image Loading"><br>
<img src="/img/note_nlp_phrased_based_system_a_sketch_step5.png" alt="Image Loading"></p>
<h3 id="decoding-of-phrase-based-translation-models"><a class="markdownIt-Anchor" href="#decoding-of-phrase-based-translation-models"></a> Decoding of Phrase-based Translation Models</h3>
<h4 id="definition-of-the-decoding-problem"><a class="markdownIt-Anchor" href="#definition-of-the-decoding-problem"></a> Definition of the Decoding Problem</h4>
<p><strong>Phrase-based Translation</strong><br>
<img src="/img/note_nlp_phrased_based_translation.png" alt="Image Loading"></p>
<p><strong>Phrase-based Models: Definitions</strong><br>
<img src="/img/note_nlp_phrased_based_models_definitions.png" alt="Image Loading"></p>
<p><strong>Phrased-based Translation: Definitions</strong><br>
<img src="/img/note_nlp_phrased_based_translation_definitions.png" alt="Image Loading"><br>
s: start point.<br>
t: end point.<br>
e: sequence of 1 or more english words.</p>
<p><strong>Definitions</strong><br>
<img src="/img/note_nlp_definitions.png" alt="Image Loading"></p>
<p><strong>Valid Derivations</strong><br>
<img src="/img/note_nlp_valid_derivations.png" alt="Image Loading"><br>
第三条：distortion limit: a hard constraint on how far phrases can move.<br>
两个(s,t,e)组之间的距离不能离太远。前一组的尾与后一组的头之间。<br>
这个方法很僵化，已被淘汰。<br>
y(x)数量会随着句子长度指数增长。</p>
<p><strong>Scoring Derivations</strong><br>
<img src="/img/note_nlp_scoring_derivations.png" alt="Image Loading"><br>
An Example:<br>
<img src="/img/note_nlp_scoring_derivations_example.png" alt="Image Loading"><br>
We must also take this critic seriously.</p>
<ol>
<li>Langauge Model. 这个英语句子有多可能<br>
log q(we|**)+log(must|*,we)+log(also|we,must)+log(the|must,also)+…</li>
<li>Phrase Scores. 这个英语句子与德语有多匹配<br>
g(1,3,we must also)+g(7,7,take)+g(4,5,the critic)+…</li>
<li>Distortion Scores. 对语法结构差太多的惩罚<br>
|1-1|x{zeta}+|3+1-7|x{zeta}+|7+1-4|x{zeta}+|5+1-6|x{zeta}</li>
</ol>
<h4 id="the-decoding-algorithm"><a class="markdownIt-Anchor" href="#the-decoding-algorithm"></a> The Decoding Algorithm</h4>
<p>上面的问题NP hard，需要用到heuristic的算法。</p>
<p><strong>Decoding Algorithm: Definitions</strong><br>
<img src="/img/note_nlp_decoding_algorithm_definitions.png" alt="Image Loading"><br>
例如(we, must, 1100000, 2, -2.5)<br>
中间的int有点像占位，哪些词被翻译了。</p>
<p><strong>States, and the Search Space</strong><br>
<img src="/img/note_nlp_states_and_the_search_space.png" alt="Image Loading"><br>
可以表现为这样的树。</p>
<p><strong>Transitions</strong><br>
<img src="/img/note_nlp_transitions.png" alt="Image Loading"><br>
ph(q)就是每个state的可行选择。</p>
<p><strong>The next function</strong><br>
We define <code>next(q,p)</code> to be the state formed by combining state q with phrase p.<br>
<img src="/img/note_nlp_the_next_function.png" alt="Image Loading"></p>
<p><strong>The Equality Function</strong><br>
<img src="/img/note_nlp_the_equality_function.png" alt="Image Loading"><br>
判断两个state是否相同。</p>
<p><strong>The Decoding Algorithm</strong><br>
<img src="/img/note_nlp_the_decoding_algorithm.png" alt="Image Loading"></p>
<p><strong>Definition of Add(Q, q’, q, p)</strong><br>
<img src="/img/note_nlp_definition_of_add.png" alt="Image Loading"><br>
如果走到了同一个state，谁分数高留谁。</p>
<p><strong>Definition of beam(Q)</strong><br>
<img src="/img/note_definition_of_beam.png" alt="Image Loading"><br>
图中不应该有arg，错误。<br>
比最高分低太多的直接删了。</p>
<hr>
<h2 id="log-linear-models"><a class="markdownIt-Anchor" href="#log-linear-models"></a> Log-linear Models</h2>
<h3 id="two-example-problems"><a class="markdownIt-Anchor" href="#two-example-problems"></a> Two Example Problems</h3>
<p><strong>The Language Modeling Problem</strong><br>
<img src="/img/note_nlp_the_language_modeling_problem.png" alt="Image Loading"></p>
<p>Trigram Models:<br>
<img src="/img/note_nlp_trigram_models.png" alt="Image Loading"><br>
<img src="/img/note_nlp_trigram_models2.png" alt="Image Loading"><br>
利用的信息太少了。</p>
<p>A Naive Approach:<br>
<img src="/img/note_nlp_a_naive_approach.png" alt="Image Loading"><br>
一个Smooth Model，加权考虑每个信息。很快就大到无法控制。</p>
<p><strong>A Second Example: Part-of_Speech Tagging</strong><br>
<img src="/img/note_nlp_a_second_example_part_of_speech_tagging.png" alt="Image Loading"><br>
那么问题来了，??该是什么？<br>
<img src="/img/note_nlp_a_second_example_part_of_speech_tagging2.png" alt="Image Loading"><br>
<img src="/img/note_nlp_a_second_example_part_of_speech_tagging3.png" alt="Image Loading"><br>
同样的，有很多没有利用到的信息。</p>
<h3 id="features-in-log-linear-models"><a class="markdownIt-Anchor" href="#features-in-log-linear-models"></a> Features in Log-Linear Models</h3>
<h4 id="for-problem-1"><a class="markdownIt-Anchor" href="#for-problem-1"></a> For Problem 1</h4>
<p><strong>The General Problem</strong></p>
<ul>
<li>We have some <strong>input domain X</strong></li>
<li>Have a finite <strong>label set Y</strong></li>
<li>Aim is to provide a <strong>conditional probability</strong> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">p(y|x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mord mathrm">∣</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span> for any x,y where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>∈</mo><mi>X</mi><mo separator="true">,</mo><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">x \in  X, y \in Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span><span class="mrel">∈</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mrel">∈</span><span class="mord mathit" style="margin-right:0.22222em;">Y</span></span></span></span></li>
</ul>
<p><strong>Language Modeling</strong></p>
<ul>
<li>x is a “history” w1, w2, …, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">w_{i-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mbin">−</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> e.g.,<br>
Third, the notion “grammatical in English” cannot be identified in any way with the notion “hign order of statistical approximation to English”. It is fair to assume that neither sentence (1) nor (2) (nor indeed any part of these sentences) has ever occurred in an English discourse. Hence, in any statistical</li>
<li>y is an “outcome” <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></li>
</ul>
<p><strong>Feature Vector Representations</strong><br>
<img src="/img/note_nlp_feature_vector_representations.png" alt="Image Loading"></p>
<p><strong>Language Modeling</strong><br>
<img src="/img/note_nlp_language_modeling1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_language_modeling2.png" alt="Image Loading"><br>
f1相当于unigram feature，f2相当于bigram feature，f3相当于trigram feature，f4相当于skip bigram feature。</p>
<p><strong>Defining Features in Practice</strong><br>
<img src="/img/note_nlp_defining_features_in_practice.png" alt="Image Loading"><br>
Do not include trigrams not seen in training data.<br>
Lead to:<br>
Too many features;<br>
Many not worth including in the model.</p>
<h4 id="for-problem-2"><a class="markdownIt-Anchor" href="#for-problem-2"></a> For Problem 2</h4>
<p><strong>The POS-Tagging Example</strong><br>
<img src="/img/note_the_pos_tagging_example.png" alt="Image Loading"><br>
i: the position being tagged.</p>
<p><strong>The Full Set of Features in Ratnaparkhi, 1996</strong><br>
<img src="/img/note_nlp_the_full_set_of_features_in_ratnaparkhi_1996_1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_the_full_set_of_features_in_ratnaparkhi_1996_2.png" alt="Image Loading"></p>
<p><strong>The Final Result</strong><br>
<img src="/img/note_nlp_the_final_result.png" alt="Image Loading"><br>
绿色是x，红色是y。<br>
Often sparse: relatively few 1’s than 0’s.</p>
<h3 id="definition-of-log-linear-models"><a class="markdownIt-Anchor" href="#definition-of-log-linear-models"></a> Definition of Log-linear Models</h3>
<p><strong>Parameter Vectors</strong><br>
前文介绍了feature，这里再引入parameter vector的概念。<br>
<img src="/img/note_nlp_parameter_vectors.png" alt="Image Loading"><br>
两个vector点乘。</p>
<p><strong>Language Modeling Example</strong><br>
<img src="/img/note_nlp_language_modeling.png" alt="Image Loading"><br>
p(model|x;v) = e^5.6 / (e^5.6 + e^1.5 + e^4.5 + e^-3.2 + …)<br>
p(the|x;v) = e^-3.2 / (e^5.6 + e^1.5 + e^4.5 + e^-3.2 + …)</p>
<p><strong>Log-Linear Models</strong><br>
总结定义：<br>
<img src="/img/note_nlp_log_linear_models.png" alt="Image Loading"><br>
经过这样处理后就满足条件p都大于0，且和为1了。</p>
<p><strong>Why the name?</strong><br>
<img src="/img/note_nlp_why_the_name.png" alt="Image Loading"></p>
<h3 id="parameter-estimation-in-log-linear-models"><a class="markdownIt-Anchor" href="#parameter-estimation-in-log-linear-models"></a> Parameter Estimation in Log-linear Models</h3>
<p>求v。<br>
<strong>Maximum-Likelihood Estimation</strong><br>
<img src="/img/note_nlp_maximum_likelihood_estimation.png" alt="Image Loading"><br>
选让整个training example总值最大的。naive。<br>
L(v)是一个倒吊钟形的函数，concave。比较容易优化，可以用gradient ascent。</p>
<p><strong>Calculating the Maximum-Likelihood Estimates</strong><br>
<img src="/img/note_nlp_calculating_the_maximum_likelihood_estimates.png" alt="Image Loading"></p>
<p><strong>Gradient Ascent Methods</strong><br>
<img src="/img/note_nlp_gradient_ascent_methods.png" alt="Image Loading"></p>
<p><strong>Conjugate Gradient Methods</strong><br>
<img src="/img/note_nlp_conjugate_gradient_methods.png" alt="Image Loading"><br>
一般情况下工具包已经实现此算法。</p>
<h3 id="smoothingregularization-in-log-linear-models"><a class="markdownIt-Anchor" href="#smoothingregularization-in-log-linear-models"></a> Smoothing/Regularization in Log-linear Models</h3>
<p><strong>Smoothing in Log-Linear Models</strong><br>
<img src="/img/note_nlp_smoothing_in_log_linear_models.png" alt="Image Loading"></p>
<p><strong>Regularization</strong><br>
<img src="/img/note_nlp_regularization.png" alt="Image Loading"></p>
<p><strong>Experiments with Regularization</strong><br>
<img src="/img/note_nlp_experiments_with_regularization.png" alt="Image Loading"></p>
<p><strong>Experiments with Gaussian Priors</strong><br>
<img src="/img/note_nlp_experiments_with_gaussian_priors.png" alt="Image Loading"></p>
<hr>
<h2 id="log-linear-models-for-taggingmemms"><a class="markdownIt-Anchor" href="#log-linear-models-for-taggingmemms"></a> Log-linear Models for Tagging(MEMMs)</h2>
<p>Applications for log-linear models.<br>
MEMMs : Maximum-entropy(熵) Markov Models</p>
<h3 id="recap-the-tagging-problem"><a class="markdownIt-Anchor" href="#recap-the-tagging-problem"></a> Recap: The Tagging Problem</h3>
<p>有以下几种：<br>
<strong>Part-of-Speech Tagging</strong><br>
<img src="/img/note_nlp_part_of_speech_tagging.png" alt="Image Loading"></p>
<p><strong>Named Entity Recognition</strong><br>
<img src="/img/note_nlp_named_entity_recognition.png" alt="Image Loading"></p>
<p><strong>Named Entity Extraction as Tagging</strong><br>
<img src="/img/note_nlp_named_entity_extraction_as_tagging.png" alt="Image Loading"></p>
<p><strong>Our Goal</strong><br>
<img src="/img/note_nlp_log_linear_tagging_our_goal.png" alt="Image Loading"><br>
已经讲过的方法有：<br>
Hidden Markov Models 和 Viterbi Algorithm。</p>
<h3 id="log-linear-taggers"><a class="markdownIt-Anchor" href="#log-linear-taggers"></a> Log-Linear Taggers</h3>
<h4 id="independence-assumptions-in-log-linear-taggers"><a class="markdownIt-Anchor" href="#independence-assumptions-in-log-linear-taggers"></a> Independence Assumptions in Log-linear Taggers</h4>
<p><strong>Log-Linear Models for Tagging</strong><br>
<img src="/img/note_nlp_log_linear_models_for_tagging.png" alt="Image Loading"></p>
<p><strong>How to model p(t[1:n]|w[1:n])?</strong><br>
<img src="/img/note_nlp_how_to_model_ptw.png" alt="Image Loading"><br>
例子：<code>the dog barks</code> <code>**DNV</code><br>
<code>P(D N V|the dog barks) = P(D|the dog barks, * *) x P(N|the dog barks, * D) x P(V|the dog barks, D N)</code></p>
<h4 id="features-in-log-linear-taggers"><a class="markdownIt-Anchor" href="#features-in-log-linear-taggers"></a> Features in Log-Linear Taggers</h4>
<p><strong>Representation: Histories</strong><br>
<img src="/img/note_nlp_representation_histories.png" alt="Image Loading"></p>
<p><strong>Recap: Feature Vector Representations in Log-Linear Models</strong><br>
<img src="/img/note_nlp_feature_vector_representations_in_log_linear_models.png" alt="Image Loading"></p>
<p><strong>An Example</strong><br>
<img src="/img/note_nlp_an_example_log_linear_taggers.png" alt="Image Loading"><br>
<img src="/img/note_nlp_an_example_log_linear_taggers_feature.png" alt="Image Loading"><br>
f2对于HMM来说很难。</p>
<p><strong>The Full Set of Features in [(Ratnaparkhi, 96)]</strong><br>
<img src="/img/note_nlp_the_full_set_of_features_in_ratnaparkhi_1996_1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_the_full_set_of_features_in_ratnaparkhi_1996_2.png" alt="Image Loading"></p>
<h4 id="parameters-in-log-linear-models"><a class="markdownIt-Anchor" href="#parameters-in-log-linear-models"></a> Parameters in Log-linear Models</h4>
<p><strong>Recap: Log-Linear Models</strong><br>
<img src="/img/note_nlp_log_linear_models_recap.png" alt="Image Loading"></p>
<p><strong>Training the Log-Linear Model</strong><br>
<img src="/img/note_nlp_training_the_log_linear_model.png" alt="Image Loading"></p>
<h4 id="the-viterbi-algorithm-for-log-linear-taggers"><a class="markdownIt-Anchor" href="#the-viterbi-algorithm-for-log-linear-taggers"></a> The Viterbi Algorithm for Log-linear Taggers</h4>
<p><strong>The Viterbi Algorithm</strong><br>
<img src="/img/note_nlp_the_viterbi_algorithm1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_the_viterbi_algorithm2.png" alt="Image Loading"></p>
<p><strong>A Recursive Definition</strong><br>
<img src="/img/note_nlp_a_recursive_definition.png" alt="Image Loading"><br>
与HMM的区别只在q那里。</p>
<p><strong>The Viterbi Algorithm with Backpointers</strong><br>
<img src="/img/note_nlp_the_viterbi_algorithm_with_backpointers_llt.png" alt="Image Loading"></p>
<h4 id="an-example-application"><a class="markdownIt-Anchor" href="#an-example-application"></a> An Example Application</h4>
<p><strong>FAQ Segmentation: McCallum et. al</strong></p>
<ul>
<li>McCallum et. al compared HMM and log-linear taggers on a FAQ Segmentation task</li>
<li>Main point: in an HMM, modeling <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mi mathvariant="normal">∣</mi><mi>t</mi><mi>a</mi><mi>g</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">p(word|tag)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">d</span><span class="mord mathrm">∣</span><span class="mord mathit">t</span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mclose">)</span></span></span></span> is difficult in this domain</li>
</ul>
<p>从文本中自动tag哪些行是head、question、answer。</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span>X-NNTP-POSTER: NewsHound v1.33</span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span>Archive name: acorn/faq/part2</span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span>Frequency: monthly</span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">question</span>&gt;</span>2.6) What configuration of serial cable should I use</span><br><span class="line"><span class="tag">&lt;<span class="name">answer</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">answer</span>&gt;</span> Here follows a diagram of the necessary connections</span><br><span class="line"><span class="tag">&lt;<span class="name">answer</span>&gt;</span>programs to work properly. They are as far as I know t</span><br><span class="line"><span class="tag">&lt;<span class="name">answer</span>&gt;</span>agreed upon by commercial comms software developers fo</span><br><span class="line"><span class="tag">&lt;<span class="name">answer</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">answer</span>&gt;</span> Pins 1, 4, and 8 must be connected together inside</span><br><span class="line"><span class="tag">&lt;<span class="name">answer</span>&gt;</span>is to avoid the well known serial port chip bugs. The</span><br></pre></td></tr></table></figure>
<p><strong>FAQ Segmentation: Line Features</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">begins-with-number</span><br><span class="line">begins-with-ordinal</span><br><span class="line">begins-with-punctuation</span><br><span class="line">begins-with-question-word</span><br><span class="line">begins-with-subject</span><br><span class="line">blank</span><br><span class="line">contains-alphanum</span><br><span class="line">contains-bracketed-number</span><br><span class="line">contains-http</span><br><span class="line">contains-non-space</span><br><span class="line">contains-number</span><br><span class="line">contains-pipe</span><br><span class="line">contains-question-mark</span><br><span class="line">ends-with-question-mark</span><br><span class="line">first-alpha-is-capitalized</span><br><span class="line">indented-1-to-4</span><br></pre></td></tr></table></figure>
<p><strong>FAQ Segmentation: The Log-Linear Tagger</strong><br>
得到feature。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">“tag=question;prev=head;begins-with-number”</span><br><span class="line">“tag=question;prev=head;contains-alphanum”</span><br><span class="line">“tag=question;prev=head;contains-nonspace”</span><br><span class="line">“tag=question;prev=head;contains-number”</span><br><span class="line">“tag=question;prev=head;prev-is-blank”</span><br></pre></td></tr></table></figure>
<p><strong>FAQ Segmentation: An HMM Tagger</strong><br>
对于：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">question</span>&gt;</span>2.6) What configuration of serial cable should I use</span><br></pre></td></tr></table></figure>
<p>得到</p>
<ul>
<li>First solution<br>
把一句话分成很多词。<br>
<img src="/img/note_nlp_faq_segmentation_an_hmm_tagger1.png" alt="Image Loading"><br>
i.e. have a <em>language model</em> for each <em>tag</em></li>
<li>Second solution<br>
<img src="/img/note_nlp_faq_segmentation_an_hmm_tagger2.png" alt="Image Loading"></li>
</ul>
<p><strong>FAQ Segmentation: Results</strong></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Precision</th>
<th>Recal</th>
</tr>
</thead>
<tbody>
<tr>
<td>ME-Stateless(Maximum Entropy Stateless)</td>
<td>0.038</td>
<td>0.362</td>
</tr>
<tr>
<td>TokenHMM</td>
<td>0.276</td>
<td>0.140</td>
</tr>
<tr>
<td>FeatureHMM</td>
<td>0.413</td>
<td>0.529</td>
</tr>
<tr>
<td>MEMM</td>
<td>0.867</td>
<td>0.681</td>
</tr>
</tbody>
</table>
<ul>
<li>Precision and recall results are for recovering segments</li>
<li>ME-stateless is a log-linear model that treats every sentence seperately (no dependence between adjacent tags)</li>
<li>TokenHMM is an HMM with first solution we’ve just seen</li>
<li>FeatureHMM is an HMM with second solution we’ve just seen</li>
<li>MEMM is a log-linear trigram tagger (MEMM stands for “Maximum-Entropy Markov Model”)</li>
</ul>
<h3 id="summary-6"><a class="markdownIt-Anchor" href="#summary-6"></a> Summary</h3>
<p><img src="/img/note_nlp_log_linear_models_for_tagging_summary.png" alt="Image Loading"></p>
<hr>
<h2 id="log-linear-models-for-history-based-parsing"><a class="markdownIt-Anchor" href="#log-linear-models-for-history-based-parsing"></a> Log-Linear Models for History-based Parsing</h2>
<p>这个方法的应用范围更广。</p>
<h3 id="conditional-history-based-models"><a class="markdownIt-Anchor" href="#conditional-history-based-models"></a> Conditional History-based Models</h3>
<p><strong>Recap: Log-Linear Taggers</strong><br>
<img src="/img/note_nlp_recap_log_linear_taggers.png" alt="Image Loading"></p>
<p><strong>A General Approach: (Conditional) History-Based Models</strong></p>
<ul>
<li>We’ve shown how to define p(t[1:n]|w[1:n]) where t[1:n] is a tag sequence.</li>
<li>How do we define p(T|S) if T is a parse tree (or another structure)? (We use the notation S = w[1:n])</li>
</ul>
<p><img src="/img/note_nlp_a_general_approach_conditional_history_based_models.png" alt="Image Loading"><br>
并不使用Markov Assumption，所以不能用动态规划。</p>
<h3 id="representing-trees-as-decision-sequences关于step-1"><a class="markdownIt-Anchor" href="#representing-trees-as-decision-sequences关于step-1"></a> Representing Trees as Decision Sequences(关于Step 1)</h3>
<p><img src="/img/note_nlp_an_example_tree.png" alt="An Example Tree"></p>
<p><strong>Ratnaparkhi’s Parser: Three Layers of Structure</strong></p>
<ol>
<li>Part-of-speech tags</li>
<li>Chunks</li>
<li>Remaining structure</li>
</ol>
<p><strong>Layer 1: Part-of-Speech Tags</strong><br>
<img src="/img/note_nlp_layer1_part_of_speech_tags.png" alt="Image Loading"></p>
<p><strong>Layer 2: Chunks</strong><br>
<img src="/img/note_nlp_layer2_chunks1.png" alt="Image Loading"><br>
QP: numeric phrases.<br>
<img src="/img/note_nlp_layer2_chunks2.png" alt="Image Loading"></p>
<p><strong>Layer 3: Remaining Structure</strong><br>
<img src="/img/note_nlp_layer3_remaining_structure.png" alt="Image Loading"><br>
前两步之后到这里：<br>
<img src="/img/note_nlp_layer3_1.png" alt="Image Loading"><br>
选择最左边的：<br>
<img src="/img/note_nlp_layer3_2.png" alt="Image Loading"><br>
因为句子并没有结束，check为no：<br>
<img src="/img/note_nlp_layer3_3.png" alt="Image Loading"><br>
继续，省略数步：<br>
<img src="/img/note_nlp_layer3_4.png" alt="Image Loading"><br>
当前的constituent已结束，所以Check为yes：<br>
<img src="/img/note_nlp_layer3_5.png" alt="Image Loading"><br>
然后再选择最左边的没有Join或Start的项：<br>
<img src="/img/note_nlp_layer3_6.png" alt="Image Loading"><br>
Check又选yes：<br>
<img src="/img/note_nlp_layer3_7.png" alt="Image Loading"><br>
继续选择最左边：<br>
<img src="/img/note_nlp_layer3_8.png" alt="Image Loading"><br>
Check为yes：<br>
<img src="/img/note_nlp_layer3_9.png" alt="Image Loading"></p>
<p><strong>The Final Sequence of decisions</strong><br>
<img src="/img/note_nlp_the_final_sequence_of_decisions.png" alt="Image Loading"></p>
<h3 id="features关于step-3"><a class="markdownIt-Anchor" href="#features关于step-3"></a> Features(关于Step 3)</h3>
<p><strong>Applying a Log-Linear Model</strong><br>
<img src="/img/note_nlp_applying_a_log_linear_model1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_applying_a_log_linear_model2.png" alt="Image Loading"></p>
<p><strong>Layer 3: Join of Start</strong></p>
<ul>
<li>Looks at head word, constituent (or POS) label, and start/join annotation of n’th tree relative to the decision, where n = −2, −1</li>
<li>Looks at head word, constituent (or POS) label of n’th tree relative to the decision, where n = 0, 1, 2</li>
<li>Looks at bigram features of the above for (-1,0) and (0,1)</li>
<li>Looks at trigram features of the above for (-2,-1,0), (-1,0,1) and (0, 1, 2)</li>
<li>The above features with all combinations of head words excluded</li>
<li>Various punctuation features</li>
</ul>
<p><strong>Layer 3: Check=NO or Check=YES</strong></p>
<ul>
<li>A variety of questions concerning the proposed constituent</li>
</ul>
<h3 id="beam-search关于step-4"><a class="markdownIt-Anchor" href="#beam-search关于step-4"></a> Beam Search(关于Step 4)</h3>
<p><strong>The Search Problem</strong><br>
<img src="/img/note_nlp_the_search_problem.png" alt="Image Loading"><br>
同理，不能用动态规划了。<br>
得到很多可能的parse tree，每个有自己的概率。</p>
<hr>
<h2 id="unsupervised-learning-brown-clustering"><a class="markdownIt-Anchor" href="#unsupervised-learning-brown-clustering"></a> Unsupervised Learning: Brown Clustering</h2>
<p>本课讲的首个Unsupervised learning algorithm。</p>
<h3 id="word-cluster-representations"><a class="markdownIt-Anchor" href="#word-cluster-representations"></a> Word Cluster Representations</h3>
<p><strong>The Brown Clustering Algorithm</strong></p>
<ul>
<li>Input: a (large) corpus of words</li>
<li>Output 1: a partition of words into <em>word clusters</em></li>
<li>Output 2 (generalization of 1): a hierarchichal word clustering</li>
</ul>
<p><strong>Example Clusters (from Brown et al, 1992)</strong><br>
论文里给出的cluster结果，效果很好啊！</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Friday Monday Thursday Wednesday Tuesday Saturday Sunday weekends Sundays Saturdays</span><br><span class="line">June March July April January December October November September August</span><br><span class="line">people guys folks fellows CEOs chaps doubters commies unfortunates blokes</span><br><span class="line">down backwards ashore sideways southward northward overboard aloft downwards adrift</span><br><span class="line">water gas coal liquid acid sand carbon steam shale iron</span><br><span class="line">great big vast sudden mere sheer gigantic lifelong scant colossal</span><br><span class="line">man woman boy girl lawyer doctor guy farmer teacher citizen</span><br><span class="line">American Indian European Japanese German African Catholic Israeli Italian Arab</span><br><span class="line">pressure temperature permeability density porosity stress velocity viscosity gravity tension</span><br><span class="line">mother wife father son husband brother daughter sister boss uncle</span><br><span class="line">machine device controller processor CPU printer spindle subsystem compiler plotter</span><br><span class="line">John George James Bob Robert Paul William Jim David Mike</span><br><span class="line">anyone someone anybody somebody</span><br><span class="line">feet miles pounds degrees inches barrels tons acres meters bytes</span><br><span class="line">director chief professor commissioner commander treasurer founder superintendent dean custodian</span><br></pre></td></tr></table></figure>
<p><strong>A Sample Hierarchy (from Miller et al., NAACL 2004)</strong><br>
论文给出的Hierarchy结果。<br>
哈夫曼树，左1右0。可以表示Hierarchy。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">lawyer 1000001101000</span><br><span class="line">newspaperman 100000110100100</span><br><span class="line">stewardess 100000110100101</span><br><span class="line">toxicologist 10000011010011</span><br><span class="line">slang 1000001101010</span><br><span class="line">babysitter 100000110101100</span><br><span class="line">conspirator 1000001101011010</span><br><span class="line">womanizer 1000001101011011</span><br><span class="line">mailman 10000011010111</span><br><span class="line">salesman 100000110110000</span><br><span class="line">bookkeeper 1000001101100010</span><br><span class="line">troubleshooter 10000011011000110</span><br><span class="line">bouncer 10000011011000111</span><br><span class="line">technician 1000001101100100</span><br><span class="line">janitor 1000001101100101</span><br><span class="line">saleswoman 1000001101100110</span><br><span class="line">...</span><br><span class="line">Nike 1011011100100101011100</span><br><span class="line">Maytag 10110111001001010111010</span><br><span class="line">Generali 10110111001001010111011</span><br><span class="line">Gap 1011011100100101011110</span><br><span class="line">Harley-Davidson 10110111001001010111110</span><br><span class="line">Enfield 101101110010010101111110</span><br><span class="line">genus 101101110010010101111111</span><br><span class="line">Microsoft 10110111001001011000</span><br><span class="line">Ventritex 101101110010010110010</span><br><span class="line">Tractebel 1011011100100101100110</span><br><span class="line">Synopsys 1011011100100101100111</span><br><span class="line">WordPerfect 1011011100100101101000</span><br><span class="line">....</span><br><span class="line">John 101110010000000000</span><br><span class="line">Consuelo 101110010000000001</span><br><span class="line">Jeffrey 101110010000000010</span><br><span class="line">Kenneth 10111001000000001100</span><br><span class="line">Phillip 101110010000000011010</span><br><span class="line">WILLIAM 101110010000000011011</span><br><span class="line">Timothy 10111001000000001110</span><br><span class="line">Terrence 101110010000000011110</span><br><span class="line">Jerald 101110010000000011111</span><br><span class="line">Harold 101110010000000100</span><br><span class="line">Frederic 101110010000000101</span><br><span class="line">Wendell 10111001000000011</span><br></pre></td></tr></table></figure>
<h3 id="the-brown-clustering-algorithm"><a class="markdownIt-Anchor" href="#the-brown-clustering-algorithm"></a> The Brown Clustering Algorithm</h3>
<p><strong>The Intuition</strong></p>
<ul>
<li>Similar words appear in similar contexts</li>
<li>More precisely: siilar words have similar distributions of words to their imediate left and right</li>
</ul>
<p><strong>The Formulation</strong><br>
<img src="/img/note_nlp_brow_clustering_algorithm_formulation.png" alt="Image Loading"><br>
<img src="/img/note_nlp_brown_example.png" alt="An Example"></p>
<p><strong>The Brown Clustering Model</strong><br>
<img src="/img/note_nlp_the_brown_clustering_model.png" alt="Image Loading"></p>
<p><strong>Measuring the Quality of C</strong><br>
<img src="/img/note_nlp_measuring_the_quality_of_c.png" alt="Image Loading"><br>
例如：</p>
<table>
<thead>
<tr>
<th>1</th>
<th>2</th>
<th>3</th>
<th>1</th>
<th>2</th>
</tr>
</thead>
<tbody>
<tr>
<td>the</td>
<td>dog</td>
<td>saw</td>
<td>the</td>
<td>cat</td>
</tr>
</tbody>
</table>
<p>n(1,2) = 2<br>
n(2) = 2</p>
<p><strong>A First Algorithm</strong><br>
<img src="/img/note_nlp_a_first_algorithm.png" alt="Image Loading"></p>
<p><strong>A Second Algorithm</strong><br>
<img src="/img/note_nlp_a_second_algorithm.png" alt="Image Loading"></p>
<h3 id="clusters-in-ne-recognition"><a class="markdownIt-Anchor" href="#clusters-in-ne-recognition"></a> Clusters in NE Recognition</h3>
<p><strong>Miller et al, NAACL 2004</strong><br>
<code>&quot;Name Tagging with Word Clusters and Discriminative Training&quot;</code><br>
Scott Miller, Jethran Guinness, Alex Zamanian</p>
<p>At a recent meeting, we presented name-tagging technology to a potential user. The technology had performed well in formal evaluations, had been applied successfully by several research groups, and required only annotated training examples to configure for new name classes. Nevertheless, it did not meet the user’s needs.</p>
<p>To achieve reasonable performance, the HMM-based technology we presented required roughly 150,000 words of annotated examples, and over a million words to achieve peak accuracy. Given a typical annotation rate of 5,000 words per hour, we estimated that setting up a name finder for a new problem would take four person days of annotation work – a period we considered reasonable. However, this user’s problems were too dynamic for that much setup time. To be useful, the system would have to be trainable in minutes or hours, not days or weeks.</p>
<p>可行的feature种类：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1. Tag + PrevTag</span><br><span class="line">2. Tag + CurWord</span><br><span class="line">3. Tag + CapAndNumFeatureOfCurWord</span><br><span class="line">4. ReducedTag + CurWord</span><br><span class="line">//collapse start and continue tags</span><br><span class="line">5. Tag + PrevWord</span><br><span class="line">6. Tag + NextWord</span><br><span class="line">7. Tag + DownCaseCurWord</span><br><span class="line">8. Tag + Pref8ofCurrWord	// 当前词的feature们的某8bit</span><br><span class="line">9. Tag + Pref12ofCurrWord</span><br><span class="line">10. Tag + Pref16ofCurrWord</span><br><span class="line">11. Tag + Pref20ofCurrWord</span><br><span class="line">12. Tag + Pref8ofPrevWord</span><br><span class="line">13. Tag + Pref12ofPrevWord</span><br><span class="line">14. Tag + Pref16ofPrevWord</span><br><span class="line">15. Tag + Pref20ofPrevWord</span><br><span class="line">16. Tag + Pref8ofNextWord</span><br><span class="line">17. Tag + Pref12ofNextWord</span><br><span class="line">18. Tag + Pref16ofNextWord</span><br><span class="line">19. Tag + Pref20ofNextWord</span><br></pre></td></tr></table></figure>
<p>结果评估：<br>
<img src="/img/note_nlp_miller_et_al1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_miller_et_al2.png" alt="Image Loading"><br>
Active Learning -&gt; Choose to label the most “useful” examples at each stage.</p>
<hr>
<h2 id="global-linear-modelsglms"><a class="markdownIt-Anchor" href="#global-linear-modelsglms"></a> Global Linear Models(GLMs)</h2>
<p><strong>Techiniques</strong></p>
<ul>
<li>So far:<br>
Smoothed estimation<br>
Probabilistic context-free grammars<br>
Log-linear models<br>
Hidden markov models<br>
The EM Algorithm<br>
History-based models</li>
<li>Today<br>
Global linear models</li>
</ul>
<h3 id="a-brief-review-of-history-based-methods"><a class="markdownIt-Anchor" href="#a-brief-review-of-history-based-methods"></a> A Brief Review of History-based Methods</h3>
<p><strong>Supervised Learning in Natural Language</strong><br>
<img src="/img/note_nlp_supervised_learning_in_natural_language.png" alt="Image Loading"></p>
<p><strong>The Models so far</strong><br>
<img src="/img/note_nlp_the_models_so_far.png" alt="Image Loading"><br>
第一个比如HMMs和PCFGs，第二个如Log-Linear Tagging。</p>
<p><strong>Example 1: PCFGs</strong><br>
<img src="/img/note_nlp_example1_pcfgs.png" alt="Image Loading"><br>
S<br>
S-&gt;NP VP<br>
q(S-&gt;NP VP) = p(S-&gt;NP VP|S)</p>
<p><strong>Example 2: Log-linear Taggers</strong><br>
<img src="/img/note_nlp_example2_log_linear_taggers.png" alt="Image Loading"></p>
<h3 id="a-new-set-of-techniques-global-linear-models"><a class="markdownIt-Anchor" href="#a-new-set-of-techniques-global-linear-models"></a> A New Set of Techniques: Global Linear Models</h3>
<h4 id="a-new-framework-global-linear-models"><a class="markdownIt-Anchor" href="#a-new-framework-global-linear-models"></a> A New Framework: Global Linear Models</h4>
<h5 id="intuition"><a class="markdownIt-Anchor" href="#intuition"></a> Intuition</h5>
<ul>
<li>We’ll move away from history-based models<br>
No idea of a “derivation”, or attaching probabilities to “decisions”</li>
<li>Instead, we’ll have feature vectors over entire structures “Global features”<br>
“Global features”</li>
<li>First piece of motivation:<br>
<em>Freedom in defining features</em></li>
</ul>
<p><strong>A Need for Flexible Features</strong><br>
<img src="/img/note_nlp_a_need_for_flexible_features_example1.png" alt="Image Loading"><br>
<img src="/img/note_nlp_a_need_for_flexible_features_example2.png" alt="Image Loading"></p>
<h5 id="three-components-of-global-linear-models"><a class="markdownIt-Anchor" href="#three-components-of-global-linear-models"></a> Three Components of Global Linear Models</h5>
<p><img src="/img/note_nlp_three_components_of_global_linear_models.png" alt="Image Loading"><br>
例如，x是sentence，y是parse tree。candidates是一堆parse tree。</p>
<p><strong>Component 1: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></strong><br>
<img src="/img/note_nlp_component1_f.png" alt="Image Loading"><br>
Features:<br>
<img src="/img/note_nlp_component1_features.png" alt="Image Loading"><br>
Feature Vectors:<br>
<img src="/img/note_nlp_component1_feature_vectors.png" alt="Image Loading"><br>
feature的设计很重要，将来要送进learning algorithm的。</p>
<p><strong>Component 2: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi><mi>E</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">GEN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">G</span><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span></strong><br>
<img src="/img/note_nlp_component2_gen.png" alt="Image Loading"></p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi><mi>E</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">GEN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">G</span><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span> enumerates a set of candidates for an input x</li>
<li>Some examples of how <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi><mi>E</mi><mi>N</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">GEN(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">G</span><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span> can be defined:<br>
Parsing: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi><mi>E</mi><mi>N</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">GEN(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">G</span><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span> is the set of parses for <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span> under a grammar<br>
Any task: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi><mi>E</mi><mi>N</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">GEN(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">G</span><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span> is the top N most probable parses under a history-based model<br>
Tagging: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi><mi>E</mi><mi>N</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">GEN(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">G</span><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span> is the set of all possible tag sequences with the same length as <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span><br>
Translation: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi><mi>E</mi><mi>N</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">GEN(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">G</span><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span> is the set of all possible English translations for the French sentence <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span></li>
</ul>
<p><strong>Component 3: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">v</span></span></span></span></strong><br>
<img src="/img/note_nlp_component3_v.png" alt="Image Loading"></p>
<p><strong>Putting is all Together</strong><br>
<img src="/img/note_nlp_putting_it_all_together.png" alt="Image Loading"><br>
<img src="/img/note_nlp_putting_it_all_together1.png" alt="Image Loading"></p>
<h4 id="parsing-problems-in-this-framework-reranking-problems"><a class="markdownIt-Anchor" href="#parsing-problems-in-this-framework-reranking-problems"></a> Parsing Problems in This Framework: Reranking Problems</h4>
<p><strong>Reranking Approaches to Parsing</strong><br>
<img src="/img/note_nlp_reranking_approaches_to_parsing.png" alt="Image Loading"><br>
baseline parser比如lexicalized PCFG。</p>
<p><strong>The Representation <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></strong><br>
<img src="/img/note_nlp_the_representation_f.png" alt="Image Loading"><br>
Some Features in the Paper:<br>
From <em>[Colins and Koo, 2005]</em>:<br>
The following types of features were included in the model. We will use the rule VP -&gt; PP VBD NP NP SBAR with head VBD as an example. Note that the output of out baseline parser produces syntactic trees with headword annotations.</p>
<p><strong>Rules</strong> These include all context-free rules in the tree, for example VP -&gt; PP VBD NP NP SBAR<br>
<img src="/img/note_nlp_features_in_paper_rules.png" alt="Image Loading"><br>
<strong>Bigrams</strong> These are adjacent pairs of non-terminals to the left and right of the head. As shown, the example rule would contribute the bigrams (Right, VP, NP, NP), (Right, VP, NP, SBAR), (Right, VP, SBAR, STOP), and (Left, VP, PP, STOP) to the left of the head.<br>
<img src="/img/note_nlp_features_in_paper_bigrams.png" alt="Image Loading"><br>
<strong>Grandparent Rules</strong> Same as <strong>Rules</strong>, but also including the non-terminal above the rule.<br>
<img src="/img/note_nlp_features_in_paper_grandparent_rules.png" alt="Image Loading"><br>
<strong>Two-level Rules</strong> Same as <strong>Rules</strong>, but also including the entire rule above the rule.<br>
<img src="/img/note_nlp_features_in_paper_two_level_rules.png" alt="Image Loading"></p>
<h4 id="parameter-estimation-method-1-a-variant-of-the-perceptron-algorithm"><a class="markdownIt-Anchor" href="#parameter-estimation-method-1-a-variant-of-the-perceptron-algorithm"></a> Parameter Estimation Method 1: A Variant of the Perceptron Algorithm</h4>
<p><strong>A Variant of the Perceptron Algorithm</strong><br>
<img src="/img/note_nlp_a_variant_of_the_perceptron_algorithm.png" alt="Image Loading"><br>
T: number of iterations, small 5, 10, 20<br>
Any features which are seen in the true candidate structure have their parameter value increased, whereas any features seen in the incorrectly proposed structure have their parameter values decreased. Kind of Reinforcing Step.</p>
<p><strong>Perceptron Experiments: Parse Reranking</strong><br>
<em>Parsing the Wall Street Journal Treebank</em><br>
Training set = 40,000 sentences, test = 2,416 sentences<br>
Generative model(Collins 1999): 88.2% F-measure<br>
Reranked model: 89.5% F-measure(11% relative error reduction)</p>
<ul>
<li>Results from Charniak and Johnson, 2005<br>
Improvement from 89.7% (baseline generative model) to 91.0% accuracy<br>
Gains from improved n-best lists, better features, better baseline model</li>
</ul>
<h3 id="summary-7"><a class="markdownIt-Anchor" href="#summary-7"></a> Summary</h3>
<ul>
<li>A new framework: <strong>global linear models</strong><br>
GEN, f, v</li>
<li>There are several ways to train the parameters <strong>v</strong>:<br>
Perceptron<br>
Boosting<br>
Log-linear models (maximum-likelihood)</li>
<li>Applications:<br>
Parsing<br>
Generation<br>
Machine translation<br>
Tagging problems<br>
Speech recognition</li>
</ul>
<hr>
<h2 id="glms-for-tagging"><a class="markdownIt-Anchor" href="#glms-for-tagging"></a> GLMs for Tagging</h2>
<p>Global Linear Models Part 2: The Perceptron Algorithm for Tagging<br>
针对Tagging，我们之前已经学过HMMs和Log-Linear Taggers，现在学习第三种GLMs and Perceptron。<br>
<strong>Tagging using Global Linear Models</strong><br>
<img src="/img/note_nlp_tagging_using_global_linear_models.png" alt="Image Loading"><br>
例如：<br>
n = 3<br>
w = the dog barks<br>
T = {D, N ,V}<br>
GEN(the dog barks) = {DDD, DDN, DND, DNN, …} 共3^3 = 27个<br>
f(the dog barks, D N V) = (1, 0, 0, 1)</p>
<p><strong>Representation: Histories</strong><br>
<img src="/img/note_nlp_representation_histories1.png" alt="Image Loading"></p>
<p><strong>Local Feature-Vector Representations</strong><br>
<img src="/img/note_nlp_local_feature_vector_representations.png" alt="Image Loading"><br>
A tagged sentence with n words has n history/tag pairs.<br>
<img src="/img/note_nlp_history_tag_pairs.png" alt="Image Loading"><br>
global feature vector = the sum of local feature vectors</p>
<p><strong>Global and Local Features</strong><br>
<img src="/img/note_nlp_global_and_local_features.png" alt="Image Loading"></p>
<p><strong>Putting it all Together</strong><br>
<img src="/img/note_nlp_putting_it_all_together_glm_tagging.png" alt="Image Loading"></p>
<p><strong>Recap: A Variant of the Perceptron Algorithm</strong><br>
<img src="/img/note_nlp_a_variant_of_the_perceptron_algorithm.png" alt="Image Loading"></p>
<p><strong>Training a Tagger Using the Perceptron Algorithm</strong><br>
<img src="/img/note_nlp_training_a_tagger_using_the_perceptron_algorithm.png" alt="Image Loading"></p>
<p><strong>An Example</strong><br>
<img src="/img/note_nlp_example_glm_tagging.png" alt="Image Loading"></p>
<p><strong>Experiments</strong></p>
<ul>
<li>Wall Street Journal part-of-speech tagging data<br>
Perceptron = 2.89% error, Log-linear tagger = 3.28% error</li>
<li>[Ramshaw and Marcus, 1995] NP chunking data<br>
Perceptron = 93.63% accuracy, Log-linear tagger = 93.29% accuracy</li>
</ul>
<hr>
<h2 id="glms-for-dependency-parsing"><a class="markdownIt-Anchor" href="#glms-for-dependency-parsing"></a> GLMs for Dependency Parsing</h2>
<p>Global Linear Models Part 3: The Perceptron Algorithm for Dependency Parsing</p>
<h3 id="dependency-parsing"><a class="markdownIt-Anchor" href="#dependency-parsing"></a> Dependency Parsing</h3>
<p><strong>Unlabeled Dependency Parses</strong><br>
<img src="/img/note_nlp_unlabeled_dependency_parses.png" alt="Image Loading"><br>
labeled就是标上每个arc代表的关系。</p>
<p>**All Dependency Parses for <em>John saw Mary</em> **<br>
<img src="/img/note_nlp_all_dependency_parses_for_john_saw_mary.png" alt="Image Loading"><br>
正确的是左列第二个。</p>
<p><strong>Conditions on Dependency Structures</strong><br>
<img src="/img/note_nlp_a_more_complex_example.png" alt="Image Loading"></p>
<ul>
<li>The dependency arcs from a <em>directed tree</em>, with the root symbol at the root of the tree.<br>
(Definition: A directed tree rooted at <em>root</em> is a tree, where for every word <em>w</em> other than the root, there is a directed path from <em>root</em> to <em>w</em>.)</li>
<li>There are no “crossing dependencies”.<br>
Dependency structures with no crossing dependencies are sometimes referred to as <strong>projective</strong> structures.</li>
</ul>
<p>没有环，也没有交叉。<br>
有交叉的叫做Non-projective Structures，在本课中认为是不允许的。<br>
<img src="/img/note_nlp_non_projective_structures.png" alt="Image Loading"></p>
<p><strong>Dependency Parsing Resources</strong></p>
<ul>
<li>CoNLL 2006 conference had a “shared task” with dependency parsing of 12 languages (Arabic, Chinese, Czech, Danish, Dutch, German, Japanese, Portuguese, Slovene, Spanish, Swedish, Turkish). 19 different groups developed dependency parsing systems. (See also CoNLL 2007).</li>
<li>PhD thesis on the topic: Ryan McDonald, Discriminative Training and Spanning Tree Algorithms for Dependency Parsing, University of Pennsylvania.</li>
<li>For some languages, e.g., Czech, there are “dependency banks” available which contain training data in the form of sentences paired with dependency structures.</li>
<li>For other languages, we can extract dependency structures from treebanks.<br>
<img src="/img/note_nlp_dependency_parsing_resources.png" alt="extract dependency structures from treebank"><br>
图上右下角的S应该是S(was,Vt)<br>
head word一层层往下找，这个过程就是之前讲过的lexicalization。</li>
</ul>
<p><strong>Efficiency of Dependency Parsing</strong></p>
<ul>
<li>PCFG parsing is <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>n</mi><mn>3</mn></msup><msup><mi>G</mi><mn>3</mn></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n^3G^3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">n</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">3</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">G</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">3</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span> where n is the length of the sentence, G is the number of non-terminals in the grammar.</li>
<li>Lexicalized PCFG parsing is <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>n</mi><mn>5</mn></msup><msup><mi>G</mi><mn>3</mn></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n^5G^3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">n</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">5</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">G</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">3</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span> where n is the length of the sentence, G is the number of non-terminals in the grammar.</li>
<li>Unlabeled dependency parsing is <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>n</mi><mn>3</mn></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">n</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">3</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>.因为用了dynamic programming，看Jason Eisner的paper。</li>
</ul>
<p>very efficient at parsing, very useful representations.</p>
<h3 id="glms-for-dependency-parsing-2"><a class="markdownIt-Anchor" href="#glms-for-dependency-parsing-2"></a> GLMs for Dependency Parsing</h3>
<p><strong>GLMs for Dependency parsing</strong><br>
<img src="/img/note_nlp_glms_for_dependency_parsing1.png" alt="Image Loading"><br>
size of GEN is exponentional in n.<br>
<img src="/img/note_nlp_glms_for_dependency_parsing2.png" alt="Image Loading"><br>
x是句子，y是一种Dependency结构。</p>
<p><strong>Definition of Local Feature Vectors</strong><br>
<img src="/img/note_nlp_definition_of_local_feature_vectors.png" alt="Image Loading"></p>
<p>例子：针对前文的John saw a movie，可以 有如下feature。<br>
g1(x,h,m) = 1 if xh=saw, and xm=movie; 0 otherwise<br>
g2(x,h,m) = 1 if POS(h)=VBD, and POS(m)=NN; 0 otherwise<br>
g3(x,h,m) = 1 if |h-m|&gt;10; 0 otherwise</p>
<h3 id="results-from-mcdonald2005"><a class="markdownIt-Anchor" href="#results-from-mcdonald2005"></a> Results from McDonald(2005)</h3>
<p><img src="/img/note_nlp_results_from_mcdonald.png" alt="Image Loading"><br>
G = Num of non-terminals</p>
<p><strong>Extensions</strong></p>
<ul>
<li>2nd-order dependency parsing, 3rd-order dependency parsing<br>
不再只孤立看一个dependency arc。能得到93-94% accuracy。</li>
<li>Non-projective dependency structures<br>
spanning tree algorithms</li>
</ul>
<h3 id="summary-8"><a class="markdownIt-Anchor" href="#summary-8"></a> Summary</h3>
<p><img src="/img/note_nlp_glms_for_dependency_parsing_summary.png" alt="Image Loading"></p>
<hr>
<p>(｢･ω･)｢<br>
春假前一天顺利完成！</p>

  </div>
</article>



        
    </div>
    
      <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/archives/">博文</a></li>
         
          <li><a href="/categories/">分类</a></li>
         
          <li><a href="/tags/">标签</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction-to-natural-language-processing"><span class="toc-number">1.</span> <span class="toc-text"> Introduction to Natural Language Processing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#what-is-natural-language-processingnlp"><span class="toc-number">1.1.</span> <span class="toc-text"> What is Natural Language Processing(NLP)?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#why-is-nlp-hard"><span class="toc-number">1.2.</span> <span class="toc-text"> Why is NLP Hard?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-will-this-course-be-about"><span class="toc-number">1.3.</span> <span class="toc-text"> What will this course be about?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-language-modeling-problem"><span class="toc-number">2.</span> <span class="toc-text"> The Language Modeling Problem</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#introduction-to-the-language-modeling-problem"><span class="toc-number">2.1.</span> <span class="toc-text"> Introduction to the language modeling problem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#markov-processes"><span class="toc-number">2.2.</span> <span class="toc-text"> Markov Processes</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#trigram-language-models"><span class="toc-number">2.3.</span> <span class="toc-text"> Trigram Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#evaluating-language-models-perplexity"><span class="toc-number">2.4.</span> <span class="toc-text"> Evaluating language models: perplexity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#parameter-estimation-in-language-models"><span class="toc-number">2.5.</span> <span class="toc-text"> Parameter Estimation in Language Models</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#linear-interpolation"><span class="toc-number">2.5.1.</span> <span class="toc-text"> Linear Interpolation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#discounting-methods"><span class="toc-number">2.5.2.</span> <span class="toc-text"> Discounting Methods</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary"><span class="toc-number">2.6.</span> <span class="toc-text"> Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tagging-problems-and-hidden-markov-models"><span class="toc-number">3.</span> <span class="toc-text"> Tagging Problems, and Hidden Markov Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#the-tagging-problem"><span class="toc-number">3.1.</span> <span class="toc-text"> The Tagging Problem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#generative-models-and-the-noisy-channel-model-for-supervised-learning"><span class="toc-number">3.2.</span> <span class="toc-text"> Generative models, and the noisy-channel model, for supervised learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hidden-markov-model-hmm-taggers"><span class="toc-number">3.3.</span> <span class="toc-text"> Hidden Markov Model (HMM) taggers</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#basic-definitions"><span class="toc-number">3.3.1.</span> <span class="toc-text"> Basic definitions</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#parameter-estimation"><span class="toc-number">3.3.2.</span> <span class="toc-text"> Parameter estimation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-viterbi-algorithm"><span class="toc-number">3.3.3.</span> <span class="toc-text"> The Viterbi algorithm</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary-2"><span class="toc-number">3.4.</span> <span class="toc-text"> Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#parsing-and-context-free-grammars"><span class="toc-number">4.</span> <span class="toc-text"> Parsing, and Context-Free Grammars</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#an-introduction-to-the-parsing-problem"><span class="toc-number">4.1.</span> <span class="toc-text"> An Introduction to the Parsing Problem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#context-free-grammars"><span class="toc-number">4.2.</span> <span class="toc-text"> Context Free Grammars</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-brief-sketch-of-the-syntax-of-english"><span class="toc-number">4.3.</span> <span class="toc-text"> A Brief(!) Sketch of the Syntax of English</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#examples-of-ambiguous-structures"><span class="toc-number">4.4.</span> <span class="toc-text"> Examples of Ambiguous Structures</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#probabilistic-context-free-grammars-pcfgs"><span class="toc-number">5.</span> <span class="toc-text"> Probabilistic Context-Free Grammars (PCFGs)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#probabilistic-context-free-grammars-pcfgs-2"><span class="toc-number">5.1.</span> <span class="toc-text"> Probabilistic Context-Free Grammars (PCFGs)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-cky-algorithm-for-parsing-with-pcfgs"><span class="toc-number">5.2.</span> <span class="toc-text"> The CKY Algorithm for parsing with PCFGs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary-3"><span class="toc-number">5.3.</span> <span class="toc-text"> Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#weeknesses-of-pcfgs"><span class="toc-number">6.</span> <span class="toc-text"> Weeknesses of PCFGs</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#lack-of-sensitivity-to-lexical-information"><span class="toc-number">6.1.</span> <span class="toc-text"> Lack of Sensitivity to Lexical Information</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lack-of-sensitivity-to-structural-frequencies"><span class="toc-number">6.2.</span> <span class="toc-text"> Lack of Sensitivity to Structural Frequencies</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lexicalized-pcfgs"><span class="toc-number">7.</span> <span class="toc-text"> Lexicalized PCFGs</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#lexicalization-of-a-treebank"><span class="toc-number">7.1.</span> <span class="toc-text"> Lexicalization of a treebank</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lexicalized-probabilistic-context-free-grammars"><span class="toc-number">7.2.</span> <span class="toc-text"> Lexicalized probabilistic context-free grammars</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#parameter-estimation-in-lexicalized-probabilistic-context-free-grammars"><span class="toc-number">7.3.</span> <span class="toc-text"> Parameter estimation in lexicalized probabilistic context-free grammars</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#accuracy-of-lexicalized-probabilistic-context-free-grammars"><span class="toc-number">7.4.</span> <span class="toc-text"> Accuracy of lexicalized probabilistic context-free grammars</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary-4"><span class="toc-number">7.5.</span> <span class="toc-text"> Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction-to-machine-translationmt"><span class="toc-number">8.</span> <span class="toc-text"> Introduction to Machine Translation(MT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#challenges-in-machine-translation"><span class="toc-number">8.1.</span> <span class="toc-text"> Challenges in machine translation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#classical-machine-translation"><span class="toc-number">8.2.</span> <span class="toc-text"> Classical machine translation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-brief-introduction-to-statistical-mt"><span class="toc-number">8.3.</span> <span class="toc-text"> A brief introduction to statistical MT</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-ibm-translation-models"><span class="toc-number">9.</span> <span class="toc-text"> The IBM Translation Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ibm-model-1"><span class="toc-number">9.1.</span> <span class="toc-text"> IBM Model 1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ibm-model-2"><span class="toc-number">9.2.</span> <span class="toc-text"> IBM Model 2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#em-training-of-models-1-and-2"><span class="toc-number">9.3.</span> <span class="toc-text"> EM Training of Models 1 and 2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary-5"><span class="toc-number">9.4.</span> <span class="toc-text"> Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#phrase-based-translation-models"><span class="toc-number">10.</span> <span class="toc-text"> Phrase-based Translation Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#learning-phrases-from-alignments"><span class="toc-number">10.1.</span> <span class="toc-text"> Learning Phrases From Alignments</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-phrase-based-model"><span class="toc-number">10.2.</span> <span class="toc-text"> A Phrase-Based Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#decoding-of-phrase-based-translation-models"><span class="toc-number">10.3.</span> <span class="toc-text"> Decoding of Phrase-based Translation Models</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#definition-of-the-decoding-problem"><span class="toc-number">10.3.1.</span> <span class="toc-text"> Definition of the Decoding Problem</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-decoding-algorithm"><span class="toc-number">10.3.2.</span> <span class="toc-text"> The Decoding Algorithm</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#log-linear-models"><span class="toc-number">11.</span> <span class="toc-text"> Log-linear Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#two-example-problems"><span class="toc-number">11.1.</span> <span class="toc-text"> Two Example Problems</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#features-in-log-linear-models"><span class="toc-number">11.2.</span> <span class="toc-text"> Features in Log-Linear Models</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#for-problem-1"><span class="toc-number">11.2.1.</span> <span class="toc-text"> For Problem 1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#for-problem-2"><span class="toc-number">11.2.2.</span> <span class="toc-text"> For Problem 2</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#definition-of-log-linear-models"><span class="toc-number">11.3.</span> <span class="toc-text"> Definition of Log-linear Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#parameter-estimation-in-log-linear-models"><span class="toc-number">11.4.</span> <span class="toc-text"> Parameter Estimation in Log-linear Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#smoothingregularization-in-log-linear-models"><span class="toc-number">11.5.</span> <span class="toc-text"> Smoothing/Regularization in Log-linear Models</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#log-linear-models-for-taggingmemms"><span class="toc-number">12.</span> <span class="toc-text"> Log-linear Models for Tagging(MEMMs)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#recap-the-tagging-problem"><span class="toc-number">12.1.</span> <span class="toc-text"> Recap: The Tagging Problem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#log-linear-taggers"><span class="toc-number">12.2.</span> <span class="toc-text"> Log-Linear Taggers</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#independence-assumptions-in-log-linear-taggers"><span class="toc-number">12.2.1.</span> <span class="toc-text"> Independence Assumptions in Log-linear Taggers</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#features-in-log-linear-taggers"><span class="toc-number">12.2.2.</span> <span class="toc-text"> Features in Log-Linear Taggers</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#parameters-in-log-linear-models"><span class="toc-number">12.2.3.</span> <span class="toc-text"> Parameters in Log-linear Models</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-viterbi-algorithm-for-log-linear-taggers"><span class="toc-number">12.2.4.</span> <span class="toc-text"> The Viterbi Algorithm for Log-linear Taggers</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#an-example-application"><span class="toc-number">12.2.5.</span> <span class="toc-text"> An Example Application</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary-6"><span class="toc-number">12.3.</span> <span class="toc-text"> Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#log-linear-models-for-history-based-parsing"><span class="toc-number">13.</span> <span class="toc-text"> Log-Linear Models for History-based Parsing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#conditional-history-based-models"><span class="toc-number">13.1.</span> <span class="toc-text"> Conditional History-based Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#representing-trees-as-decision-sequences关于step-1"><span class="toc-number">13.2.</span> <span class="toc-text"> Representing Trees as Decision Sequences(关于Step 1)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#features关于step-3"><span class="toc-number">13.3.</span> <span class="toc-text"> Features(关于Step 3)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#beam-search关于step-4"><span class="toc-number">13.4.</span> <span class="toc-text"> Beam Search(关于Step 4)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#unsupervised-learning-brown-clustering"><span class="toc-number">14.</span> <span class="toc-text"> Unsupervised Learning: Brown Clustering</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#word-cluster-representations"><span class="toc-number">14.1.</span> <span class="toc-text"> Word Cluster Representations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-brown-clustering-algorithm"><span class="toc-number">14.2.</span> <span class="toc-text"> The Brown Clustering Algorithm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#clusters-in-ne-recognition"><span class="toc-number">14.3.</span> <span class="toc-text"> Clusters in NE Recognition</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#global-linear-modelsglms"><span class="toc-number">15.</span> <span class="toc-text"> Global Linear Models(GLMs)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#a-brief-review-of-history-based-methods"><span class="toc-number">15.1.</span> <span class="toc-text"> A Brief Review of History-based Methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-new-set-of-techniques-global-linear-models"><span class="toc-number">15.2.</span> <span class="toc-text"> A New Set of Techniques: Global Linear Models</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#a-new-framework-global-linear-models"><span class="toc-number">15.2.1.</span> <span class="toc-text"> A New Framework: Global Linear Models</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#intuition"><span class="toc-number">15.2.1.1.</span> <span class="toc-text"> Intuition</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#three-components-of-global-linear-models"><span class="toc-number">15.2.1.2.</span> <span class="toc-text"> Three Components of Global Linear Models</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#parsing-problems-in-this-framework-reranking-problems"><span class="toc-number">15.2.2.</span> <span class="toc-text"> Parsing Problems in This Framework: Reranking Problems</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#parameter-estimation-method-1-a-variant-of-the-perceptron-algorithm"><span class="toc-number">15.2.3.</span> <span class="toc-text"> Parameter Estimation Method 1: A Variant of the Perceptron Algorithm</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary-7"><span class="toc-number">15.3.</span> <span class="toc-text"> Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#glms-for-tagging"><span class="toc-number">16.</span> <span class="toc-text"> GLMs for Tagging</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#glms-for-dependency-parsing"><span class="toc-number">17.</span> <span class="toc-text"> GLMs for Dependency Parsing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#dependency-parsing"><span class="toc-number">17.1.</span> <span class="toc-text"> Dependency Parsing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#glms-for-dependency-parsing-2"><span class="toc-number">17.2.</span> <span class="toc-text"> GLMs for Dependency Parsing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#results-from-mcdonald2005"><span class="toc-number">17.3.</span> <span class="toc-text"> Results from McDonald(2005)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary-8"><span class="toc-number">17.4.</span> <span class="toc-text"> Summary</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://conglang.github.io/2015/09/05/note-natural-language-processing/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&text=Natural Language Processing 课程笔记"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&title=Natural Language Processing 课程笔记"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&is_video=false&description=Natural Language Processing 课程笔记"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Natural Language Processing 课程笔记&body=Check out this article: http://conglang.github.io/2015/09/05/note-natural-language-processing/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&title=Natural Language Processing 课程笔记"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&title=Natural Language Processing 课程笔记"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&title=Natural Language Processing 课程笔记"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&title=Natural Language Processing 课程笔记"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://conglang.github.io/2015/09/05/note-natural-language-processing/&name=Natural Language Processing 课程笔记&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

    
    <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2019 聪
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/archives/">博文</a></li>
         
          <li><a href="/categories/">分类</a></li>
         
          <li><a href="/tags/">标签</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

</body>

</html>

<!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

<!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-74786593-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?4e074986ce7bd4c6c94338ce1a49c4be";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->



